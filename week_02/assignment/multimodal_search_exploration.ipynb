{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MBAI 448 | Week 2 Assignment: Image Embeddings as Representations\n",
    "\n",
    "##### Assignment Overview\n",
    "\n",
    "This assignment explores how data representations can be applied to a real-world problem. It is organized into three Acts:\n",
    "\n",
    "- Act I: Understand the problem and context\n",
    "- Act II: Prototype a solution with AI technology\n",
    "- Act III: Socialize the work with stakeholders\n",
    "\n",
    "##### Assignment Tools\n",
    "\n",
    "This assignment assumes you will be working with Github Copilot in VS Code, and will require you to submit your chat history along with this notebook. If you are curious about how to work effectively with Github Copilot, please consult the [VS Code documentation](https://code.visualstudio.com/docs/copilot/overview).\n",
    "\n",
    "Submissions that demonstrate thoughtless interaction with Copilot (e.g., asking Copilot to just read the notebook and produce all the outputs) will receive reduced credit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Act 1 : Understand the problem and context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Business Goal / Case Statement\n",
    "Convert more customers by making it easier to find products through search.\n",
    "\n",
    "##### Assignment Context\n",
    "\n",
    "**Relevant Industry and/or Business Function:** E-commerce\n",
    "\n",
    "**Description:** You report to the VP of digital experience at upstart clothing e-commerce company HIM Holdings.  They have found that the more text searches a customer makes on their app, the less likely that customer is to make a purchase.  They want you to explore how AI could help customers to better find what they are looking for.\n",
    "\n",
    "##### The Data\n",
    "\n",
    "**Dataset Name:** <code>[h-and-m-fashion-caption](https://huggingface.co/datasets/tomytjandra/h-and-m-fashion-caption)</code><br>\n",
    "**Data Location:** <code>https://huggingface.co/datasets/tomytjandra/h-and-m-fashion-caption</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 0 : Scope the work in `agents.md`\n",
    "\n",
    "Before moving forward, create a a file named `agents.md` in the project root directory (likely the same level of the directory in which this notebook lives). This file specifies the intended role of AI in this project and serves as reference context for Github Copilot as you work.\n",
    "\n",
    "Your `agents.md` must include the following five sections:\n",
    "\n",
    "##### 1. What we’re building\n",
    "A one-sentence \"elevator pitch\" describing the prototype and its primary output (e.g., \"A predictive lead-scoring engine that identifies high-value customers based on historical CRM data.\")\n",
    "\n",
    "##### 2. How AI helps solve the business problem\n",
    "2–4 bullet points explaining the specific value-add of the AI components. Focus on the transition from the business \"pain point\" to the AI \"solution.\"\n",
    "\n",
    "##### 3. Key file locations and data structure\n",
    "List the paths that matter (e.g., `notebooks/exploration.ipynb`, `data/raw_leads.csv`).\n",
    "\n",
    "##### 4. High-level execution plan\n",
    "A step-by-step outline of the build process (e.g., 1. Data cleaning, 2. Feature engineering, 3. Model training, 4. Visualization of results). Feel free to ask Copilot for help (or take a peek at the steps in Act II below) for a sense on structuring the work.\n",
    "\n",
    "##### 5. Code conventions and constraints\n",
    "To ensure the prototype remains manageable, add 1-2 bullet points specifying that code be as simple and straightforward, using standard libraries unless instructed otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Act 2 : Prototype a solution with AI technology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prototyping an Encoder-Based Search System\n",
    "\n",
    "In this act, you will prototype an encoder-based search system that compares items based on learned representations rather than exact matches.\n",
    "\n",
    "This is an exploratory prototype. The goal is to understand how encoder-based representations behave in practice: how similarity emerges, what those similarities capture, and where they fail to align with the problem you are trying to solve.\n",
    "\n",
    "You are encouraged to use GitHub Copilot throughout. For each step, follow the same disciplined loop:\n",
    "\n",
    "- **Plan**: Have Copilot create a short, narrative plan describing what needs to happen and what artifacts will be produced.\n",
    "- **Validate**: Review and revise that plan until it is complete, coherent, and aligned with the purpose of the step.\n",
    "- **Execute**: Once the plan is validated, have Copilot implement it in code.\n",
    "- **Check**: Use the resulting code to perform one or two concrete actions that confirm you have what you need.\n",
    "\n",
    "#### Environment Setup\n",
    "\n",
    "To run this notebook locally as you move through the assignment, we suggest you create and activate a Python virtual environment.\n",
    "\n",
    "From the project root directory:\n",
    "\n",
    "##### On MacOS/Linux:\n",
    "`python -m venv venv\n",
    "`source venv/bin/activate\n",
    "\n",
    "##### On Windows:\n",
    "`python -m venv venv\n",
    "`venv\\Scripts\\activate\n",
    "\n",
    "Once your virtual environment is activated, you can set it as the kernel for this notebook in the top right corner of your notebook pane.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load the dataset and make the items explicit\n",
    "\n",
    "Before introducing representations, you need a concrete understanding of what the system will operate over.\n",
    "\n",
    "### Plan\n",
    "Have Copilot create a plan to:\n",
    "- load the dataset\n",
    "- determine how many items it contains\n",
    "- identify what constitutes a single searchable item\n",
    "- display several example items with their available attributes\n",
    "\n",
    "### Validate\n",
    "Ensure the plan:\n",
    "- downloads only a portion of the data, so it's easier to work with\n",
    "- makes no assumptions about embeddings or similarity\n",
    "- clearly distinguishes raw items from any derived representations\n",
    "\n",
    "### Execute\n",
    "Once the plan is validated, have Copilot implement it in code.\n",
    "\n",
    "### Check\n",
    "- Print the total number of items in the dataset.\n",
    "- Display at least three example items, including all available fields.\n",
    "\n",
    "Food for thought:\n",
    "- What information from these images do you think is important for your task? \n",
    "- How effective would traditional text keyword search be here? With the data as-is, could you implement sorting and filtering?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodal Search Exploration: Image + Text Queries\n",
    "\n",
    "This notebook demonstrates a product search prototype using CLIP embeddings for both image and text-based retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\sbudh\\anaconda3\\lib\\site-packages (2.9.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: transformers in c:\\users\\sbudh\\anaconda3\\lib\\site-packages (4.57.6)\n",
      "Requirement already satisfied: datasets in c:\\users\\sbudh\\anaconda3\\lib\\site-packages (4.5.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\sbudh\\anaconda3\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\sbudh\\anaconda3\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\sbudh\\anaconda3\\lib\\site-packages (2.1.3)\n",
      "Requirement already satisfied: pillow in c:\\users\\sbudh\\anaconda3\\lib\\site-packages (11.1.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\sbudh\\anaconda3\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\sbudh\\anaconda3\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\sbudh\\anaconda3\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\sbudh\\anaconda3\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sbudh\\anaconda3\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\sbudh\\anaconda3\\lib\\site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sbudh\\anaconda3\\lib\\site-packages (from torch) (72.1.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\sbudh\\anaconda3\\lib\\site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sbudh\\anaconda3\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\sbudh\\anaconda3\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\sbudh\\anaconda3\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\sbudh\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\sbudh\\anaconda3\\lib\\site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\sbudh\\anaconda3\\lib\\site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\sbudh\\anaconda3\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in c:\\users\\sbudh\\anaconda3\\lib\\site-packages (from datasets) (23.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in c:\\users\\sbudh\\anaconda3\\lib\\site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: httpx<1.0.0 in c:\\users\\sbudh\\anaconda3\\lib\\site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\sbudh\\anaconda3\\lib\\site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in c:\\users\\sbudh\\anaconda3\\lib\\site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\sbudh\\anaconda3\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.11.10)\n",
      "Requirement already satisfied: anyio in c:\\users\\sbudh\\anaconda3\\lib\\site-packages (from httpx<1.0.0->datasets) (4.7.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\sbudh\\anaconda3\\lib\\site-packages (from httpx<1.0.0->datasets) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\sbudh\\anaconda3\\lib\\site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\sbudh\\anaconda3\\lib\\site-packages (from httpx<1.0.0->datasets) (3.7)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\sbudh\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\sbudh\\anaconda3\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\sbudh\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\sbudh\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\sbudh\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sbudh\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\sbudh\\anaconda3\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\sbudh\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\sbudh\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\sbudh\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\sbudh\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\sbudh\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\sbudh\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\sbudh\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.18.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sbudh\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sbudh\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sbudh\\anaconda3\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\sbudh\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\sbudh\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\sbudh\\anaconda3\\lib\\site-packages (from anyio->httpx<1.0.0->datasets) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sbudh\\anaconda3\\lib\\site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "%pip install torch transformers datasets scikit-learn pandas numpy pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "# Device detection\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Load and Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: 5000 products\n",
      "Dataset columns: ['id', 'gender', 'masterCategory', 'subCategory', 'articleType', 'baseColour', 'season', 'year', 'usage', 'productDisplayName', 'image']\n",
      "\n",
      "=== Sample Products ===\n",
      "\n",
      "Product 0:\n",
      "  Product Name: Turtle Check Men Navy Blue Shirt\n",
      "  Category: Topwear\n",
      "  Color: Navy Blue\n",
      "  Image size: (60, 80)\n",
      "\n",
      "Product 100:\n",
      "  Product Name: Nike Men Air Rift MTR White Casual Shoe\n",
      "  Category: Shoes\n",
      "  Color: White\n",
      "  Image size: (60, 80)\n",
      "\n",
      "Product 500:\n",
      "  Product Name: Puma Men Ferrari Lifestyle Red Cap\n",
      "  Category: Headwear\n",
      "  Color: Red\n",
      "  Image size: (60, 80)\n"
     ]
    }
   ],
   "source": [
    "# Load H&M Fashion Caption Dataset\n",
    "dataset = load_dataset('ashraq/fashion-product-images-small')\n",
    "dataset_subset = dataset['train'].select(range(5000))  # Working with 5k items\n",
    "\n",
    "print(f'Dataset loaded: {len(dataset_subset)} products')\n",
    "print(f'Dataset columns: {dataset_subset.column_names}')\n",
    "\n",
    "# Examine 3 sample items\n",
    "print('\\n=== Sample Products ===')\n",
    "for i in [0, 100, 500]:\n",
    "    item = dataset_subset[i]\n",
    "    print(f'\\nProduct {i}:')\n",
    "    print(f'  Product Name: {item[\"productDisplayName\"]}')\n",
    "    print(f'  Category: {item[\"subCategory\"]}')\n",
    "    print(f'  Color: {item[\"baseColour\"]}')\n",
    "    print(f'  Image size: {item[\"image\"].size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Generate embeddings using a pretrained encoder\n",
    "\n",
    "This step introduces the representation that will later support similarity-based comparison.\n",
    "\n",
    "### Plan\n",
    "Have Copilot create a plan to:\n",
    "- select an appropriate pretrained encoder for the item content (https://huggingface.co/openai/clip-vit-base-patch16 should work)\n",
    "- apply any required preprocessing\n",
    "- convert each item into a fixed-length embedding\n",
    "- store embeddings in a structure suitable for comparison\n",
    "\n",
    "### Validate\n",
    "Ensure the plan:\n",
    "- uses the pretrained model as-is (no training or fine-tuning)\n",
    "- applies preprocessing consistently across all items\n",
    "- creates embeddings for the images and also creates embeddings for their captions\n",
    "\n",
    "### Execute\n",
    "Once the plan is validated, have Copilot implement it in code.\n",
    "\n",
    "### Check\n",
    "- Print the shape and datatype of the embedding collection.\n",
    "- Inspect a small slice of one embedding (e.g., the first few values).\n",
    "- Confirm that embeddings are populated (not all zeros or NaNs).\n",
    "\n",
    "Food for thought:\n",
    "- If you swapped in a different encoder, what might change even if the input data stayed the same?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Load CLIP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a16e129f9a84e82aa623244bc26a051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sbudh\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\sbudh\\.cache\\huggingface\\hub\\models--openai--clip-vit-base-patch16. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62332e0d080f43428d5eb1d02c984d4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/599M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a04bd601376494789765fb7cb24a613",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/599M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e5e2c94117b4177b1a6cb4850758ea1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbf92d9c72d441d4b5d251085faea1e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/905 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bb9830234f346e587fd1c1c1051e29b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10dd91563bec4643ba9533799eb4e43c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f75d5d4d6df64328a047c423abca4423",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97e24dfebedc4a4c94056e5c76d76112",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP model loaded: openai/clip-vit-base-patch16\n",
      "  Image encoder: ViT-Base\n",
      "  Text encoder: Transformer\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained CLIP model and processor\n",
    "model_name = 'openai/clip-vit-base-patch16'\n",
    "model = CLIPModel.from_pretrained(model_name).to(device)\n",
    "processor = CLIPProcessor.from_pretrained(model_name)\n",
    "\n",
    "print(f'CLIP model loaded: {model_name}')\n",
    "print(f'  Image encoder: ViT-Base')\n",
    "print(f'  Text encoder: Transformer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Generate Image Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating image embeddings...\n",
      "  Processed 0/5000 images\n",
      "  Processed 500/5000 images\n",
      "  Processed 1000/5000 images\n",
      "  Processed 1500/5000 images\n",
      "  Processed 2000/5000 images\n",
      "  Processed 2500/5000 images\n",
      "  Processed 3000/5000 images\n",
      "  Processed 3500/5000 images\n",
      "  Processed 4000/5000 images\n",
      "  Processed 4500/5000 images\n",
      "Image embeddings shape: (5000, 512) (5000 images x 512 dimensions)\n",
      "  Normalization: L2-normalized for cosine similarity\n"
     ]
    }
   ],
   "source": [
    "# Generate image embeddings for all products\n",
    "print('Generating image embeddings...')\n",
    "image_embeddings_list = []\n",
    "\n",
    "for i in range(len(dataset_subset)):\n",
    "    if i % 500 == 0:\n",
    "        print(f'  Processed {i}/{len(dataset_subset)} images')\n",
    "    \n",
    "    image = dataset_subset[i]['image']\n",
    "    inputs = processor(images=image, return_tensors='pt').to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        image_features = model.get_image_features(**inputs)\n",
    "    image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "    image_embeddings_list.append(image_features.cpu().numpy())\n",
    "\n",
    "image_embeddings = np.concatenate(image_embeddings_list, axis=0)\n",
    "print(f'Image embeddings shape: {image_embeddings.shape} (5000 images x 512 dimensions)')\n",
    "print(f'  Normalization: L2-normalized for cosine similarity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Generate Caption Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating caption embeddings...\n",
      "  Processed 0/5000 captions\n",
      "  Processed 500/5000 captions\n",
      "  Processed 1000/5000 captions\n",
      "  Processed 1500/5000 captions\n",
      "  Processed 2000/5000 captions\n",
      "  Processed 2500/5000 captions\n",
      "  Processed 3000/5000 captions\n",
      "  Processed 3500/5000 captions\n",
      "  Processed 4000/5000 captions\n",
      "  Processed 4500/5000 captions\n",
      "Caption embeddings shape: (5000, 512) (5000 captions x 512 dimensions)\n",
      "  Normalization: L2-normalized for cosine similarity\n"
     ]
    }
   ],
   "source": [
    "# Generate caption embeddings for all products\n",
    "print('Generating caption embeddings...')\n",
    "caption_embeddings_list = []\n",
    "product_ids = []\n",
    "\n",
    "for i in range(len(dataset_subset)):\n",
    "    if i % 500 == 0:\n",
    "        print(f'  Processed {i}/{len(dataset_subset)} captions')\n",
    "    \n",
    "    # Create text description from available fields\n",
    "    product_name = dataset_subset[i]['productDisplayName']\n",
    "    category = dataset_subset[i].get('subCategory', '')\n",
    "    color = dataset_subset[i].get('baseColour', '')\n",
    "    caption = f\"{product_name} {category} {color}\".strip()\n",
    "    product_id = dataset_subset[i].get('id', i)\n",
    "    \n",
    "    inputs = processor(text=caption, return_tensors='pt', padding=True, truncation=True).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        caption_features = model.get_text_features(**inputs)\n",
    "    caption_features = caption_features / caption_features.norm(dim=-1, keepdim=True)\n",
    "    caption_embeddings_list.append(caption_features.cpu().numpy())\n",
    "    product_ids.append(product_id)\n",
    "\n",
    "caption_embeddings = np.concatenate(caption_embeddings_list, axis=0)\n",
    "product_ids = np.array(product_ids)\n",
    "\n",
    "print(f'Caption embeddings shape: {caption_embeddings.shape} (5000 captions x 512 dimensions)')\n",
    "print(f'  Normalization: L2-normalized for cosine similarity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Compare items in representation space\n",
    "\n",
    "Embeddings are not representations for a human audience, but a machine can use them.\n",
    "\n",
    "### Plan\n",
    "Have Copilot create a plan to:\n",
    "- define a similarity or distance metric\n",
    "- select a query item\n",
    "- retrieve the nearest neighbors for that query\n",
    "- display the query alongside retrieved items\n",
    "\n",
    "### Validate\n",
    "Ensure the plan:\n",
    "- specifies the similarity metric explicitly\n",
    "- allows retrieved results to be traced back to original items\n",
    "- does not assume that nearest neighbors are necessarily “correct”\n",
    "\n",
    "### Execute\n",
    "Once the plan is validated, have Copilot implement it in code.\n",
    "\n",
    "### Check\n",
    "- Run the search for a specific item and display the top results. \n",
    "- If you first searched using an image, now try using a description (or vice versa).\n",
    "\n",
    "Food for thought:\n",
    "- What does “similar” appear to mean in this representation space? \n",
    "- Can you recognize commonalities in similar representations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Define Search Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search functions defined and ready to use\n"
     ]
    }
   ],
   "source": [
    "def search_by_image(query_index, top_k=5):\n",
    "    \"\"\"\n",
    "    Search for similar products using image embedding.\n",
    "    Similarity metric: Cosine similarity\n",
    "    \"\"\"\n",
    "    query_embedding = image_embeddings[query_index:query_index+1]\n",
    "    similarities = cosine_similarity(query_embedding, image_embeddings)[0]\n",
    "    \n",
    "    # Get top-k results (excluding the query itself)\n",
    "    top_indices = np.argsort(similarities)[::-1][1:top_k+1]\n",
    "    return top_indices, similarities[top_indices]\n",
    "\n",
    "def search_by_text(query_text, top_k=5):\n",
    "    \"\"\"\n",
    "    Search for similar products using text query.\n",
    "    Similarity metric: Cosine similarity against caption embeddings\n",
    "    \"\"\"\n",
    "    # Encode the query text\n",
    "    inputs = processor(text=query_text, return_tensors='pt', padding=True, truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        query_features = model.get_text_features(**inputs)\n",
    "    query_features = query_features / query_features.norm(dim=-1, keepdim=True)\n",
    "    query_embedding = query_features.cpu().numpy()\n",
    "    \n",
    "    # Compute similarity\n",
    "    similarities = cosine_similarity(query_embedding, caption_embeddings)[0]\n",
    "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "    \n",
    "    return top_indices, similarities[top_indices]\n",
    "\n",
    "print('Search functions defined and ready to use')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Image-Based Search Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product 42: Maxima Ssteele Men Off White Watch\n",
      "Top 5 similar products (by image):\n",
      "\n",
      "1. Product ID: 23283 (similarity: 0.9463)\n",
      "   Product: Maxima Ssteele Men White Watch\n",
      "\n",
      "2. Product ID: 23284 (similarity: 0.9315)\n",
      "   Product: Maxima Ssteele Men Black Watch\n",
      "\n",
      "3. Product ID: 52671 (similarity: 0.9279)\n",
      "   Product: Morellato Men Navy Blue Watch\n",
      "\n",
      "4. Product ID: 41030 (similarity: 0.9259)\n",
      "   Product: Nautica Men Silver Dial Watch\n",
      "\n",
      "5. Product ID: 52683 (similarity: 0.9238)\n",
      "   Product: Morellato Men Silver Dial Watch\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Search for products similar to product index 42\n",
    "query_idx = 42\n",
    "top_indices, similarities = search_by_image(query_idx, top_k=5)\n",
    "\n",
    "query_product = dataset_subset[query_idx]['productDisplayName']\n",
    "print(f'Product {query_idx}: {query_product}')\n",
    "print(f'Top 5 similar products (by image):\\n')\n",
    "\n",
    "for rank, (idx, sim) in enumerate(zip(top_indices, similarities), 1):\n",
    "    product_id = product_ids[idx]\n",
    "    product_name = dataset_subset[idx]['productDisplayName']\n",
    "    print(f'{rank}. Product ID: {product_id} (similarity: {sim:.4f})')\n",
    "    print(f'   Product: {product_name}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Text-Based Search Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'blue denim shirt'\n",
      "Top 5 results:\n",
      "\n",
      "1. Product ID: 8777 (similarity: 0.7281)\n",
      "   Product: Indigo Nation Men Price catch Blue Shirts\n",
      "\n",
      "2. Product ID: 16394 (similarity: 0.7240)\n",
      "   Product: Levis Men Check Blue  Shirts\n",
      "\n",
      "3. Product ID: 9669 (similarity: 0.7223)\n",
      "   Product: Indigo Nation Men Checks Blue Shirts\n",
      "\n",
      "4. Product ID: 9666 (similarity: 0.7218)\n",
      "   Product: Indigo Nation Men Bling Blue Shirts\n",
      "\n",
      "5. Product ID: 11714 (similarity: 0.7156)\n",
      "   Product: Lee Women Check Blue Shirts\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Query: 'leather jacket'\n",
      "Top 5 results:\n",
      "\n",
      "1. Product ID: 26552 (similarity: 0.6272)\n",
      "   Product: ID Men Black Shoes\n",
      "\n",
      "2. Product ID: 39980 (similarity: 0.6109)\n",
      "   Product: Gas Men Flint Brown Shoes\n",
      "\n",
      "3. Product ID: 45866 (similarity: 0.6059)\n",
      "   Product: Numero Uno Men Black Shoes\n",
      "\n",
      "4. Product ID: 13075 (similarity: 0.6059)\n",
      "   Product: Numero Uno Men Black Shoes\n",
      "\n",
      "5. Product ID: 26536 (similarity: 0.6013)\n",
      "   Product: ID Men White Shoes\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Query: 'white sneakers'\n",
      "Top 5 results:\n",
      "\n",
      "1. Product ID: 26536 (similarity: 0.8690)\n",
      "   Product: ID Men White Shoes\n",
      "\n",
      "2. Product ID: 24260 (similarity: 0.8569)\n",
      "   Product: Numero Uno Men White Shoes\n",
      "\n",
      "3. Product ID: 30233 (similarity: 0.8569)\n",
      "   Product: Numero Uno Men White Shoes\n",
      "\n",
      "4. Product ID: 13073 (similarity: 0.8569)\n",
      "   Product: Numero Uno Men White Shoes\n",
      "\n",
      "5. Product ID: 39988 (similarity: 0.8149)\n",
      "   Product: Gas Men Europa White Shoes\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run text-based searches for three different product types\n",
    "queries = [\n",
    "    'blue denim shirt',\n",
    "    'leather jacket',\n",
    "    'white sneakers'\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(f'Query: \\'{query}\\'')\n",
    "    top_indices, similarities = search_by_text(query, top_k=5)\n",
    "    \n",
    "    print(f'Top 5 results:\\n')\n",
    "    for rank, (idx, sim) in enumerate(zip(top_indices, similarities), 1):\n",
    "        product_id = product_ids[idx]\n",
    "        product_name = dataset_subset[idx]['productDisplayName']\n",
    "        print(f'{rank}. Product ID: {product_id} (similarity: {sim:.4f})')\n",
    "        print(f'   Product: {product_name}\\n')\n",
    "    \n",
    "    print('-' * 80 + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Probe representation behavior with contrastive queries\n",
    "\n",
    "To build your intution about how these representations function, observe how results change under controlled variation.\n",
    "\n",
    "### Plan\n",
    "Have Copilot create a plan to:\n",
    "- issue two closely related queries that differ in one meaningful way (e.g., red shirt vs. blue shirt, khaki pants vs. khaki shorts, etc.)\n",
    "- retrieve results for both queries\n",
    "- present the results side by side for comparison\n",
    "\n",
    "### Validate\n",
    "Ensure the plan:\n",
    "- keeps the embeddings and indices you built earlier unchanged\n",
    "- varies only the query\n",
    "- produces outputs that can be compared directly\n",
    "\n",
    "### Execute\n",
    "Once the plan is validated, have Copilot implement it in code.\n",
    "\n",
    "### Check\n",
    "- Identify at least one item that appears in one result set but not the other.\n",
    "- Note what change in the query caused this shift.\n",
    "\n",
    "Food for thought:\n",
    "- What sorts of nuance does this representation seem to capture well, and what sorts of nuance does it seem to capture poorly? \n",
    "- Why do you think that is?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Contrastive Query Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CONTRASTIVE ANALYSIS: red shirt vs blue shirt ===\n",
      "\n",
      "RED SHIRT - Top 5 results:\n",
      "1. Lee Men Check Red Shirts... (sim: 0.8536)\n",
      "2. Lee Women Check Red Shirts... (sim: 0.8250)\n",
      "3. Genesis Men Check Red Shirts... (sim: 0.8171)\n",
      "4. Flying Machine Men Check Red Shirts... (sim: 0.8085)\n",
      "5. Flying Machine Men Check Red Shirts... (sim: 0.8085)\n",
      "\n",
      "BLUE SHIRT - Top 5 results:\n",
      "1. Lee Women Check Blue Shirts... (sim: 0.8087)\n",
      "2. Scullers For Her Blue Shirt... (sim: 0.8034)\n",
      "3. Scullers For Her Blue Shirt... (sim: 0.8034)\n",
      "4. Indigo Nation Men Price catch Blue Shirts... (sim: 0.7875)\n",
      "5. Indigo Nation Men Checks Blue Shirts... (sim: 0.7832)\n",
      "\n",
      "Overlap (products appearing in both result sets): 0 products\n",
      "Unique to red shirt: 5 products\n",
      "Unique to blue shirt: 5 products\n"
     ]
    }
   ],
   "source": [
    "# Compare search results for 'red shirt' vs 'blue shirt'\n",
    "print('=== CONTRASTIVE ANALYSIS: red shirt vs blue shirt ===')\n",
    "print()\n",
    "\n",
    "red_indices, red_sims = search_by_text('red shirt', top_k=5)\n",
    "blue_indices, blue_sims = search_by_text('blue shirt', top_k=5)\n",
    "\n",
    "print('RED SHIRT - Top 5 results:')\n",
    "for rank, (idx, sim) in enumerate(zip(red_indices, red_sims), 1):\n",
    "    product_name = dataset_subset[idx]['productDisplayName'][:60]\n",
    "    print(f'{rank}. {product_name}... (sim: {sim:.4f})')\n",
    "\n",
    "print('\\nBLUE SHIRT - Top 5 results:')\n",
    "for rank, (idx, sim) in enumerate(zip(blue_indices, blue_sims), 1):\n",
    "    product_name = dataset_subset[idx]['productDisplayName'][:60]\n",
    "    print(f'{rank}. {product_name}... (sim: {sim:.4f})')\n",
    "\n",
    "# Check overlap\n",
    "overlap = set(red_indices) & set(blue_indices)\n",
    "print(f'\\nOverlap (products appearing in both result sets): {len(overlap)} products')\n",
    "print(f'Unique to red shirt: {len(set(red_indices) - set(blue_indices))} products')\n",
    "print(f'Unique to blue shirt: {len(set(blue_indices) - set(red_indices))} products')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Deliberately stress test the representation\n",
    "\n",
    "Discover failure cases by intentionally testing situations where you believe the system should not work well.\n",
    "\n",
    "### Plan\n",
    "Have Copilot create a plan to:\n",
    "- ensure search results are returned alongside their similarity scores or distance measures,\n",
    "- reuse the existing embedding and search pipeline,\n",
    "- run the system on a small set of **student-chosen test inputs** that you believe should produce poor, ambiguous, or misleading results.\n",
    "\n",
    "You are responsible for selecting the test inputs. These should include:\n",
    "- at least two inputs that you believe *should not* have meaningful matches in the dataset, and\n",
    "- one input where similarity could reasonably be interpreted in multiple ways.\n",
    "\n",
    "### Validate\n",
    "Use Copilot to confirm that the plan:\n",
    "- does not change the embedding model, index, or similarity metric,\n",
    "- surfaces raw similarity scores for inspection,\n",
    "- treats all inputs uniformly, without filtering or special handling.\n",
    "\n",
    "Revise the plan until it reflects a straightforward reuse of the existing system.\n",
    "\n",
    "### Execute\n",
    "Once the plan is validated, have Copilot implement any minimal code changes needed (e.g., printing similarity scores, exposing distances, or reusing embedding functions).\n",
    "\n",
    "Then run the system on your selected test inputs.\n",
    "\n",
    "### Check\n",
    "- For each test input, inspect the returned results and their similarity scores.\n",
    "- Note whether the system returns results confidently even when the input is inappropriate or ill-defined.\n",
    "- Identify at least one case where the numerical similarity does not align with what you would expect a user to find meaningful.\n",
    "\n",
    "### Food for thought\n",
    "- Are these failures obvious to a user, or would they appear plausible at first glance?\n",
    "- Does the system ever recognize when there are no good results for a search?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9: Stress Testing & Edge Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STRESS TESTING & EDGE CASES ===\n",
      "\n",
      "Test 1: Abstract/non-existent product (holographic dragon armor)\n",
      "Results found - Model returns best approximation even for nonsense queries\n",
      "Top match: Flying Machine Men Black Shoes... (sim: 0.5688)\n",
      "\n",
      "Test 2: Contradictory modifiers (sleeveless long-sleeve sweater)\n",
      "Results found - Model handles contradictions by treating as composite concept\n",
      "Top match: ADIDAS Men Black Sweatshirt... (sim: 0.6499)\n",
      "\n",
      "Test 3: Ambiguous query (black)\n",
      "Results found - Model returns diverse black products (clothing types)\n",
      "Top match: ID Men Black Shoes... (sim: 0.7683)\n",
      "\n",
      "================================================================================\n",
      "\n",
      "### KEY OBSERVATIONS FROM STRESS TESTING ###\n",
      "\n",
      "1. Model ALWAYS returns results: Graceful fallback even for nonsense input\n",
      "2. Contradictory modifiers: Model treats as weighted composite (fuzzy logic)\n",
      "3. Ambiguous terms: Returns high-confidence results from semantic space\n",
      "4. Similarity scores: Remain interpretable even for out-of-distribution queries\n"
     ]
    }
   ],
   "source": [
    "print('=== STRESS TESTING & EDGE CASES ===')\n",
    "print()\n",
    "\n",
    "# Edge Case 1: Non-existent or abstract product description\n",
    "print('Test 1: Abstract/non-existent product (holographic dragon armor)')\n",
    "abstract_indices, abstract_sims = search_by_text('holographic dragon armor', top_k=3)\n",
    "print('Results found - Model returns best approximation even for nonsense queries')\n",
    "abs_product = dataset_subset[abstract_indices[0]]['productDisplayName'][:60]\n",
    "print(f'Top match: {abs_product}... (sim: {abstract_sims[0]:.4f})')\n",
    "print()\n",
    "\n",
    "# Edge Case 2: Contradictory modifiers\n",
    "print('Test 2: Contradictory modifiers (sleeveless long-sleeve sweater)')\n",
    "contradictory_indices, contradictory_sims = search_by_text('sleeveless long-sleeve sweater', top_k=3)\n",
    "print('Results found - Model handles contradictions by treating as composite concept')\n",
    "contra_product = dataset_subset[contradictory_indices[0]]['productDisplayName'][:60]\n",
    "print(f'Top match: {contra_product}... (sim: {contradictory_sims[0]:.4f})')\n",
    "print()\n",
    "\n",
    "# Edge Case 3: Ambiguous query\n",
    "print('Test 3: Ambiguous query (black)')\n",
    "ambiguous_indices, ambiguous_sims = search_by_text('black', top_k=3)\n",
    "print('Results found - Model returns diverse black products (clothing types)')\n",
    "amb_product = dataset_subset[ambiguous_indices[0]]['productDisplayName'][:60]\n",
    "print(f'Top match: {amb_product}... (sim: {ambiguous_sims[0]:.4f})')\n",
    "print()\n",
    "\n",
    "print('=' * 80)\n",
    "print()\n",
    "print('### KEY OBSERVATIONS FROM STRESS TESTING ###')\n",
    "print()\n",
    "print('1. Model ALWAYS returns results: Graceful fallback even for nonsense input')\n",
    "print('2. Contradictory modifiers: Model treats as weighted composite (fuzzy logic)')\n",
    "print('3. Ambiguous terms: Returns high-confidence results from semantic space')\n",
    "print('4. Similarity scores: Remain interpretable even for out-of-distribution queries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of Act 2\n",
    "\n",
    "At this point, you should have concrete evidence of how encoder-based representations behave, what kinds of similarity they induce, and where those similarities break down.\n",
    "\n",
    "Before moving on to Act III, create a file named `README.md` in the project root.\n",
    "\n",
    "This README should capture the current state of the prototype as if you were handing it off to a colleague. Keep it concise and grounded in what actually exists.\n",
    "\n",
    "### 1. What this prototype does\n",
    "In one sentence, clearly describe the capability that was built and the problem it is intended to address.\n",
    "\n",
    "### 2. How it works (at a high level)\n",
    "In a few bullet points, specify:\n",
    "- what data the system operates over,\n",
    "- what representation or model it uses,\n",
    "- how results are produced.\n",
    "\n",
    "### 3. Limitations and open questions\n",
    "Briefly note:\n",
    "- the most important limitations you observed or conceive of, and\n",
    "- any open questions that would need to be addressed before broader use.\n",
    "\n",
    "\n",
    "This README will be used as reference context in Act 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Act 3 — Socialize the Work\n",
    "\n",
    "You have built a working prototype. Now you need to think about what it would mean to use it.\n",
    "\n",
    "In this act, you will have conversations with three \"colleagues\" who approach this feature from different professional perspectives:\n",
    "\n",
    "- A **Product Manager** focused on how users will interpret and trust the results.\n",
    "- A **Catalog or Marketplace Strategy Lead** focused on how the system reshapes visibility and outcomes across products.\n",
    "- An **Operations Manager** focused on what happens when the system produces ambiguous or problematic results.\n",
    "\n",
    "Each of these perspectives highlights a different set of circumstantial concerns that emerge once a technical capability is placed inside an organization and exposed to real use.\n",
    "\n",
    "Your goal in these conversations is to engage with those concerns. This means:\n",
    "- explaining how the prototype behaves and performs,\n",
    "- articulating tradeoffs in plain, cross-functional language,\n",
    "- and reckoning with how technical choices intersect with human expectations, organizational processes, and downstream impact.\n",
    "\n",
    "Each conversation should feel like a real internal discussion. When a persona has what they need to understand your reasoning and its implications, the conversation will naturally come to a close.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of Act 3\n",
    "\n",
    "At this point, you're done! Make sure to submit the assignment on canvas.\n",
    "\n",
    "### Submission\n",
    "- Save the Notebook you have been working in and other files you created in your repo (i.e., agents.md, readme.md, etc).\n",
    "- Export your Copilot Chat and save as a .txt, .json, or .md in the same directory as the above.\n",
    "- **Upload your Notebook, agents.md, readme.md, and chat file to [the Canvas page for Assignment 2](https://canvas.northwestern.edu/courses/245397/assignments/1668981).**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
