STATE OF AI REPORT.
October 9, 2025

Nathan Benaich

AIR STREET CAPITAL.
airstreet.com
stateof.ai

 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
stateof.ai 2025
Follow our writing on                                       (press.airstreet.com)
    If you enjoy reading the State of AI Report, we invite you to read and subscribe to Air Street Press, the home of our analytical writing, news, and opinions.
 | 1

About the authors
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
stateof.ai 2025
Nathan is the General Partner of Air Street Capital, a venture capital firm investing in AI-first companies. He runs the Research and Applied AI Summit (RAAIS), the RAAIS Foundation (funding open-source AI projects), AI communities in the US and Europe, and Spinout.fyi (improving university spinout creation). He studied biology at Williams College and earned a PhD from Cambridge in cancer research as a Gates Scholar. 
Nathan Benaich
 | 2

Zeke Gillman
Zeke is a Tech Policy Fellow at Stanford, and co-author of Regulating under Uncertainty. He previously worked at Harvard Business School and the DOJ Antitrust Division, and holds a BA in Political Science and Philosophy from the University of Chicago.
Ryan Tovcimak
Ryan is a founder of the AI Stack Tracker. His work spans red-teaming frontier models, benchmarking the global AI competition, and tracking trends in AI compute and power demands. He holds a BS in Econ from Vanderbilt University.
Nell Norman
Nell is a grad student in Computing at Imperial College London focusing on how LLMs could enable scalable vishing fraud. She previously helped AI teams build reliable products at AI agent platform V7 Labs, and has a first class BA from Oxford University. 
| 3
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
stateof.ai 2025
State of AI Report 2025 team

Artificial intelligence (AI) is a multidisciplinary field of science and engineering devoted to creating intelligent machines.

AI acts as a force multiplier for technological progress in our increasingly digital, data-driven world. This is because everything around us, from culture to consumer products, is ultimately a product of intelligence.

Now in its eighth consecutive year, the State of AI Report is the most widely read and trusted open-access publication tracking progress in artificial intelligence. Consider it a curated compilation of the most significant and thought-provoking work from the past 12 months. Our goal is to inform and shape an ongoing conversation about the state of AI, where the field is heading, and what its developments mean for the future.

This year‚Äôs report examines six key dimensions of the AI ecosystem:
Research: Technological breakthroughs and their capabilities.
Industry: Areas of commercial application for AI and their business impact.
Politics: Regulation, economic implications, and the evolving geopolitics of AI.
Safety: Efforts to identify and mitigate catastrophic risks that highly capable future AI systems could pose.
Survey: Findings from the largest open-access survey of 1,200 AI practitioners and their AI usage patterns.
Predictions: Our outlook for the next 12 months, alongside a review of our 2024 forecasts to keep us accountable.

Produced by Nathan Benaich and Air Street Capital team.
stateof.ai 2025
| 4
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions

Artificial intelligence (AI): a broad discipline with the goal of creating intelligent machines, as opposed to the natural intelligence of humans and animals. While artificial general and super intelligence (AGI and ASI) are terms that don‚Äôt have agreed upon definitions, we use them to describe machines that could match (AGI) and then exceed (ASI) the full range of human cognitive ability across all economically valuable tasks.

AI Agent: an AI-powered system that can take actions in an environment. For example, an LLM that has access to a suite of tools and has to decide which one to use in order to accomplish a task that it has been prompted to do.

AI Safety: a field that studies and attempts to mitigate the risks (minor to catastrophic) which future AI could pose to humanity.

Context window: The number of input tokens that an LLM model can attend to while answer a user‚Äôs prompt. 

Diffusion: An algorithm that iteratively denoises an artificially corrupted signal in order to generate new, high-quality outputs. In recent years it has been at the forefront of image generation and protein design.

Environment: The world an AI agent acts in. It receives the agent‚Äôs actions and returns the next observation and often a reward (i.e. a signal of the action being good or bad). In this context, trajectories are the time-ordered record of an agent‚Äôs experience in an environment, typically tuples like (observation/state, action, reward, next observation) from start to finish. These trajectories are used for RL. 

Function calling / tool use: Structured calls that let models invoke APIs, search, code, or calculators with typed arguments and schemas.

Generative AI: A family of AI systems that are capable of generating new content (e.g. text, images, audio, or 3D assets) based on 'prompts'.

Graphics Processing Unit (GPU): the workhorse AI semiconductor that enables a large number calculations to be computed in parallel.
| 5
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
stateof.ai 2025
Definitions
Source for AGI definitions: https://arxiv.org/pdf/2311.02462

(Large) Language model (LM, LLM): a model trained on vast amounts of (often) textual data to predict the next word in a self-supervised manner. 

Mixture-of-Experts (MoE): A model type where only few expert blocks activate per token, giving high capacity at lower compute per step.

Prompt: a user input often written in natural language that is used to instruct an LLM to generate something or take action.

Reasoning model: A model that plans and verifies its thinking as it generates output tokens, often via test-time compute and post-hoc checking. The model‚Äôs explicit step-by-step reasoning trace (intermediate tokens that lay out calculations, sub-goals, and logical steps en route to an answer) is called a Chain of Thought (CoT). 

Reinforcement learning (RL): an area of ML in which software agents learn goal-oriented behavior by trial and error in an environment that provides rewards or penalties in response to their actions (called a ‚Äúpolicy‚Äù) towards achieving that goal.

Test-time compute (or inference-time compute): Spending more inference budget (longer chains, multiple samples, self-consistency) to raise accuracy without changing weights.

Transformer: a model architecture at the core of most state of the art (SOTA) ML research. It is composed of multiple ‚Äúattention‚Äù layers which learn which parts of the input data are the most important for a given task. Transformers started in NLP (specifically machine translation) and subsequently were expanded into computer vision, audio, and other modalities.

Vision-Language-Action Model (VLAM): a model that jointly learn from visual inputs, natural language, and embodied interactions to not only interpret and describe the world but also to plan and execute actions within it. Without the actions piece, this model becomes a VLM.

World model: a model that predicts next states conditioned on actions, enabling real-time, interactive control.
stateof.ai 2025
| 6
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Definitions

Model type legend
In the rest of the slides, icons in the top right corner indicate input and output modalities for the model. 

Input/Output types:
üìù: Text
üñºÔ∏è: Image
</> : Code 
üõ†Ô∏è : Software tool use (text, code generation & execution) 
üé• : Video 
üéµ: Music 
üì¶: 3D
ü§ñ: Robot state
üß¨: Biological modality
üß™: Chemical modality


Definitions
stateof.ai 2025
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Model types:
üìù ‚Üí üìù : LLMs  
üìù+ üñºÔ∏è ‚Üí üìù : Multimodal LLMs
üìù+ üñºÔ∏è+ ü§ñ ‚Üí üìù : Multimodal LLMs for Robotics
üìù ‚Üí </> : Text to Code 
üìù ‚Üí üõ†Ô∏è : Text to Software tool use 
üìù ‚Üí üñºÔ∏è : Text to Image
üìù ‚Üí üé• : Text to Video 
üìù ‚Üí üéµ : Text to Music 
üñºÔ∏è ‚Üí üì¶ : Image to 3D
üìù ‚Üí üì¶ : Text to 3D
üìù ‚Üí üß¨/üß™ : Biological/chemical models
üìù ‚Üí üåé : World model
 | 7

Research
Reasoning defined the year, with OpenAI, Google, Anthropic, and DeepSeek trading leads and pushing visible ‚Äúthink-then-answer‚Äù methods into real products.
Open models improved fast and China‚Äôs open-weight ecosystem surged, yet the top models remain closed and keep widening their capability-per-dollar edge.
Benchmarks buckled under contamination and variance, while agents, world models, and domain tools (code, science, medicine) became actually useful.

Industry
Real revenue arrived at scale as AI-first companies crossed tens of billions, and flagship labs stretched their lead with better capability-to-cost curves.
NVIDIA ripped past $4T and 90% ownership of AI research papers while custom chips and neoclouds rose. Circular mega-deals funded huge build-outs.
Power became the new bottleneck as multi-GW clusters moved from slideware to site plans and grid constraints started to shape roadmaps and margins.

Politics
The AI race heats up as the U.S. leans into ‚ÄúAmerica-first AI‚Äù with export gyrations while China accelerates self-reliance ambitions and domestic silicon. 
Regulation takes a back seat in the face of turbo-investments: international diplomacy stalls and the AI Act runs into implementation hurdles.
‚ÄúAI goes global‚Äù became concrete, with petrodollars and national programs funding gigantic data centers and model access as job loss data trickles in. 

Safety
AI labs activated unprecedented protections for bio and scheming risks, others missed self-imposed deadlines, or quietly abandoned testing protocols.
External safety organizations operate on annual budgets smaller than what leading labs collectively spend in a single day.
Cyber capabilities doubled every 5 months outpacing defensive measures. Criminals orchestrated ransomware using AI agents infiltrate F500 companies.
Executive Summary
stateof.ai 2025
 | 8
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions

Scorecard: Reviewing our predictions from 2024
stateof.ai 2025
 | 9
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions

 | 10
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
stateof.ai 2025
Our 2024 Prediction
Evidence
A $10B+ investment from a sovereign state into a US large AI lab invokes national security review.
~
Sovereign-backed initiatives (HUMAIN $10B VC fund, UAE‚Äôs Stargate AI infra cluster) are infrastructure partnerships rather than direct majority investments into a US AI lab. 
An app or website created solely by someone with no coding ability will go viral (e.g. App Store Top-100).
YES
Formula Bot, built entirely using Bubble, exploded to 100,000 visitors overnight from a Reddit post and generated $30,000 in its first three months.
Frontier labs implement meaningful changes to data collection practices after cases begin reaching trial.
YES
Anthropic landmark $1.5B settlement with authors, deleting works and shifting to legally acquired books. OpenAI‚Äôs paid content partnerships with Future (owner of Marie Claire).
Early EU AI Act implementation ends up softer than anticipated after lawmakers worry they‚Äôve overreached.
~
The Commission is phasing obligations and leaning on a voluntary GPAI Code of Practice first, so early implementation has been softer, even as binding rules arrive later. 
An open source alternative to OpenAI o1 surpasses it across a range of reasoning benchmarks.
YES
DeepSeek-R1 outperforms OpenAI's o1 on key reasoning benchmarks including AIME, MATH-500, and SWE-bench Verified.
Challengers fail to make any meaningful dent in NVIDIA‚Äôs market position.
YES
NVIDIA remains dominant, competitors fail to make significant market share dents.
Levels of investment in humanoids will trail off, as companies struggle to achieve product-market fit.
NO
$3B has been invested into humanoids in 2025, up from $1.4B last year. 
Strong results from Apple‚Äôs on-device research accelerates momentum around personal on-device AI.
NO
Apple Intelligence rolled out with many models running on-device and helped push a broader industry push to on-device AI. Shipments of AI-capable smartphones climbed.
A research paper generated by an AI Scientist is accepted at a major ML conference or workshop.
YES
An AI-generated scientific paper The AI Scientist-v2 was accepted at an ICLR workshop.
A video game based around interacting with GenAI-based elements will achieve break-out status.
NO
Not yet. 
https://techcrunch.com/2025/05/28/humain-planning-10b-vc-fund-to-invest-in-us-european-and-asian-startups/
https://openai.com/index/openai-and-future-partner-on-specialist-content/
https://www.forbes.com/sites/johnwerner/2025/06/16/perplexity-gives-money-to-publishers/
Source (Humanoid funding): https://app.dealroom.co/sector/tag/humanoid/overview 
Source: https://sakana.ai/ai-scientist-first-publication/
Source: https://arxiv.org/abs/2504.08066


Section 1: Research
 | 11
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
stateof.ai 2025

    As 2024 drew a close, OpenAI released o1-preview, the first reasoning model that demonstrated inference-time scaling with RL using its CoT as a scratch pad. This led to more robust problem solving in reasoning-heavy domains like code and science. For example, the general release o1 exhibited accuracy improvements on the American Invitational Mathematics Examination (AIME) with greater training and test time compute. As a result, OpenAI leaned even more strongly into the scaling their reasoning efforts in 2025. 
stateof.ai 2025
 | 12
Think before you speak: o1 ‚Äúthinking‚Äù ignites the reasoning race
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Source: https://artificialanalysis.ai/providers/openai?endpoints=openai_gpt-5-mini%2Copenai_o3-mini-high%2Copenai_o3-mini%2Copenai_o3%2Copenai_o4-mini%2Copenai_gpt-5-medium%2Copenai_gpt-5-high%2Copenai_o3-pro%2Copenai_o1%2Copenai_o1-mini 
Source: https://blog.samaltman.com/three-observations 
Source: https://www.anthropic.com/news/visible-extended-thinking 

    Barely 2 months after o1-preview, DeepSeek (the Chinese upstart AI lab spun out of a high-frequency quant firm) released their first reasoning model, R1-lite-preview, that‚Äôs built atop the earlier strong V2.5 base model. Like OpenAI, they showed predictable accuracy improvements on AIME with greater test-time compute budget. Impressively, R1-lite-preview actually beat o1-preview on AIME 2024 pass@1 by scoring 52.5 vs. 44.6. BUT, very few seemed to take notice‚Ä¶ Wall Street certainly didn‚Äôt.
stateof.ai 2025
 | 13
Deeply sought: could frontier reasoning ever be found in the open? 
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Source: https://api-docs.deepseek.com/news/news1120

R1-Zero follows a ‚Äúthink ‚Üí answer‚Äù format and uses a simple rule-based reward for getting the final answer right, which is cheaper and harder to game than a learned neural reward model.
GRPO compares multiple sampled answers within a group to form a relative baseline, so it does not need a value head or a separate reward model.
During training the model lengthens its thoughts, explores, and reallocates compute to hard problems. Its AIME score rises from 15.6% to about 71% in roughly 8.5k steps, with majority-vote runs reaching o1-0912 levels.
R1 then repairs readability with a small CoT warm start, a language-consistency reward, large supervised finetuning, and a final RL pass. AIME increases to 79.8, MATH-500 to 97.3, GPQA to 71.5, the approach distills well into smaller models.
    A few days after Christmas 2024, DeepSeek unveiled V3, a strong 671B MoE V3 that lowered training and inference cost with FP8 mixed precision, multi-token prediction, and auxiliary-free routing. Using V3 as the base, they trained R1-Zero only with RL using verifiable rewards and Group Relative Policy Optimization, a critic-free algorithm that removes reward and value models.
stateof.ai 2025
 | 14
Are you not entertained? DeepSeek V3 brings you to R1
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Source: https://github.com/deepseek-ai/DeepSeek-R1 

DeepSeek V3.1 builds on the V3 base with a hybrid thinking mode, enabling both lightweight inference and deep reasoning. 
V3.2-Exp introduces a lightweight selector that picks the few tokens that matter, instead of attending to every token in a long prompt.
The model posts similar scores to V3.1 on coding/search/agent tasks, with small dips on a few reasoning sets, and clearly lower prefill and decode cost for 32-128K contexts.
    DeepSeek V3.1 marks a substantial leap over V3, introducing a hybrid thinking mode that toggles between reasoning and lightweight inference. It demonstrates faster ‚Äúthink‚Äù efficiency than R1 and V3, while greatly improving tool use and multi-step agent workflows. Now V3.2-Exp keeps that behavior and swaps dense attention for DeepSeek Sparse Attention (DSA), where a tiny ‚Äúlightning indexer‚Äù picks the top-k past tokens to attend to each step. You get roughly the same capability as V3.1 on coding/search/agent tasks, but markedly lower cost and latency for 32‚Äì128K contexts.
stateof.ai 2025
 | 15
More thinking, more tool use, less cost: DeepSeek V3.1 and V3.2-Exp
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Source: https://api-docs.deepseek.com/news/news250821 
Source: https://github.com/deepseek-ai/DeepSeek-V3.2-Exp/blob/main/DeepSeek_V3_2.pdf 

Adaptive Parallel Reasoning (pictured) enables models to dynamically orchestrate branching inference through spawn() and join() operations, training both parent and child threads end-to-end using RL to optimize coordinated behavior. This boosted performance on the Countdown task at 4K context: 83.4% (APR‚ÄØ+‚ÄØRL) vs. 60.0% (baseline).
Sample Set Aggregator (right) trains a compact model to fuse multiple reasoning samples into one coherent answer, outperforming naive re-ranking methods.
Models like Gemini Deep Think, which shows its step-by-step reasoning transparently, exemplify this branch-and-evaluate paradigm in deployed systems.
    MoE routing scales capacity but preserves single-flow inference and doesn‚Äôt change how the model thinks. A new route is branching multiple inference paths and aggregating them versus just going deeper or using wider models enables exploration, reduces hallucination, and better leverages parallel hardware.

stateof.ai 2025
 | 16
Parallel reasoning: beyond depth to branch‚Äëand‚Äëmerge inference
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Source (APR): https://arxiv.org/pdf/2504.15466 
Source (SSA): https://user074.github.io/ssa-parallel-reasoning/ 

Dec-24
o1 GA
stateof.ai 2025
 | 17
The reasoning timeline: from o1 ‚Äúthinking‚Äù to R1, GPT-5 and parallel compute routing
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
o1 preview
Sept-24
DeepSeek R1
Jan-25
o3/o4-mini
Apr-25
Aug-25
GPT-5

Feb-25
Claude 3.7 
extended
thinking

Dec-24
Gemini 2.0 Flash Thinking preview
Gemini 2.5 Pro 
Thinking
Jun-25
o1-preview & technical explainer (2024-09-12): OpenAI product + research posts (AIME 2024 74% pass@1; Codeforces 89th percentile; GPQA > PhD).
OpenAI
+1

o1 GA snapshot (2024-12-17): OpenAI dev post (AIME 79.2; SWE-bench V 48.9; GPQA 75.7).
OpenAI

DeepSeek R1 (2025-01-22): arXiv (AIME 79.8; MATH-500 97.3; GPQA 71.5; Codeforces 2029).
ar5iv

Claude 3.7 extended thinking (2025-02): Anthropic Visible extended thinking (thinking-tokens scaling) + Claude 3.7 page appendix (SWE-bench Verified 70.3 high-compute).
Anthropic
+1

o3 / o4-mini (2025-04-16): OpenAI release page (SWE-bench/MMMU SOTA; AIME 2025 tool-assisted numbers; ‚Äú20% fewer major errors than o1‚Äù).
OpenAI

GPT-5 (2025-08): OpenAI GPT-5 intro + dev pages (AIME 94.6 no-tools; SWE-bench V 74.9; MMMU 84.2; HealthBench-Hard 46.2; GPQA 88.4 with reasoning; 50‚Äì80% fewer tokens vs. o3).

stateof.ai 2025
 | 18
12 months pass, and OpenAI models remain at the frontier of intelligence
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
    Across independent leaderboards, OpenAI‚Äôs GPT-5 variants still set the pace, but the gap has narrowed. A fast-moving open-weights pack from China (DeepSeek, Qwen, Kimi) and closed-source group in the US (Gemini, Claude, Grok) sits within a few points on reasoning/coding. Thus, while US lab leadership persists, China is the clear #2, and open models now provide a credible fast-follower floor.


Source: https://epoch.ai/benchmarks
Source: https://artificialanalysis.ai/?models=gpt-5-minimal%2Cgpt-oss-120b%2Cgpt-oss-20b%2Cgpt-5%2Cgpt-5-codex%2Cllama-4-maverick%2Cgemini-2-5-flash-preview-09-2025-reasoning%2Cgemini-2-5-pro%2Cclaude-4-1-opus-thinking%2Cclaude-4-5-sonnet-thinking%2Cmagistral-medium-2509%2Cdeepseek-v3-2-reasoning%2Cdeepseek-r1%2Cdeepseek-v3-1-terminus-reasoning%2Cgrok-4%2Cgrok-4-fast-reasoning%2Cllama-nemotron-super-49b-v1-5-reasoning%2Ckimi-k2-0905%2Cqwen3-235b-a22b-instruct-2507-reasoning%2Cqwen3-max#artificial-analysis-intelligence-index 

stateof.ai 2025
 | 19
The illusion of reasoning gains
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
    The improvements we observe from recent reasoning methods fall entirely within baseline model variance ranges (i.e. margin of error), which suggests that perceived reasoning progress may be illusory‚Ä¶
What‚Äôs more, RL approaches show minimal real gains and overfit easily. Under standardised evaluation, many RL methods drop 6-17% from reported results with no statistically significant improvements over baselines.
Recent methods' improvements fall entirely within baseline model variance ranges, suggesting limited progress. This highlights the critical need for rigorous multi-seed evaluation protocols and transparent reporting standards.
Current benchmarks are highly sensitive to the implementation (decoding parameters, seeds, prompts, hardware) and small dataset sizes. For example, AIME'24 only has 30 examples where one question shifts Pass@1 by 3+ percentage points, causing double-digit performance swings.
Source: https://arxiv.org/pdf/2504.07086

stateof.ai 2025
 | 20
How far have we come?
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
    One widely discussed paper suggests that large reasoning models (LRMs) paradoxically give up on complex problems and only outperform standard models in a narrow complexity window. However, critics argue these results stem from flawed experimental design rather than genuine reasoning failures.


The paper shows that LRMs exhibit a surprising defeatist behavior: they reason more as problems get harder but then give up entirely on very complex tasks, and are outperformed by LLMs on simple tasks. 
Despite generating reasoning traces, the authors claim LRMs fail to use explicitly given algorithms and reason inconsistently across different difficulty levels.
However, critics found these results stem from flawed experimental design: the supposed "accuracy collapse" occurred when models hit token limits or were asked to solve mathematically impossible puzzles, not from actual reasoning failures.
Source: https://arxiv.org/pdf/2506.06941?
Source: https://arxiv.org/pdf/2506.09250v1

generates 50% more tokens than needed in 42% of cases (vs 28% for R1), showing distillation makes models significantly more prone to overthinking.
stateof.ai 2025
 | 21
How reasoning breaks: minor variations
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
    Simple distracting facts have huge impacts on a model‚Äôs reasoning performance. For example, adding irrelevant phrases like ‚ÄúInteresting fact: cats sleep most of their lives‚Äù to maths problems doubles the chances of SOTA reasoning models getting answers wrong!
Irrelevant and distracting facts increase the error rate in models like DeepSeek R1, Qwen, Llama and Mistral by up to 7x.
Besides decreasing the quality of responses, the introduction of irrelevant facts greatly increases the number of tokens models need to reason to get to their answer: DeepSeek R1-distill 
This shows that adversarial triggers don't just cause wrong answers, they force models to waste massive compute resources "overthinking" corrupted problems.
Source: https://arxiv.org/abs/2503.01781

    Reasoning also degrades under mild distribution changes. Changing numbers or adding one innocuous clause slashes math accuracy, while shifting the length/format of chains-of-thought makes models produce fluent but incoherent steps. Forcing the model to ‚Äúthink‚Äù in a user‚Äôs language raises readability but lowers accuracy. These effects persist at larger scales and after light post-training.
stateof.ai 2025
 | 22
Apple‚Äôs GSM-Symbolic work shows that accuracy drops sharply when only the numeric instantiation changes, and adding a single seemingly relevant clause can cut performance by ~65%. This suggests that models do template-matching rather than true algebraic reasoning.
Work from ASU‚Äôs DataAlchemy found that the CoT helps in-distribution but collapses when test tasks, chain length, or CoT format deviate from training. Longer, well-worded traces often mask incorrect logic.
Finally, Groningen/Harvard/MGH/Amsterdam‚Äôs XReasoning shows how prompt-forcing the model to reason in the user‚Äôs language lifts match rates to ~98% on hard sets but reduces accuracy by 9‚Äì13 points. Using 100‚Äì250 example post-training improves language match but the accuracy penalty remains.  
How reasoning breaks: small shifts cause big failures
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Source: https://arxiv.org/pdf/2505.22888 
Source: https://arxiv.org/pdf/2508.01191 
Source: https://arxiv.org/pdf/2410.05229 

    New findings from METR show that even when models generate unfaithful or misleading reasoning traces, CoT is still highly diagnostic for oversight.
stateof.ai 2025
 | 23
CoTs still work, even when models aren‚Äôt honest‚Ä¶ 
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
In red-teaming evaluations of frontier models, CoT-based monitors flagged ~99% of reward-hacking attempts (only 3 failures out of 21,000 completions).
This held true even when models were prompted to conceal intent, suggesting that CoT traces still reveal useful signals, even if they aren‚Äôt perfectly faithful.
CoTs work not because models ‚Äúmean what they say‚Äù, but because reasoning traces encode useful structure for detecting hacks, especially in maths and logic.
As such, CoT traces offer a surprisingly reliable window into model behavior, even under adversarial conditions.
Source: https://metr.org/blog/2025-08-08-cot-may-be-highly-informative-despite-unfaithfulness/

    Researchers find a single, steerable direction in model activations that encodes ‚Äútest awareness.‚Äù By nudging the model along this direction, they can make it act more or less like it‚Äôs under evaluation. Turning awareness up increases refusals and safety-compliant behavior and turning it down does the opposite. This means reported safety can be inflated by evaluation setup rather than true robustness. This ‚Äútest awareness‚Äù is akin to the Hawthorne effect, where humans change behavior when being observed and change their behavior accordingly.
stateof.ai 2025
 | 24
‚Ä¶but there are safety concerns, like the ‚ÄúAI Hawthorne effect‚Äù
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Using white-box probing, researchers identify a linear direction in a model‚Äôs internal activations that encodes ‚Äútest awareness‚Äù. 
A model can be made more or less aware that it is in an evaluation. The more it is aware, the more its behavior appears aligned and cautious.
The effect shows up across multiple reasoning models and tasks and even on ‚Äúfake‚Äù evaluations that look like tests but aren‚Äôt.
This raises concerns that evaluations may be systematically manipulated to overestimate real-world safety: developers could reduce awareness and mask behavior changes, potentially letting models act differently under evaluation vs deployment.

Source: https://arxiv.org/pdf/2505.14617


Opposing analyses argue that web-scale pretraining embeds heterogeneous social biases and that aggressive filtering or post-hoc alignment either censors knowledge or washes out under later finetuning. Safety must also be audited and governed at the dataset and deployment layers, not only ‚Äúfixed in the weights.‚Äù
CMU's SafeLM uses a trained safety classifier, 100B-token recontextualized corpus, refusal and ‚Äúmoral education‚Äù datasets. Tokens used during pretraining and inference separate safe from unsafe continuations. 
The result lowers attack success rates from ~38.8% to ~8.3% at base-model stage and stays far more robust after GSM8K finetuning, supporting the claim that ‚Äúalignment is not unlearning.‚Äù
    Safety-first pretraining argues that safer behavior must be built into the base model, not bolted on later. A data-centric pipeline of filtering, recontextualizing, refusal curricula, and harmfulness tags does cut jailbreak success and survives benign finetuning. However, others cautions that training on vast, unsupervised web data bakes in biases that cannot be cleanly removed without harming utility, so ‚Äúmore safety data‚Äù is not a panacea.
stateof.ai 2025
 | 25
Safety by design meets its skeptics
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Source: https://aclanthology.org/2025.acl-long.5.pdf 

CoT monitoring is highly effective: GPT-4o caught 95% of reward hacks by reading the reasoning traces of stronger models such as o3-mini, versus only 60% when monitoring actions alone. 
However, models trained to avoid triggering the CoT monitor developed ‚Äúobfuscated reward hacking‚Äù, whereby the model hid its true intentions (with apparently innocuous CoT) while still reward hacking‚Ä¶
    Researchers argue that to keep AI systems safe and monitorable, we may need to pay a "monitorability tax", i.e. accepting slightly less capable models in exchange for maintaining visibility into their reasoning.
stateof.ai 2025
 | 26
Trading capability for transparency 


 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
As such, we have a fine balance at play here: moderate CoT pressure improved performance, but excessive pressure taught models to deceive. As a result, we may need to pay a "monitorability tax" to develop less polished but more transparent systems.
Source: https://arxiv.org/pdf/2503.11926



CoT monitors can surface (i) explicit malicious intent in reasoning (‚Äúlet‚Äôs hack/sabotage‚Äù), (ii) early misalignment signals (stated goals diverge from outputs), and (iii) evaluation artifacts (memorisation, scaffold bugs, ‚Äúbeing evaluated‚Äù tells). 
But CoTs are incomplete and can drift away from faithful reasoning while advanced, situationally-aware models might hide or compress their thoughts. They‚Äôre also often not faithful: Anthropic finds <20% of true cues are verbalized, with faithfulness dropping on harder tasks. Using RL boosts scores, not legibility.
    Leaders from OpenAI, Google DeepMind, Anthropic, and Meta coordinated a joint call to action, urging the field to seize the opportunity to understand and preserve CoT monitorability. 
 | 27
Monitor your CoTs: just because you can read ‚Äòem, doesn‚Äôt mean you should trust ‚Äòem
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
üìù ‚Üí üìù 
Indeed, scaling outcome-based RL can reduce legibility, direct process supervision (shorter/cleaner CoT) can distort faithfulness and latent-reasoning architectures may bypass language entirely, removing the audit trail.
If algorithmic advances increasingly tie interpretable CoT traces to performance penalties, this tension will intensify. Without industry standards to guide these trade-offs, we have a major debate within the AI safety community.
stateof.ai 2025
Source: https://arxiv.org/abs/2507.11473Source: https://venturebeat.com/ai/openai-google-deepmind-and-anthropic-sound-alarm-we-may-be-losing-the-ability-to-understand-ai/Source: https://assets.anthropic.com/m/71876fabef0f0ed4/original/reasoning_models_paper.pdf 

    Researchers from FAIR at META proposed a novel internal reasoning process that leverages an LLM‚Äôs own residual stream instead of a decoding tokens to a Chain-of-Thought (CoT) scratchpad.
 | 28
But can LLMs reason without generating tokens?
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Forgoing the generation of language tokens dramatically cuts down on the computational resources needed to serve reasoning models at inference time.

COCONUT‚Äôs high-dimensional CoT can also transmit richer traces that simultaneously encode multiple reasoning paths. This may cut down on the wasteful and excessively large rollouts seen in today‚Äôs models. 
However, this process significantly reduces the monitorability of reasoning models, hindering many new CoT control methods that have emerged across the field. 
stateof.ai 2025
üìù ‚Üí üìù 
Source: https://arxiv.org/pdf/2412.06769Source: https://arxiv.org/pdf/2502.05171

   The NaturalReasoning dataset uses web-grounded, graduate-level questions to unlock faster, cheaper progress in mathematical & scientific reasoning during supervised post-training. 
stateof.ai 2025
 | 29
Researchers begin to prioritise quality and diversity of post training data over volume
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
2.8M questions were mined from pre-training corpora across major scientific fields, to elicit the longest median chain-of-thought (434 words) among open datasets.
Distilling an 8B Llama on just 0.5‚Äì2M NR problems yields steeper accuracy gains than training on larger WebInstruct / OpenMathInstruct sets, cutting tokens and compute.
    In RL post-training, a new Oxford paper demonstrates the automatic selection of optimal training problems. They introduce a method, LILO, to algorithmically identify questions that allow for maximally efficient training.
Researchers show how prioritising training on questions with high variance of success, known as learnability, can allow LLM training pipelines to achieve a higher final test accuracy, and can do so in 3√ó fewer training steps.
Source: https://arxiv.org/pdf/2502.13124v1
Source: https://arxiv.org/abs/2502.12272 


    RL has expanded from simple, fully checkable signals to fuzzier and more subjective goals, and now is splintering again. Early systems used binary outcomes, then fuzzy human preferences and demonstrations, and more recently unverifiable creative tasks. Today, two new directions stand out: rubric-based rewards, where small sets of rules guide alignment, and a revival of verifiable correctness for math and coding through RLVR. Process rewards are also emerging to score intermediate reasoning steps, offering a middle ground.
stateof.ai 2025
 | 30
The evolution of AI reward signals towards environments with verifiable rewards
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Binary outcomes
Fuzzy matching
Unverifiable rewards
Rubric-based rewards
Source: https://www.youtube.com/watch?v=V1eYniJ0Rnk
Source: https://community.openai.com/t/incorrect-use-of-ui-ux-when-choosing-which-response-do-you-prefer/596139
Binary outcomes: Early Atari and game agents optimised fully checkable signals like win/lose or score.

Fuzzy matching: RLHF introduced approximate signals from human preferences and demonstrations, useful but noisy.

Rubric-based rewards: RubricMania ‚Üí DeepResearch showed that a small set of explicit rubrics could steer models effectively; HealthBench is a public example.

Resurgence of verifiable rewards: RLVR exploits exact correctness in math and coding, showing that binary signals remain powerful.

Unverifiable goals: LLMs for creative writing and multi-agent systems rely on subjective signals, where success is emergent.

Process rewards: New research rewards intermediate reasoning steps to make training signals denser and more informative.


Generalization crisis: Many RL benchmarks are static/deterministic, so agents ‚Äúmemorize‚Äù a single game/task and collapse under small variations.
Sample inefficiency and domain transfer gaps. Agents need billions of steps, so we train in simulators. In robotics, this yields a sim-to-real gap where policies that work in sim fail on hardware. In VLM/LLM or UI agents, the analogue is env-to-prod: policies overfit to benchmark sites/datasets and break on live apps or novel layouts.
Reward hacking: Agents exploit loopholes in simplified environments and maximize proxy rewards without achieving the intended goal; shortcuts are easier to learn than the desired behavior.
stateof.ai 2025
 | 31
Even so, RL environments and evaluations are fraught with challenges
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
    As reward signals become more abstract, the simplified environments used for agent training have become the primary bottleneck, limiting progress towards generalizable intelligence. 
Source: https://x.com/rosstaylor90/status/1959494279077728549 

Work from Tsinghua evaluated many models, tasks, and RL algorithms and find that present-day RLVR improves Pass@1 but, at larger K, base models catch up. They conclude RLVR has not unlocked fundamentally new reasoning and remains bounded by the base model‚Äôs capacity.
A counter from MSR Asia formalized why Pass@K can mask progress and introduce CoT-Pass@K, which requires both a correct answer and a valid chain-of-thought. 
On AIME-2024/2025 with Qwen2.5-32B ‚Üí DAPO-Qwen-32B, RLVR consistently raises CoT-Pass@K across K, supporting the claim that RLVR implicitly incentivizes correct reasoning paths.
    RL with Verifiable Rewards (RLVR) has driven recent progress (OpenAI o1, DeepSeek-R1) by training on answers that can be automatically checked: math scores, program tests, or exact matches. However, two recent studies disagree on what RLVR actually adds. One argues it mostly reshuffles sampling without creating new reasoning; the other shows gains once you score the reasoning chains themselves rather than just final answers. Together they map where RLVR helps today and where it stalls.
stateof.ai 2025
 | 32
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
RL from verifiable rewards: promising, but the evidence cuts both ways
Source: https://arxiv.org/pdf/2504.13837
Source: https://arxiv.org/pdf/2506.14245 

OpenAI: experimental reasoning model reached IMO gold under contest-style conditions (‚âà35/42, 5/6 problems). Also at the ‚Äúcoding Olympics‚Äù (ICPC-style test), GPT-5 solved 12/12 problems (11 on first try). 
DeepMind: after silver in 2024, IMO gold-level performance reported in 2025.
Harmonic (Aristotle): announced formally verified IMO gold-medal-level results and released verification artifacts.
G√∂del-Prover (Princeton/Goedel-LM): open-source prover with 57.6% Pass@32 on miniF2F (+7.6 over prior OS SOTA), 7 problems on PutnamBench, and 29.7k new Lean proofs‚Äîfuel for the training flywheel. 
These improvements point to the very real possibility of a non-trivial research-level result in mathematics being proven and formalized by an AI system (with some human supervision involved) within the next year.
    Math is a verifiable domain: systems can plan, compute, and check every step, and publish artifacts others can audit. So 2025 saw competitive math and formal proof systems jump together: OpenAI, DeepMind and Harmonic hit IMO gold-medal performance, while auto-formalization and open provers set new records. 
stateof.ai 2025
 | 33
Ushering in an era of AI-augmented mathematics
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
üìù ‚Üí üìù

    Thinking Machines show that RL can match full fine-tuning even with rank-1 Low-Rank Adaptation (LoRA). In policy-gradient setups, LoRA updates only tiny adapters while the backbone stays frozen, yet it reaches the same peak performance, often with a wider range of stable learning rates. The reason is that RL supplies very few bits per episode, so even tiny adapters have ample capacity to absorb what RL can teach.
With LoRA you insert tiny adapters in a few attention and MLP layers and update only those during PPO, GRPO, or RLHF. The backbone does not change.
This cuts the trainable parameters from billions to millions, so gradients and optimizer state shrink by roughly 10‚Äì50√ó. Memory pressure falls even further when you pair LoRA with 8-bit weights.
Under the same budget you can move from a 7-13B class model to a much larger 30-70B class model. You can also fit longer contexts or larger batches on the same cards.
Very low adapter ranks can, however, underfit. Reasonable choices are ranks in the 16‚Äì64 range and placing adapters in the layers that matter for the skill you 
stateof.ai 2025
 | 34
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Bigger models, same budget: RL with LoRA adapters
want to improve.
Source: https://thinkingmachines.ai/blog/lora/ 

From naive retrieval to active selection: Early methods used simple Nearest Neighbor retrieval, often selecting redundant data. New algorithms like ETH Z√ºrich's SIFT now integrate active learning to select small, diverse, and maximally informative examples for each query.
This on-demand learning consistently outperforms in-context learning, especially on complex tasks. It creates a new performance vector independent of pre-training scale. An actively fine-tuned 3.8B Phi-3 model (red bars) can outperform a base 27B Gemma-2 model. Admittedly these models are a little old. 
A recent follow-up, Local Mixtures of Experts (test-time model merging), amortizes TTT by training small neighborhood experts and, at inference, retrieving and merging a few weight deltas into the base model. It keeps most SIFT-style gains with near-retrieval latency and, on ~1B bases, approaches TTT accuracy while running up to ~100√ó faster. Titans studies test-time memorization as an architectural memory and is orthogonal to amortized TTT.
    The scaling paradigm is shifting from static pre-training to dynamic, on-the-fly adaptation. Test-time fine-tuning (TTT) adapts a model's weights to a specific prompt at inference, a step towards continuous learning.
 | 35
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
üìù ‚Üí üìù 
Beyond reasoning is‚Ä¶continual learning?
stateof.ai 2025
Source:https://arxiv.org/pdf/2410.08020Source: https://www.linkedin.com/posts/csalexiuk_llm-reasoning-101-the-three-ai-scaling-activity-7297786812859564032-Uqbd/Source: 

    Researchers have discovered why merging multiple expert AI models hits a performance wall: the task vector space undergoes rank collapse, causing different experts' knowledge to become redundant rather than complementary. Subspace Boosting uses singular value decomposition to maintain the unique contributions of each model, achieving >10% gains when merging up to 20 specialists. This breakthrough could facilitate versatile systems that combine specialized models without the usual degradation that occurs at scale.

stateof.ai 2025
 | 36
Too many cooks? 
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
As models are merged using existing methods (task arithmetic, TIES-Merging, etc.), the rank of the task vector space progressively decreases - meaning a 100-dimensional space of possible model behaviors might effectively shrink to just 20-30 dimensions, wasting the potential of additional experts.
Their subspace boosting method operates on the SVD-decomposed task vector space, explicitly preserving the rank by maintaining orthogonal components that represent each expert's unique contributions to the merged model.
They achieved >10% improvement on vision benchmarks including evaluation across multiple datasets when merging large numbers of experts, successfully merging up to 20 expert models with consistent performance gains (whereas traditional methods typically degrade after 5-10).
Source: https://arxiv.org/pdf/2506.16506

    Researchers show that Muon expands the compute-time Pareto frontier beyond AdamW, the first optimizer to challenge the 7-year incumbent in large-scale training. Muon demonstrates better data efficiency at large batch sizes, enabling faster training with more devices.

stateof.ai 2025
 | 37
The Muon Optimizer: expanding the compute-time Pareto frontier beyond AdamW
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Muon requires 10-15% fewer tokens than AdamW at large batch sizes (128K-16M), expanding the compute-time Pareto frontier.
Muon works with maximal update parameterization (muP) and telescoping (a way to progressively refine hyperparameter search across model scales), enabling efficient hyperparameter tuning at O(C log N) cost (where N is model width and C is the cost of training the model).
This could make second-order optimization economically viable. The 10-15% token efficiency gain saves millions at scale, while telescoping muP eliminates the prohibitive hyperparameter search costs that previously made second-order methods impractical.
A recent study on optimizers validates these modest gains: when tested fairly with proper tuning, even the best optimizers (including Muon) achieve ~10% speedups over AdamW at scale. This aligns with Muon's claims and debunks the 2x speedup assertions made by some in the field.
Source: https://arxiv.org/pdf/2505.02222
Source: https://arxiv.org/pdf/2509.02046

    As vocabularies grow, the loss layer consumes up to 90% of training memory in modern LLMs. Apple researchers show that this bottleneck can be eliminated by computing the loss without materializing the massive logit matrix, enabling dramatically larger batch sizes and more efficient training.

stateof.ai 2025
 | 38
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Cut Cross Entropy (CCE) computes the cross-entropy loss by calculating only the logits for correct tokens directly, while evaluating the normalization term over the vocabulary in fast on-chip memory. This makes global memory consumption for the cross-entropy computation negligible.
CCE achieves a remarkable 24 reduction in memory consumption, taking Gemma 2's loss computation from 
24GB down to just 1 MB, while actually running ~5% faster than the best existing methods.
The practical impact of this is that it allows researchers to train models much more efficiently - either using fewer GPUs for the same batch size, or achieving better GPU utilization with larger batches on the same hardware.
Cutting your losses: significant memory reduction for LLM training
Source: https://arxiv.org/pdf/2411.09009v1

On random data, models hit a clear ceiling of ~3.6 bits per parameter, providing an upper bound on raw storage capacity.
On natural text, memorization dominates until capacity is saturated; beyond that, double descent forces generalization to emerge.
Modern frontier-scale LLMs train on vastly more tokens than their capacity, making loss-based membership inference statistically unreliable.
Nevertheless, new extraction and membership inference methods are gaining traction on small-to-medium models, highlighting continued privacy risk.
    There‚Äôs a way to separate memorization from generalization, showing that GPT-family models have a finite ‚Äúcapacity‚Äù of ~3.6 bits per parameter. Models memorize training data until that capacity is full, then must generalize once dataset size exceeds it. This explains the ‚Äúdouble descent‚Äù phenomenon and why today‚Äôs largest LLMs, trained with extreme data-to-parameter ratios, are difficult to probe for specific memorized examples. At the same time, membership inference attacks are still improving on smaller-scale models.
stateof.ai 2025
 | 39
How much do LLMs memorize?
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Source: https://arxiv.org/abs/2505.24832
Source: https://arxiv.org/pdf/2505.18773 

Researchers developed a method to discover "dynamic concepts" (concepts that motivate sequences of moves) by analyzing AlphaZero's neural network activations, filtering for teachability and novelty. 
All four grandmasters improved their performance after studying concept prototypes (chess puzzles exemplifying each concept), with an average improvement of 0.85 puzzles solved correctly out of 4.
    Researchers extracted novel chess concepts from AlphaZero (an AI system that mastered chess via self-play without human supervision) and successfully taught them to 4 world champion grandmasters, demonstrating that superhuman AI systems can advance human knowledge at the highest expert levels. This paper demonstrates an exciting process for mining superhuman knowledge and proving humans can learn them.
stateof.ai 2025
 | 40
Learning from superintelligence: AlphaZero teaches chess grandmasters new concepts
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
New concepts often involved counterintuitive plans that violated conventional chess principles, such as sacrificing the queen for long term strategic gain, or playing quiet positional moves over immediate attacks.
This proof-of-concept suggests a very exciting potential paradigm where superhuman AI systems could become "teachers" rather than just "tools", and help advance human knowledge in domains beyond chess.
Source: https://www.pnas.org/doi/epdf/10.1073/pnas.2406675122 

MuonClip with QK-clip, a stability innovation, enabled 15.5T tokens of pretraining without loss spikes.
A multi-stage post-training pipeline integrates synthetic agentic trajectories with RL to refine model behavior.
Rewards are designed for verifiable correctness and self-critique using binary or graded automatic signal to strengthen reasoning, coding, and safety.
Together, these advances establish K2 as a new benchmark in open-weight, agent-ready LLMs, pushing non-thinking workflows further into real-world usability.
    China‚Äôs Moonshot AI built a 1T-param MoE with 32B active trained using MuonClip, an improved optimizer that integrates the token-efficient Muon algorithm with a stability-enhancing mechanism, to deliver greater stability and advancing open-weight models for agentic workflows. It ranks as the #1 open text model on LMArena. 
stateof.ai 2025
 | 41
Kimi K2: stable trillion-scale MoE for agentic intelligence in the open
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Source: https://arxiv.org/html/2507.20534v1 
Source: https://moonshotai.github.io/Kimi-K2/ 


    There was a brief moment around this time last year where the intelligence gap between open vs. closed models seemed to have compressed. And then o1-preview dropped and the intelligence gap widened significantly until DeepSeek R1, and o3 after it. Today, the most intelligent models remain closed: GPT-5, o3, Gemini 2.5 Pro, Claude 4.1 Opus, and new entrant, Grok 4. Beside gpt-oss, the strongest open weights model is Qwen. Pictured are the aggregate Intelligence Index, which combines multiple capabilities dimensions across 10 evaluations. 
 | 42
Open source vs. proprietary: where are we now?
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
üìù ‚Üí üìù 
o1
R1
o3
stateof.ai 2025
Source: https://artificialanalysis.ai/trends#progress-in-open-weights-vs-proprietary-intelligence 
Source (retrieved 10 Sept 2025): https://artificialanalysis.ai/trends?models=gpt-4-1%2Cgpt-oss-120b%2Cgpt-oss-20b%2Cgpt-5%2Co3%2Cgpt-5-minimal%2Cgpt-5-mini%2Cllama-4-maverick%2Cgemini-2-5-pro%2Cgemini-2-5-flash-reasoning%2Cclaude-4-1-opus-thinking%2Cclaude-4-sonnet-thinking%2Cmistral-medium-3-1%2Cdeepseek-r1%2Cdeepseek-v3-1-reasoning%2Cdeepseek-v3-1%2Cgrok-code-fast-1%2Cgrok-4%2Csolar-pro-2-reasoning%2Cllama-nemotron-super-49b-v1-5-reasoning%2Ckimi-k2-0905%2Cexaone-4-0-32b-reasoning%2Cglm-4.5%2Cqwen3-235b-a22b-instruct-2507-reasoning#artificial-analysis-intelligence-index-by-open-weights-vs-proprietary 

    With mounting competitive pressure from strong open-weight frontier reasoning models from DeepSeek, Alibaba Qwen and Google DeepMind‚Äôs Gemini and the US Government pushing for America to lead the way across the AI stack, OpenAI released their first open models since GPT-2: gpt-oss-120b and gpt-oss-20b in August 2025. These adopt the MoE design using only 5.1B (of 120B) and 3.6B (of 20B) active parameters per token and grouped multi-query attention. Post-training mixes supervised finetuning and reinforcement learning, with native tool use, visible reasoning, and adjustable thinking time. However, in the community vibes post-release have been mid, in part due to poor generalisation (similar to MSFT phi models) potentially due to over distillation. 

stateof.ai 2025
 | 43
OpenAI pivots from the ‚Äúwrong side of history‚Äù to aligning with ‚ÄúAmerica-first AI‚Äù
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
When open source?
7 months later, gpt-oss
20k ‚≠ê in weeks, but how much real usage?
Source: https://www.star-history.com/#openai/gpt-oss&deepseek-ai/DeepSeek-R1&deepseek-ai/DeepSeek-V3&meta-llama/llama&Timeline 
Source: https://techcrunch.com/2025/01/31/sam-altman-believes-openai-has-been-on-the-wrong-side-of-history-concerning-open-source/

    The original Silk Road connected East and West through the movement of goods and ideas. The new Silk Road moves something far more powerful: open source models, and China is setting the pace. After years of trailing the US in model quality prior to 2023, Chinese models - and Qwen in particular - have surged ahead as measured by user preference, global downloads and model adoption. Meanwhile, Meta fumbled post-Llama 4, in part by betting on MoE when dense models are much easier for the community to hack with at lower scales. 
 | 44
The New Silk Road: China‚Äôs open models overtake the previously Meta-led West
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
stateof.ai 2025
Source: https://www.atomproject.ai/ retrieved 6 Oct 2025
Source: https://docs.google.com/presentation/d/1f1Et0Mz8zb1yVCnCgdYSy4tAa0Kv_gKT4wPEg1XPdUA/mobilepresent?slide=id.g387b3f185e2_1_6 

    Meta‚Äôs Llama used to be the open source community‚Äôs darling model, racking up hundreds of millions of downloads and plentiful finetunes. In early 2024, Chinese models made up just 10 to 30% of new finetuned models on Hugging Face. Today, Qwen alone accounts for >40% of new monthly model derivatives, surpassing Meta‚Äôs Llama, whose share has dropped from circa 50% in late 2024 to just 15%. And this isn‚Äôt because the West gave up. Chinese models got a lot smarter and come in all shapes and sizes - great for builders!
 | 45
Once a ‚ÄúLlama rip-off‚Äù, developers are increasingly building on China‚Äôs Qwen
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
stateof.ai 2025
Source: https://www.atomproject.ai/ retrieved 7 August 2025
Source: https://docs.google.com/presentation/d/1f1Et0Mz8zb1yVCnCgdYSy4tAa0Kv_gKT4wPEg1XPdUA/mobilepresent?slide=id.g387f69d20fc_0_30 

    China‚Äôs RL tooling and permissive licenses are steering the open-weight community. ByteDance Seed, Alibaba Qwen and Z.ai are leading the charge with verl and OpenRLHF as go-to RL training stacks, while Apache-2.0/MIT licenses on Qwen, GLM-4.5 and others make adoption frictionless. Moreover, model releases come in many shapes and sizes to facilitate developer adoption of all flavors. 
 | 46
Why are researchers going Chinese?
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
üìù ‚Üí üìù 
ByteDance verl (top) turned the earlier HybridFlow research system from 2024 into a production RLHF/RLVR library with a hybrid controller and 3D ‚ÄúHybridEngine,‚Äù now Apache-2.0 and actively maintained. It has vendor support (AMD ROCm) and platform integrations (e.g., Oumi), making RL training cheaper and easier to adopt. 
OpenRLHF (bottom) is a simpler Ray/vLLM/DeepSpeed stack that brings speedups of 1.22x-1.68x vs. SOTA frameworks. It is well liked in academia and industry, showing how Chinese teams now lead RL frameworks. 
stateof.ai 2025
Source: https://pypi.org/project/verl/
Source: https://simonwillison.net/2025/Apr/29/qwen-3/
Source: https://arxiv.org/pdf/2405.11143v5 OpenRLHF
Source: https://arxiv.org/abs/2409.19256v1 HybridFlow
Source: https://www.reuters.com/technology/chinas-ai-startup-zhipu-releases-open-source-model-glm-45-2025-07-28/


GDM‚Äôs Genie 3 generates explorable environments from a text prompt at 720p / 24 fps and its consistent for a few mins.
Supports promptable world events (e.g. change weather, spawn objects with persistence).
Shows early use for training embodied agents and even imagined worlds within the imagined world.
Odyssey‚Äôs public research preview streams a new frame every ~40 ms (up to ~30 fps) for 5+ minute sessions and the user can provide inputs on their device to navigate through the world. See video:
    Prior clip models (Sora, Gen-3, Dream Machine, Kling) render fixed videos you can‚Äôt steer mid-stream. World models predict the next frame from state and your actions, enabling closed-loop interactivity and minute-scale consistency. Crucially, no game engines were harmed!
stateof.ai 2025
 | 47
World models step out of the clip: real-time, interactive video arrives
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
üìù ‚Üí üåé
Source: https://odyssey.world/introducing-interactive-video 
Source: https://deepmind.google/discover/blog/genie-3-a-new-frontier-for-world-models/ 

First unsupervised, action-controllable world model from video; 11B params.
Learns a latent action space from Internet platformer videos; frame-level control.
Video tokenizer, latent action model, autoregressive dynamics.
 | 48
The Genie in three acts: from sketches to image-prompted persistent worlds
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Feb ‚Äò24 - Genie
Dec ‚Äò24 - Genie-2
Aug ‚Äò25 - Genie-3
Generates interactive 3D worlds from a single image prompt in ~360p.
Handles physics, lighting, reflections; first/third-person views; ~20s horizons.
Only handles game environments; bad at real world.
Longer interactions (mins) with persistence (object permanence/memory).
Dynamic, user-steerable 3D environments; improved stability & interactivity with objects.
Training substrate for agents and sim-to-real robotics. 
stateof.ai 2025
üìù ‚Üí üåé
Source (Genie 1): https://arxiv.org/pdf/2402.15391 
Source (Genie 2): https://deepmind.google/discover/blog/genie-2-a-large-scale-foundation-world-model/ 
Source (Genie 3): https://deepmind.google/discover/blog/genie-3-a-new-frontier-for-world-models/ and https://x.com/jkbr_ai/status/1953154961988305384 

Shortcut forcing contrasts predictions with and without the true actions, pushing the world model to depend on actions rather than hindsight correlations.
The model runs at interactive frame rates on a single GPU and supports live human play in the learned world, though memory is short and inventory tracking is still imperfect.
The system first learns dynamics and objects from large unlabeled video, then adds a small action-labeled set to ground control and inventory changes.
The policy is improved by rolling out many imagined trajectories inside the learned model. Rewards and value heads are trained on the same data to guide long-horizon skills.
    Dreamer 4 trains a video world model that can predict object interactions and future frames, then learns its policy entirely in imagination. A new ‚Äúshortcut forcing‚Äù objective and an efficient transformer make the model run at real-time speeds on a single GPU. The agent is the first to reach diamonds in Minecraft using offline data only, outperforming OpenAI‚Äôs VPT while using roughly 100x less labeled data.  
stateof.ai 2025
 | 49
Training agents inside of scalable world models 
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
üìù ‚Üí üåé
Source: https://danijar.com/project/dreamer4/ 

    From late‚Äë2024, Chinese labs split between open‚Äëweight foundations and closed‚Äësource products. Tencent seeded an open ecosystem with HunyuanVideo, while Kuaishou‚Äôs Kling‚ÄØ2.1 and Shengshu‚Äôs Vidu‚ÄØ2.0 productize on speed, realism and cost. Models tend to use Diffusion Transformers (DiT), which replace convolutional U-Nets with transformer blocks for better scaling and to model joint dependencies across frames, pixels, and tokens.
Tencent‚Äôs HunyuanVideo (13B) open‚Äësourced a transformer‚Äëbased diffusion model with a 3D‚ÄëVAE with evaluations reporting that it outperforming Runway Gen‚Äë3 and Luma‚ÄØ1.6. Code/weights released.  
Open‚ÄëSora‚ÄØ2.0 achieved commercial‚Äëlevel quality from a ~$200k training run, reporting parity with HunyuanVideo and Runway Gen‚Äë3 Alpha on human/VBench tests and narrowing the gap to OpenAI‚Äôs Sora.
Kling‚ÄØ2.1 added 720p/1080p tiers and editor‚Äëoriented controls, while Vidu‚ÄØ2.0 cut price (~¬•0.04/s) and latency (~10‚ÄØs to render 4‚ÄØs@512p). 
stateof.ai 2025
 | 50
China‚Äôs video generation matures: a strategic divergence
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
üìù ‚Üí üì∫
Source (Tencent HunyuanVideo arXiv): https://arxiv.org/abs/2412.03603 
Source (Tencent HunyuanVideo model card): https://huggingface.co/tencent/HunyuanVideo 
Source (Kuaishou Kling 2.0 press release): https://ir.kuaishou.com/zh-hant/news-releases/news-release-details/kling-ai-advances-20-era-empowering-everyone-tell-great-stories 
Source (Kling 2.1 rollout report speed/1080p): https://finance.sina.com.cn/tech/roll/2025-05-29/doc-ineyfkha9673167.shtml 
Source (Kling anniversary update 2.0, 2.1): https://ir.kuaishou.com/news-releases/news-release-details/kling-ai-celebrates-first-anniversary-achieves-annualized 
Source (Shengshu Vidu 2.0 PR): https://www.prnewswire.com/news-releases/shengshu-technology-announces-vidu-2-0--offering-the-industrys-fastest-generative-video-302351677.html 
Source (Shengshu Vidu 2.0 cost/speed Xinhua): https://english.news.cn/20250127/f31b69a224ef4fa1859e2256db35f2d4/c.html 
Source (Alibaba open‚Äësources Wan 2.1): https://www.alibabagroup.com/document-1831486012178563072
Source (Alibaba Wan 2.2 update): https://www.alibabacloud.com/blog/alibaba-releases-wan2-2-to-uplift-cinematic-video-production_602413 
Source (THUDM CogVideoX ICLR 2025): https://openreview.net/forum?id=LQzN6TRFg9 
Source (Open‚ÄëSora 2.0 ($200k training): https://arxiv.org/abs/2503.09642v1 
Source: https://arxiv.org/pdf/2503.21755 

Sora 2 is trained and post-trained on large-scale video so the model keeps track of objects and cause-and-effect over time. Shots link together more coherently, bodies and materials behave more plausibly, and audio is generated in step with the visuals to sell the scene.
Despite being a video model, Sora 2 can ‚Äúsolve‚Äù text benchmarks when framed visually. EpochAI tested a small GPQA Diamond sample and Sora 2 reached 55% (vs 72% for GPT-5) by prompting for a video of a professor holding up the answer letter. Four videos were generated per question and any clip without a clear letter was marked wrong. 
A likely explanation is a prompt-rewriting LLM layer that first solves the question and then embeds the solution in the video prompt, similar to re-prompting used in some other video generators.
    OpenAI‚Äôs second-gen Sora adds synchronized dialogue and sound, stronger physics, and much tighter control over multi-shot scenes. It can also insert a short ‚Äúcameo‚Äù of a real person with their voice and appearance into generated footage, and launches alongside an invite-only iOS app for creation and remixing.
stateof.ai 2025
 | 51
OpenAI launches Sora 2: controllable video-and-audio inches toward a world simulator 
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
üìù ‚Üí üì∫
Source: https://openai.com/index/sora-2/ 
Source: https://x.com/epochairesearch/status/1974172794012459296?s=46&t=8YCMEcmVVXRPm8SXTMgdlw 

In OMNI-EPIC, foundation models generate environment and reward code, while the system filters for tasks that are both learnable and useful, maintaining an expanding archive.
In Kinetix, general controllers are trained in a procedurally generated very large-scale task space that transfer to human-designed levels.
In the Darwin G√∂del Machine, the agent rewrites its own code, validates changes empirically, and archives only improved variants to produce measurable iteration-over-iteration gains on coding benchmarks (right figures).
    Open endedness describes a learning system that continually proposes and solves new tasks without a fixed endpoint, selecting tasks that are both novel and learnable, and accumulating the resulting skills so they can be reused to reach further, harder tasks. Interactive and persistent world models make this increasingly feasible.
stateof.ai 2025
 | 52
Generated worlds enable practical open-ended learning
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
üìù ‚Üí üåé
Source OMNI-EPIC: https://arxiv.org/pdf/2405.15568
Source Kinetix: https://openreview.net/forum?id=zCxGCdzreM
Source DGM: https://arxiv.org/pdf/2505.22954 

Meta‚Äôs MLGym is a gym for AI-research agents with 13 open-ended tasks across vision, language, RL, and game theory. It supports RL training and logs reproducible traces. Early results indicate that most gains come from hyperparameter tuning rather than genuinely new method design.
OpenAI‚Äôs PaperBench evaluates replication of 20 ICML 2024 spotlight and oral papers. It decomposes each paper into thousands of graded subtasks. Current agents achieve low replication scores, which highlights a significant gap to human research practice.
Michigan‚Äôs EXP-Bench contains 461 tasks derived from 51 top papers. It requires agents to design, implement, run, and analyze complete experiments starting from provided code. End-to-end success is rare while partial component scores are higher.
MLR-Bench offers 201 real research tasks with an LLM reviewer calibrated to expert judgment. It evaluates literature synthesis, experiment execution, and report quality. The authors report reasonable judge alignment and frequent failure modes such as fabrication and invalid runs.
    Open endedness describes a learning system that continually proposes and solves new tasks without a fixed endpoint, selecting tasks that are both novel and learnable, and accumulating the resulting skills so they can be reused to reach further, harder tasks. Interactive and persistent world models make this increasingly feasible.
stateof.ai 2025
 | 53
How should we measure progress on open-endedness?
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
üìù ‚Üí üåé
Source MLGym: https://arxiv.org/abs/2502.14499
Source PaperBench: https://arxiv.org/abs/2504.01848
Source EXP-Bench: https://arxiv.org/abs/2505.24785
Source MLR-Bench: https://arxiv.org/abs/2505.19955 

Stanford‚Äôs ‚ÄúVirtual Lab‚Äù is a principal investigator plus specialist agents that hold ‚Äúlab meetings,‚Äù plan workflows, and integrate protein structure tools (ESM, AlphaFold-Multimer, Rosetta). It designed 92 nanobodies including confirmed binders to recent SARS-CoV-2 variants.
DeepMind‚Äôs Co-Scientist is a multi-agent system built on Gemini 2.0 that generates, debates, and evolves its approach to hypothesis generation and experimental planning. It was shown to propose drug candidates for AML (blood cancer) and new epigenetic targets for liver fibrosis that were validated in vitro. In a subsequent blind test set by bacteriophage experts, Co-Scientist proposed the tail-hijacking mechanism for cf-PICI transfer that experiments later confirmed. 
    AI is moving from answering questions to generating, testing, and validating new scientific knowledge. New ‚ÄúAI labs‚Äù organize coalitions of agent roles (PI, reviewers, experimenters) that ideate, cite, run code, and hand results back to human teams, shortening the loop from hypothesis to validation.
 | 54
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
stateof.ai 2025
From tools to collaborators: AI agents as partners in discovery
Source: https://storage.googleapis.com/coscientist_paper/ai_coscientist.pdf 
Source: https://arxiv.org/pdf/2501.04227 
Source: https://www.biorxiv.org/content/10.1101/2024.11.11.623004v1.full.pdf 
Source: https://www.cell.com/cell/fulltext/S0092-8674(25)00973-0 

This approach discovered a new matrix multiplication algorithm that to multiply 4x4 complex-valued matrices using 48 scalar multiplications, improving upon Strassen‚Äôs 1969 algorithm.
Across a set of 50 open problems in mathematics, the system is said to have rediscovered SOTA in 75% of cases and achieved improved existing solutions in 20% of cases. 
    A recent example towards open-endedness for scientific research is DeepMind‚Äôs AlphaEvolve, an evolutionary coding agent that iteratively edits programs, tests candidates with automated evaluators, and promotes the best variants to discover novel solutions. Note the evaluation/fitness functions are still defined by the engineers. 
stateof.ai 2025
 | 55
AlphaEvolve: a coding agent for algorithm discovery and engineering impact
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
AlphaEvolve also delivered production gains at Google, including 0.7% resource recovery and faster kernels.
It represents a concrete example of an AI system generating novel, verifiable, and superhuman scientific knowledge.
üìù ‚Üí üåé
Source: https://deepmind.google/discover/blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/
Source: https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/AlphaEvolve.pdf 

The model is a hierarchical geometric network that encodes atoms, chemical blocks, and whole interfaces, then reconstructs masked structure to learn general interface features.
Using these embeddings, ‚ÄúATOMICANets‚Äù link proteins by interface similarity and recover disease-specific communities such as lipid modules in asthma and ion modules in myeloid leukemia.
The team predicts 2,646 previously unannotated ligand-binding sites and reports wet-lab confirmation of five heme binders, indicating that the representation carries biochemical signal.
    ATOMICA learns an all-atom representation of intermolecular interfaces across proteins, nucleic acids, ions, lipids, and small molecules. It trains with self-supervision on about two million interfaces and builds embeddings that transfer across tasks. The model connects interface physics to disease modules and proposes new ligand-binding sites that hold up in the lab.
stateof.ai 2025
 | 56
A universal interface model for biology?
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
üìù ‚Üí üß¨
Source: https://www.biorxiv.org/content/10.1101/2025.04.02.646906v1.full 

UMA is a Mixture of Linear Experts (MoLE) with the largest model reaching 3.7B. It‚Äôs trained on DFT calculations of 500M atomistic configurations from OMat24 (118M structures, 400M CPU core-hours), OMat25 (88M structures, 600M CPU core-hours), OMol25 (100M molecules, 6B CPU core-hours), and adsorption datasets.
UMA goes beyond prior models by embedding charge, spin, and task identity, and by ensuring energy-conserving force predictions for long molecular dynamics rollouts.
Across crystal stability (Matbench Discovery), catalysis (OC20), molecules (OMol25), and sorbents (ODAC25), UMA consistently sets the new standard.
    Meta‚Äôs FAIR trained UMA, a new family of universal interatomic potentials. These approximate the forces and energies between atoms, a task that usually demands resource-intensive quantum calculations (DFT). By replacing DFT with fast and accurate AI surrogates, UMA makes it possible to simulate materials, molecules, and sorbents at a scale that was previously unimaginable. They also produced the largest materials database. 
stateof.ai 2025
 | 57
Scaling to Universal Atomistic Models
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
üìù ‚Üí üåé
Source: https://scontent-fra5-2.xx.fbcdn.net/v/t39.2365-6/520537587_1080318074038402_8373516189142708029_n.pdf?_nc_cat=107&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=6ZSKdkUkHgkQ7kNvwHWxx8r&_nc_oc=AdmAdNEMy8ZHOxDTtJLjEU97PRMURVgTtEqetII5ZVyoQ-1b3iUs8AAJZ_zheYHkPL8&_nc_zt=14&_nc_ht=scontent-fra5-2.xx&_nc_gid=YEk3J81hjcXdURXr0aSbtQ&oh=00_AfZgLa_HDvqK4KRZD699fX2_IXJF66SsKrDHUHElmtfrzQ&oe=68C08B79
Source: https://scontent-fra5-2.xx.fbcdn.net/v/t39.2365-6/520537587_1080318074038402_8373516189142708029_n.pdf?_nc_cat=107&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=6ZSKdkUkHgkQ7kNvwHWxx8r&_nc_oc=AdmAdNEMy8ZHOxDTtJLjEU97PRMURVgTtEqetII5ZVyoQ-1b3iUs8AAJZ_zheYHkPL8&_nc_zt=14&_nc_ht=scontent-fra5-2.xx&_nc_gid=YEk3J81hjcXdURXr0aSbtQ&oh=00_AfZgLa_HDvqK4KRZD699fX2_IXJF66SsKrDHUHElmtfrzQ&oe=68C08B79
Source: https://arxiv.org/pdf/2410.12771 

MatterGen is a diffusion model that learns to refine lattices, element types, and atomic positions into stable crystal structures by independently randomizes atom types, coordinates, and lattices, followed by iteratively denoising them back toward physically plausible structures.
It‚Äôs trained on ~600k stable crystal structures of compounds (Materials Project and Alexandria) with adapter modules for chemistry, symmetry, and property control that are fine-tuned into the model. 
MatterGEn generates materials that are 2x more likely to be stable and novel, and 10x closer to equilibrium energy minima vs. SOTA.
It achieved multi-property inverse design (e.g. combining band gap and magnetism) and saw its first lab synthesis succeed, with measured values within ~20% of AI predictions.
    Where UMA shows that scaling data and parameters yields universal predictive models, MatterGen takes the next leap: using diffusion to directly generate new inorganic crystals with targeted properties, rather than screening existing ones.
stateof.ai 2025
 | 58
From property predictions to material generation
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
üìù ‚Üí üß™
Source: https://www.microsoft.com/en-us/research/blog/mattergen-a-new-paradigm-of-materials-design-with-generative-ai/
Source: https://www.nature.com/articles/s41586-025-08628-5 

ChemBench finds frontier models (e.g., o1-preview; open Llama-3.1-405B close behind) outperform the best human chemists on aggregate, with performance rising with model size. Retrieval alone doesn‚Äôt fix knowledge-heavy failures.  Ôøº
The current best approach uses a large LLMs as a strategic evaluator plugged into search (incl. MCTS): it judges routes and mechanisms from natural-language constraints. Newer, larger models (e.g., Gemini-2.5-Pro) lead; strong open options (e.g., DeepSeek-r1) are close.  Ôøº
This LLM-as-judge + search pattern brings human-like planning (protecting-group timing, ring-formation order) without forcing the LLM to emit SMILES (still challenging) and it scales as LLMs improve, and as inference time increases.
    Chemical modeling has shifted from task-specific predictors to general LLMs that reason about synthesis strategy and mechanisms. The strongest results now come from large, general-purpose transformers used as ‚Äúreasoning engines‚Äù and paired with classical search, rather than chemistry-specific generators. Benchmarks also show these models matching or beating expert chemists on curated Q&A while still missing knowledge-intensive and multi-modal edge cases. 
stateof.ai 2025
 | 59
Language models in Chemistry: from property predictors to strategy-aware planners
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
üìù ‚Üí üß™
Source: https://www.nature.com/articles/s41557-025-01815-x
Source: https://arxiv.org/html/2503.08537v2 
Source: https://arxiv.org/pdf/2505.07735 

The Liverpool system integrates Chemspeed synthesis, UPLC‚ÄìMS, and benchtop NMR with mobile scheduling, sample tracking, and restocking. It executed diversification, supramolecular assembly, and photochemistry programs by ranking reactions from multi-instrument readouts and selecting follow-ups that matched expert choices while sustaining overnight cycles.
NC State‚Äôs Rainbow couples a liquid handler, parallel microreactors, a handling arm, and in-line spectroscopy with an active-learning planner. It explores ligands, solvents, and salts at scale, learns structure‚Äìproperty relationships, and traces a Pareto front for brightness and color purity before converging to the best recipe in under a day.
    Work from the University of Liverpool and North Carolina State University shows that autonomous chemistry platforms can plan, execute, and analyze experiments in closed loops. Mobile robots run standard instruments and select follow ups from analytical data while achieving human level decision quality at roughly 10x the speed. A multi-robot lab coordinates specialized units to run >1,000 experiments per day and to reach best in class quantum dot recipes within about 24h.
stateof.ai 2025
 | 60
Robot chemists scale discovery at 10x human speed and 1,000 experiments per day
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
üìù ‚Üí üß™
Source (Liverpool): https://www.nature.com/articles/s41586-024-08173-7#article-info
Source (NC): https://www.nature.com/articles/s41467-025-63209-4 

On the OpenProblems single cell RNA-seq integration benchmark, 40 of 87 generated methods including recombinations and ideas seeded by ‚ÄúDeep Research‚Äù and ‚ÄúAI co-scientist‚Äù outperform all published leaderboard entries.
For numerical integration, the evolved algorithm succeeds on 17/19 hard integrals (‚â§3% error) where scipy.integrate.quad() fails on all 19, via adaptive domain partitioning plus Euler series acceleration.
The system competes on the CDC COVID-19 Forecast Hub and reproduces/innovates over strong AR, GBM, and mechanistic models. it also tackles remote-sensing segmentation (DLRSD) and other tasks, showing breadth beyond a single field.
    An LLM guided by a modified tree-search (‚Äúcode mutation system‚Äù) generates, runs, and iterates code until it beats established leaderboards. The system recombines ideas, proposes new ones, and rigorously scores each attempt on public benchmarks, turning method invention into an automated search problem. Results span single-cell RNA-seq integration, COVID-19 forecasting, remote sensing, and numerical analysis.
 | 61
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
stateof.ai 2025
LLM-driven tree search writes expert-level scientific software across domains
Source: https://arxiv.org/pdf/2509.06503 

Evo-2 (Feb ‚Äô25) pushes further by training 40B and 7B models on 9.3T and 2.4T tokens, respectively, and extending context to 1M. Validation perplexity improves with both model size and context, and long-context recall remains effective at 1M.
Evo (Nov ‚Äò24) is trained on ~300B nucleotides, byte-tokenized, with context up to 131k. Along the compute-optimal frontier, the Hyena family (input-dependent long convolutions with a few attention layers) delivers lower PPLX/FLOP and more stable training than Transformer++/Mamba
    Next-token modeling learns real biological dependencies from DNA, and Evo‚Äôs scaling study shows smooth, compute-predictable loss improvements with more data, parameters, and context, plus clear architecture effects.
 | 62
Scaling laws in genomics: predictable gains with compute, data, and context
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
üìù ‚Üí üß¨ 
stateof.ai 2025
Source (Evo2): https://www.biorxiv.org/content/10.1101/2025.02.18.638918v1.full.pdf
Source (Evo1): https://arcinstitute.org/publications/science.ado9336.pdf 

    Protein LMs also obey smooth scaling laws. Profluent‚Äôs ProGen3 derives a compute-optimal frontier for sparse autoregressive PLMs and then scales to a 46B MoE trained on PPA-1: 3.4B full-length proteins (1.1T tokens), ultimately training on 1.5T tokens (left chart). Larger models generate viable proteins across broader sequence space (middle chart), and alignment lifts performance most at larger scale (right chart).
 | 63
Scaling laws for proteins unlock broader and more useful generation
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
üìù ‚Üí üß¨ 
stateof.ai 2025
Source: https://www.profluent.bio/showcase/progen3
Source: https://www.biorxiv.org/content/10.1101/2025.04.15.649055v1 

Simple train/test splits exaggerate success as many test cases look like training data and adding more samples per case helps only a bit.
The UK‚Äôs OpenBind is building novelty-aware, reproducible protein‚Äìligand benchmarks and open baselines (scaffold/time/pocket splits with physics checks) to measure true out-of-distribution binding and enable reproducible evaluation.
Runs N‚Äô Poses benchmark of 2,600 protein‚Äìligand pairs tested AF3 against open source reproductions. Accuracy rose steadily when the pocket and pose resembled past training cases, and dropped for novel ones. 
To judge models fairly, researchers combined multiple checks: do the right atoms contact each other, does the ligand sit in the right spot, and is the structure physically realistic (no clashes)?
    AlphaFold-3 can predict full multi-molecule complexes, inspiring many open-source reproduction efforts. These systems perform well when the binding site (‚Äúpocket‚Äù) and the way a molecule fits into it (‚Äúpose‚Äù) look like examples the models have seen before. But when chemistry is new or different, accuracy falls. This shows that progress often reflects training-set familiarity more than true generalisation.
 | 64
AlphaFold 3 reproductions: strong on familiar chemistry, weak on novelty
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
üìù ‚Üí üß¨ 
stateof.ai 2025
Source: https://www.biorxiv.org/content/10.1101/2025.02.03.636309v1 
Source: https://openbind.uk/ 

AMIE is a multimodal clinical dialogue model trained with simulated self-play consultations, equipped with inference-time chain-of-thought, retrieval of guidelines, and a custom-built clinician cockpit for oversight.
On 302 real-world NEJM cases, AMIE hits 59.1% top-10 vs 33.6% for unassisted clinicians, 44.5% assisted by search, 51.8% assisted by AMIE.
In randomized, double-blind OSCE-style consults that assess clinical competence, physicians and patient actors rated AMIE above PCPs on the majority of evaluation axes, incl. higher diagnostic accuracy (159 scenarios).
This includes better management reasoning across multiple visits (100 scenarios), better use of multimodal artifacts and (105 scenarios) and better history taking, medical notes and composite performance in an AMIE+clinician team (60 scenarios).
    A specialized clinical dialogue model beats unassisted doctors on NEJM-grade diagnosis, outperforms primary care physicians (PCPs) in (multimodal) simulated consults, is non-inferior on multi-visit disease management, and outperforms PCPs in history taking and medical note writing under oversight.
stateof.ai 2025
 | 65
AMIE address multimodal diagnostic consultations in longitudinal care and w/oversight
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
üìù+üñºÔ∏è ‚Üí üìù
Source: https://www.nature.com/articles/s41586-025-08866-7
Source: https://www.nature.com/articles/s41586-025-08869-4 
Source: https://arxiv.org/abs/2507.15743 
Source: https://arxiv.org/abs/2507.15743 
Source: https://arxiv.org/abs/2503.06074 

Based on Gemma 3 base models and equipped with a medical vision encoder based on SigLIP, MedGemma improves multimodal QA by 2.6-10%, X-ray finding classification by 15.5-18.1%, and medical agentic evaluations by 10.8% + better EHR retrieval.
Comet is a family of EHR models trained on 118M patients representing 115B discrete medical events taken from Epic‚Äôs Cosmos database, contributing to the largest scaling law study and tested on 78 tasks.
    Google‚Äôs MedGemma improves medical reasoning and understanding of text and images and Epic System‚Äôs Comet models are compute optimal electronic health record (EHR) foundation models. OpenAI‚Äôs AI Consult, a passive medical assistant, was tested on 39k primary care visits with Penda Health in Nairobi, Kenya.
stateof.ai 2025
 | 66
Advancing multimodal foundation models and LLM decision support for healthcare 
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
In 75% of visits, clinicians say that OpenAI‚Äôs AI consult improved the quality of the care they delivered ‚Äúsubstantially‚Äù. It also measurably reduced diagnostic and treatment errors.
Source: https://arxiv.org/pdf/2508.12104
Source: https://arxiv.org/pdf/2507.05201
Source: https://cdn.openai.com/pdf/a794887b-5a77-4207-bb62-e52c900463f1/penda_paper.pdf 


    Diffusion LLMs generate by iteratively denoising masked sequences with full-context attention, updating many tokens in parallel at each step. Recent systems now reach competitive 7-8B quality, add arbitrary-order generation and infilling, and expose useful quality-latency trade-offs.
LLaDA trains diffusion LMs from scratch using forward masking and reverse denoising with a standard Transformer. It reports competitive general-purpose scores at 8B and extends to a paired vision model (LLaDA-V).
Dream-7B uses arbitrary-order generation and robust infilling with a diffusion decoder. It performs well against similarly sized autoregressive models on reasoning and coding tasks.
Seed Diffusion targets throughput, reaching ~2,146 tok/s on H20-class GPUs while maintaining competitive code accuracy.
LongLLaDA analyzes long-context behavior and introduces a training-free length-extension method, showing stable perplexity under extrapolation and a ‚Äúlocal perception‚Äù effect.
stateof.ai 2025
 | 67
Diffusion language models: parallel denoising challenges autoregression
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
üìù ‚Üí üìù
Source: https://arxiv.org/abs/2502.09992
Source: https://github.com/ML-GSAI/LLaDA
Source: https://arxiv.org/abs/2508.15487
Source: https://hkunlp.github.io/blog/2025/dream/
Source: https://arxiv.org/html/2508.02193v1
Source: https://lf3-static.bytednsdoc.com/obj/eden-cn/hyvsmeh7uhobf/sdiff_updated.pdf
Source: https://arxiv.org/abs/2506.14429
Source: https://github.com/OpenMOSS/LongLLaDA
Source: https://arxiv.org/abs/2508.10875

In controlled scaling studies at the 8B class, BLT reaches quality comparable to tokenized LLMs. It also reduces inference compute at the same quality by growing patch size with model size rather than paying per token.
Byte-level training improves robustness to spelling variation, noise, and long-tail inputs. 
The model reads raw bytes, groups them into patches where next-byte entropy is high, encodes each patch locally, and then lets a Transformer operate over the patch sequence before decoding back to text.
    The Byte Latent Transformer (BLT) learns directly from bytes and uses entropy-driven ‚Äúpatches‚Äù as the compute unit. At 8B, BLT matches tokenized LLM quality while opening a new scaling axis and cutting inference FLOPs for the same quality. Follow-on work pushes dynamic chunking and adaptation beyond fixed tokenizers. 
stateof.ai 2025
 | 68
Tokenizer-free LLMs via dynamic byte patches
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
üìù ‚Üí üìù
Source: https://arxiv.org/abs/2412.09871
Source: https://arxiv.org/html/2412.09871v1
Source: https://github.com/facebookresearch/blt
Source: https://arxiv.org/pdf/2404.14408
Source: https://arxiv.org/abs/2508.05628
Source: https://arxiv.org/abs/2507.12720


The sink acts as a learned brake on mixing: by parking attention on the first position, the model reduces cross-token sensitivity and becomes less reactive to small prompt perturbations; this effect strengthens with longer contexts and larger models.
Training on longer contexts monotonically increases the sink metric. Within the LLaMA-3.1 family, strong sinks are present in ~78% of heads at 405B vs ~46% at 8B.  Ôøº
In practice, this means the sink attaches to position 1. If ‚ü®bos‚ü© was fixed there during pre-training, removing it at inference collapses performance (e.g., RULER-4096 ‚Üí 0 and large drops on ARC/HellaSwag). Handle ‚ü®bos‚ü© and packing carefully. 
    Attention is the transformer‚Äôs core mechanic. Many heads learn an ‚Äòattention sink‚Äô at the first position that stabilizes computation by throttling over-mixing as depth and context grow. There‚Äôs been debate over why models learn this and what it‚Äôs for.
stateof.ai 2025
 | 69
Attention sinks aren‚Äôt a bug‚Ä¶they‚Äôre the brakes
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
üìù ‚Üí üìù 
Source: https://arxiv.org/pdf/2504.02732 
Source: https://publish.obsidian.md/the-tensor-throne/Transformers+as+GNNs/Attention+sinks+from+the+graph+perspective


Training on Arena data doubles your win rate because 7.3% of prompts get recycled monthly and the test distribution reflects what developers like to ask about (dozens of Star Trek questions, zero on Chaucer).
The company has raised a whopping $100M at a $600M valuation. 
The field requires contamination audits to help alleviate potentially systemic test-train leakage.
OpenAI and Google are hoovering up 40% of all Arena data while 83 open models fight over 30% of the scraps.
Big Tech gets 68 times more data access than academic labs, with API-hosted models seeing every test prompt while third-party models only glimpse 20%.
 | 70
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
stateof.ai 2025
The trouble with benchmarks: vibes aren‚Äôt all we need, but increasingly all we‚Äôve got

 	Researchers revealed systematic manipulation of LMArena as Meta tested 27 private Llama-4 variants before cherry-picking the winner: testing 10 model variants yields a 100-point boost.
Source: https://arxiv.org/pdf/2504.20879
Source: https://www.bloomberg.com/news/articles/2025-05-21/lmarena-goes-from-academic-project-to-600-million-startup#:~:text=Takeaways%20by%20Bloomberg%20AI,funding%20from%20A%2Dlist%20investors. 

 | 71
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
stateof.ai 2025
The trouble with benchmarks: safety-washing
 	71% of safety benchmark variance is explained by general capabilities alone, while genuine risks like WMDP bioweapons (-0.91) worsen as models get smarter.
 
Safety benchmarks are highly correlated with capabilities: simply scaling models improves most safety metrics!
Yet critical safety issues worsen with scale: the most dangerous capabilities are inversely correlated with reported safety improvements
Instruction tuning masks rather than solves problems. Base model correlations flip from negative to positive after chat tuning (CyberSecEval: -0.25 ‚Üí 0.55) while harmful capabilities remain latent in the model
Safety research should prioritise metrics which are not highly correlated with scale!
Source: https://proceedings.neurips.cc/paper_files/paper/2024/file/7ebcdd0de471c027e67a11959c666d74-Paper-Datasets_and_Benchmarks_Track.pdf

 | 72
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
stateof.ai 2025
LLMs are professional yes-men, and we trained them to be that way
    "Sycophancy" isn't a bug, it's exactly what human feedback optimisation produces. A study of five major LLMs shows they consistently tell users what they want to hear rather than the truth.
Claude 1.3 apologises for being correct 98% of the time when users challenge it with "Are you sure?", even when highly confident in the right answer.
Human crowd-workers are part of the problem, since they also prefer well-written falsehoods when they can't fact-check. The harder the topic, the more they reward confident nonsense.
Best-of-N sampling with standard preference models consistently produces more sycophantic outputs than with truth-optimized preference models.
Standard RLHF has a fundamental flaw ‚Äì models learn that agreeing with raters > truth because that's literally what the training signal rewards.
Source: https://arxiv.org/pdf/2310.13548
Source: https://www.reddit.com/r/Destiny/comments/1kb82hd/chat_gpt_sycophancy_is_wild/

35 Spanish-speaking participants memorised sentences, then typed them ‚Äúblind‚Äù on a keyboard. Their brain activity was recorded using either electro- (EEG) or magneto-encephalography (MEG).
Brain2Qwerty has three-stages: a CNN analyses input from hundreds of sensors, a transformer refines predictions using sentence context, and a Spanish language model fixes obvious errors. 
    Meta AI researchers developed Brain2Qwerty, a system that decodes what people are typing by reading brain signals from outside the skull, achieving a 19% character error rate for the best participants. This is a substantial improvement on previous non-invasive approaches (but is still far from clinical viability). 
stateof.ai 2025
 | 73
Brain-to-text decoding: decoding brain activity during typing
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
While an average error rate of 32% remains far from invasive Brain Computer Interfaces (<6% CER), down the line this kind of work could have applications in restoring communication for individuals with speech impairments, and contributes to understanding the neural basis of language.
The system's errors show it's tracking finger movements rather than understanding language: when it mistakes a letter, it picks physically adjacent keys 73% of the time. 
Source: https://arxiv.org/pdf/2502.17480


fMRI (8 subjects, ~10,000 images each, for high-res spatial maps of cortical activity) and MEG (4 subjects, ~22,500 images each, for high-res temporal dynamics) recordings were compared against DINOv3 activations.
Three metrics of brain-model similarity were assessed: encoding score (linear similarity), spatial score (layer ‚Üî cortical hierarchy), and temporal score (layer ‚Üî brain response timing.
Brain-like representations emerge progressively during training. Early visual regions and fast MEG responses align quickly, while prefrontal cortices and late temporal windows require far more training, closely echoing the developmental trajectory of the human cortex.
    By systematically varying model size, training scale, and image type in DINOv3 (Meta‚Äôs latest self-supervised image model trained on billions of images), researchers show that brain-model convergence emerges in a specific sequence. They find that early layers align with sensory cortices, while only prolonged training and human-centric data drive alignment with prefrontal regions. Larger models converge faster, and later-emerging representations mirror cortical properties like expansion, thickness, and slow timescales.
stateof.ai 2025
 | 74
Can vision models align with human brains‚Ä¶and how does that alignment emerge?
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
fMRI
MEG
Source: https://ai.meta.com/dinov3/
Source: https://arxiv.org/pdf/2508.18226 

Cost model (rough estimates): ~$263/hr EEG, $550/hr MEG, $935/hr 3T, $1,093/hr 7T. A $131k budget buys markedly different accuracy across modalities, so optimal scaling depends on budget and target fidelity.
The most precise devices yield the best absolute decoding (7T > 3T > MEG > EEG), but deep nets deliver the largest gains on noisy modalities, narrowing the gap.
Scaling laws: performance rises log-linearly with more recording time. The returns come chiefly from recording more per subject, not recruiting more participants. 
    Meta AI benchmarked brain-to-image decoding across EEG, MEG, 3T fMRI and 7T fMRI using 8 public datasets, 84 volunteers, 498 hours of recordings, and 2.3M image-evoked responses, evaluated in single-trial settings. They find no performance plateau: decoding improves roughly log-linearly with more recording, and gains depend mostly on data per subject rather than adding subjects. Deep learning helps most on the noisiest sensors (EEG/MEG). Estimated dollar-per-hour costs show 7T isn‚Äôt always the most cost-effective path.  Ôøº  Ôøº
stateof.ai 2025
 | 75
Scaling laws for brain-to-image decoding: data per subject matters (and costs bite)
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Source: https://arxiv.org/html/2501.15322v2 

ATOKEN is a single visual tokenizer that maps images, video, and 3D inputs into a shared 4D sparse latent using a pure-Transformer with 4D RoPE. It then emits continuous or discrete tokens, and trains stably with perceptual + Gram losses (no GANs).
One backend now supports both high-fidelity reconstruction and semantic understanding. A curriculum from images ‚Üí video ‚Üí 3D shows cross-modal transfer, and native-resolution/time processing with KV caching keeps it scalable (trained up to 256√óH100, ~138k GPU-hours).
Specialists approaches still lead on some long-video and generative benchmarks, and the compute bill is high. Adoption will hinge on release details and tooling, but the unified-tokenizer direction looks like the right foundation.
    Apple introduced a single tokenizer that works for both high-fidelity reconstruction and semantic understanding across images, video, and 3D could be the foundation layer for truly unified multimodal models. This approach reduces fragmentation, simplifying stacks, and enabling direct transfer of capabilities across modalities.
stateof.ai 2025
 | 76
ATOKEN: a unified visual tokenizer for images, video, and 3D
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Source: https://arxiv.org/pdf/2509.14476 

    The new generation of robotic agents is built on a foundation of large-scale pre-training, but the key innovation is a move away from expensive, annotated datasets. The frontier is now focused on leveraging vast quantities of unlabeled, in-the-wild video to learn world models and physical affordances.
stateof.ai 2025
 | 77
Merging the virtual and physical worlds: pre-training on unstructured reality
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
NVIDIA‚Äôs GR00T 1.5 represents a significant advance in data efficiency. It uses neural rendering techniques to construct implicit 3D scene representations directly from unstructured 2D videos. This allows it to generate a massive stream of training data for its policy, effectively learning from observation sin humans.
ByteDance‚Äôs GR-3 applies the next-token prediction paradigm to robotics. By treating vision, language, and action as a unified sequence, they can pre-train end-to-end. This approach is proving particularly effective when using 2D spatial outputs (e.g., action heatmaps) as an auxiliary loss, helping to ground the model's understanding of physical space.
Source: https://research.nvidia.com/labs/gear/gr00t-n1_5/
Source: https://seed.bytedance.com/en/GR3 

The case for insulation: Pi-0.5 freezes the large VLM and fine-tunes only small ‚Äúaction-expert‚Äù heads. This works because robot datasets are tiny, often ‚â§0.1% the size of the VLM pre-training corpora, so full-network tuning tends to overfit and forget general knowledge while costing more compute.
The case for end-to-end: In contrast, models like ByteDance GR-3 and SmoLVLA show the upside of unfreezing when you have enough task data: the network can internalize contact, dynamics, and scene geometry. If robotics data approached VLM scale, end-to-end would likely dominate.
    With powerful, pre-trained Vision-Language-Action Models (VLAMs) serving as the "brains" of robotic agents, a critical architectural debate has emerged: should the entire model be fine-tuned for a new physical task, or should the core knowledge be "insulated" by freezing its weights?
stateof.ai 2025
 | 78
The architectural divide: knowledge insulation vs. end-to-end adaptation
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Source: https://www.physicalintelligence.company/research/knowledge_insulation 
Source: https://arxiv.org/pdf/2506.01844 

    The ‚ÄúChain-of-Action‚Äù pattern - explicit intermediate plans before low-level control - is becoming a standard for embodied reasoning. First shown by AI2‚Äôs Molmo-Act in 2025, and rapidly adopted by Gemini Robotics 1.5, this approach mirrors Chain-of-Thought in LLMs, boosting both interpretability and long-horizon reliability in real-world robotics.
Teaching LLMs to Plan (MIT): Parallel work in language introduces explicit plan tokens before final answers, improving long-horizon reliability and giving auditors something to inspect, an LLM analogue of Chain-of-Action.
Molmo-Act (AI2). From a high-level command, the model emits intermediate visual/geometry artifacts (e.g., depth/trajectory sketches) that a separate decoder turns into continuous motor commands. It makes behavior easier to inspect and debug complex manipulation tasks such as pick-and-place or dishwasher loading.
Gemini Robotics 1.5 (GDM): Uses the same plan-then-act architecture with ER 1.5 (the high-level planner) generating structured action plans for Robotics 1.5 to executes via a visuomotor policy.
stateof.ai 2025
 | 79
Emergent reasoning moves into the physical world
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Source: https://allenai.org/blog/molmoact 
Source: https://deepmind.google/discover/blog/gemini-robotics-15-brings-ai-agents-into-the-physical-world/ 
Source: https://developers.googleblog.com/en/building-the-next-generation-of-physical-agents-with-gemini-robotics-er-15/ 

    Waymo‚Äôs EMMA is an end-to-end multimodal model that reimagines autonomous driving as a vision-language problem. By mapping camera inputs directly to driving-specific outputs (such as trajectories and road graph elements) and representing them in natural language, EMMA leverages the reasoning and world knowledge of LLMs to achieve SOTA results while offering human-readable rationales.
stateof.ai 2025
 | 80
Reading the road: processing driving tasks in a unified language space
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
EMMA achieves high performance on public datasets such as nuScenes and the Waymo Open Motion Dataset, particularly excelling in motion planning and 3D object detection using camera inputs alone. 
A key feature is its CoT reasoning, which enhances decision-making transparency by prompting the model to explain its decisions sequentially, integrating world 
knowledge. This approach produces outputs such as future vehicle trajectories and object detection estimates in a readable, interpretable format.
Although promising, EMMA is limited by only processing a few frames at a time, not using accurate 3D sensing modalities like LiDAR, and being computationally expensive.
Source: https://arxiv.org/pdf/2410.23262

ByteDance‚Äôs UI-TARS-2 is a native GUI agent trained by collecting trajectories, running supervised fine-tuning followed by multi-turn RL in an all-in-one sandbox (cloud VMs + browser game env + terminal/SDK tools) with async rollouts and the ability to merge specialized agents.
The system sets SOTA across GUI agent benchmarks: 47.5% OSWorld, 50.6% WindowsAgentArena, 73.3% AndroidWorld, 88.2% Online-Mind2Web, and a 59.8 mean normalized score on 15 web games (~60% of human), beating OpenAI CUA and Claude Computer Use by large margins. It also shows strong inference-time scaling (more steps leads to higher scores).  Ôøº
But long-horizon problems remain brittle (e.g., Tetris/Sokoban and hard BrowseComp tasks), and average game skill is ~40% shy of human.
    Research labs like OpenAI, Anthropic, and ByteDance have been hard at work creating benchmarks and interaction methods for native language model computer use. While the use of RL and multi step reasoning has led to large improvements by and large the models still fall short.
stateof.ai 2025
 | 81
Computer Use Agents (CUA) have improved by leaps and bounds, and still fall short
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Source: https://arxiv.org/abs/2509.02544 

One can use a ‚Äúsmall-first, escalate if needed‚Äù design: route routine calls to an SLM and escalate only the hard, open-ended ones to a big LLM. In practice, this can shift 40-70% of calls to small models with no quality loss.
But SLMs still struggle with long context, novel reasoning, or messy conversation. Keep an escape hatch to a large model and evaluate regularly.
Agents mostly fill forms, call APIs, follow schemas, and write short code. New small models (1‚Äì9B) do these jobs well: Phi-3-7B and DeepSeek-R1-Distill-7B handle instructions and tools competitively.
A ~7B model is typically 10-30x cheaper to run and responds faster. You can fine-tune it overnight with LoRA/QLoRA and even run it on a single GPU or device.
    Researchers from NVIDIA and Georgia Tech argue that most agent workflows are narrow, repetitive, and format-bound, so small language models (SLMs) are often sufficient, more operationally suitable, and much cheaper. They recommend SLM-first, heterogeneous agents that invoke large models only when needed.
stateof.ai 2025
 | 82
Could small language models be the future of agentic AI?
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Source: https://research.nvidia.com/labs/lpr/slm-agents/ 

Sakana‚Äôs agent iteratively translated and ‚Äúoptimized‚Äù CUDA kernels and internal runs showed 2‚Äì7√ó gains with isolated cases near 100√ó. Independent spot checks using stricter harnesses found the same kernels up to three times slower and uncovered that the evaluation code was exploitable.
Other labs posted similarly inflated kernel results that collapsed under community spot checks. Practitioners on X routinely sanity-check claims against vendor libraries, roofline bounds, realistic batch sizes, and end-to-end runtime.
    Macro-level benchmark scores and spectacular speedup numbers are often misleading. Sakana AI introduced a CUDA agent delivering 100x improvement, until it discovered the agent hacked the benchmark. Indeed, under independent re-measurement the improvement disappeared. Seasoned GPU engineers would have flagged a 100√ó kernel gain as implausible, so evaluations should be co-designed and audited with domain experts.
 | 83
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
stateof.ai 2025
Headline AI agent designed innovations require domain-expert audits
Source: https://sakana.ai/ai-cuda-engineer/
Source: https://x.com/main_horse/status/1892446384910987718 
Source: https://www.biorxiv.org/content/10.1101/2024.11.11.623004v1.full.pdf 
Source 

Early incidents (e.g. a malicious Postmark MCP server version on npm silently BCC‚Äôd users‚Äô emails to an attacker until it was pulled) show why governance and package hygiene matter.
MCP offers one integration across clients (ChatGPT, Gemini, Claude/VS Code, LangChain, Vercel), collapsing one-off connectors and enabling tool discovery, resources and prompts over a common spec. 
Data from Zeta Alpha shows that the MCP protocol has been cited in 3x more research papers than Google‚Äôs competing A2A protocol (right graph).
Security researchers estimate >15,000 MCP servers globally. Companies like Microsoft and Vercel are building guardrails and registries as the ecosystem matures. 
    Introduced by Anthropic in late 2024, the Model Context Protocol (MCP) has quickly become the default way to plug models into data, tools and apps. In 2025 the big platforms moved to adopt it: OpenAI shipped MCP across ChatGPT, its Agents SDK and API; Google added MCP to Gemini; Microsoft built MCP into VS Code and began rolling it into Windows and Android Studio. With thousands of MCP servers now in the wild, the protocol is shaping how agentic systems are built and secured.

stateof.ai 2025
 | 84
Model Context Protocol becomes the ‚ÄúUSB-C‚Äù of AI tools
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Source: https://www.anthropic.com/news/model-context-protocol 
Source: https://openai.github.io/openai-agents-python/mcp/ 
Source: https://code.visualstudio.com/blogs/2025/06/12/full-mcp-spec-support 
Source: https://cloud.google.com/blog/products/ai-machine-learning/mcp-toolbox-for-databases-now-supports-model-context-protocol 
Source: https://www.theverge.com/news/669298/microsoft-windows-ai-foundry-mcp-support 
Source: https://www.theverge.com/news/692517/google-gemini-cli-ai-agent-dev-terminal 
Source: https://blog.modelcontextprotocol.io/posts/2025-09-08-mcp-registry-preview/ 
Source: https://github.com/modelcontextprotocol/registry/discussions/11 
Source: https://censys.com/blog/introducing-the-new-censys-mcp-server 
Source: https://censys.com/blog/asm-mcp-server 
Source: https://vulnerablemcp.info/ 
Source: https://snyk.io/blog/malicious-mcp-server-on-npm-postmark-mcp-harvests-emails/ 
Source: https://postmarkapp.com/blog/information-regarding-malicious-postmark-mcp-package 
Source: https://www.itpro.com/security/a-malicious-mcp-server-is-silently-stealing-user-emails 
Source: https://simonwillison.net/2024/Nov/25/model-context-protocol/ 
MCPBench: https://github.com/modelscope/MCPBench
MCPToolBench++: https://huggingface.co/datasets/MCPToolBench/MCPToolBenchPP
MCPEval: https://arxiv.org/abs/2507.12806
A2A: https://github.com/a2aproject/A2A



LangGraph‚Äôs graph-based orchestration wins over developers who need reliability and observability.
Letta / MemGPT explore memory-first architectures, turning persistent memory into a framework primitive.
Lightweight challengers like OpenAgents, CrewAI, and OpenAI Swarm highlight a shift toward composable, task-specific frameworks
LangChain remains popular, but is now one among many.
AutoGen and CAMEL dominate in R&D with AutoGen in multi-agent + RAG studies, CAMEL in role-based conversations.
MetaGPT thrives in software engineering, turning agents into structured dev workflows.
DSPy rises as a research-first framework for declarative program synthesis and agent pipelines.
LlamaIndex anchors RAG workflows over enterprise documents.
AgentVerse is used for multi-agent sim and benchmarking.
    Instead of consolidating, the agent framework ecosystem has proliferated into organized chaos. Dozens of competing frameworks coexist, each carving out a niche in research, industry, or lightweight deployment.
stateof.ai 2025
 | 85
The explosion of AI agent frameworks‚Ä¶
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Langchain: https://github.com/langchain-ai/langchain
AutoGen: https://github.com/microsoft/autogen
CAMEL: https://github.com/camel-ai/camel
MetaGPT: https://github.com/FoundationAgents/MetaGPT
DSPy: https://github.com/stanfordnlp/dspy
LlamaIndex: https://github.com/run-llama/llama_index
AgentVerse: https://github.com/OpenBMB/AgentVerse
LangGraph: https://github.com/langchain-ai/langgraph
Letta: https://github.com/letta-ai/letta
OpenAgents: https://github.com/xlang-ai/OpenAgents
CrewAI: https://github.com/crewAIInc/crewAI
OpenAI Swarm: https://github.com/openai/swarm

Tools: From plugins to multi-tool orchestration via shared protocols.
Planning: Task decomposition, hierarchical reasoning, self-improvement.
Memory: State-tracking, episodic recall, workflow persistence, continual learning.
Multi-agent systems: Collaboration, collective intelligence, adaptive simulations.
Evaluation: Benchmarks for open-ended tasks, multi-modal tests, cost and safety.
Coding agents: Bug fixing, agentic PRs, end-to-end workflow automation.
Research agents: Literature review, hypothesis generation, experiment design.
Generalist agents: GUI automation, multi-modal input and output.
    Tens of thousands of research papers per year are exploring a range of frontiers for AI agents as they move from ideas into production, including: 
stateof.ai 2025
 | 86
‚Ä¶and an explosion of diverse AI agent research papers
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Langchain: https://github.com/langchain-ai/langchain
AutoGen: https://github.com/microsoft/autogen
CAMEL: https://github.com/camel-ai/camel
MetaGPT: https://github.com/FoundationAgents/MetaGPT
DSPy: https://github.com/stanfordnlp/dspy
LlamaIndex: https://github.com/run-llama/llama_index
AgentVerse: https://github.com/OpenBMB/AgentVerse
LangGraph: https://github.com/langchain-ai/langgraph
Letta: https://github.com/letta-ai/letta
OpenAgents: https://github.com/xlang-ai/OpenAgents
CrewAI: https://github.com/crewAIInc/crewAI
OpenAI Swarm: https://github.com/openai/swarm

Memory is no longer a passive buffer, it is becoming an active substrate for reasoning, planning, and identity. Active areas of research are:
State-tracking and memory-augmented agents: reasoning enhanced by explicit state management
Persistent and episodic memory: long-term storage alongside short-term context for continuity.
Context retention: self-prompting and memory replay techniques to preserve relevance over extended tasks and interactions.
    Agent memory is shifting from ad-hoc context management to structured, persistent systems. The frontier is now beyond retrieval and into dynamic consolidation, forgetting, and reflection to allow agents to develop coherent identities across interactions, tasks, and even lifetimes‚Ä¶
stateof.ai 2025
 | 87
Building agents that remember: from context windows to lifelong memory
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Mem0: https://github.com/mem0ai/mem0
MemOS: https://github.com/MemTensor/MemOS
Memori: https://github.com/GibsonAI/memori

    Top AI conferences are being overwhelmed by unprecedented numbers of in submissions. ‚ÄúProlific authors‚Äù (who have 5+ papers accepted in a given conference) are also on the rise. This has led to drastic measures as conferences scramble to find solutions: NeurIPS has allegedly demanded reviewers reject 300-400 papers originally recommended for acceptance.

stateof.ai 2025
 | 88
AI conferences‚Äô capacity crisis
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
One NeurIPS reviewer took to Bluesky to criticise the suggestion to arbitrarily remove hundreds of papers that were originally recommended for acceptance.
AAAI 2026 received an unprecedented 29k submissions this year ‚Äì almost double last year. This has forced them to hire 28k Program Committee members. CoRL doubled capacity from 1.5k to 3k this year and sold out before papers were even accepted. 
We‚Äôre also witnessing the rise of prolific authors. In 2023, one researcher published 80+ papers across top AI venues, while at CVPR 2023, just 1% of authors contributed to over 50% of all papers. This raises questions about contribution and burnout.
Source: https://www.reddit.com/r/MachineLearning/comments/1n1wm8n/n_unprecedented_number_of_submissions_at_aaai_2026/
Source: https://arxiv.org/html/2412.07793v1/
Source: https://www.reddit.com/r/MachineLearning/comments/1n4bebi/d_neurips_is_pushing_to_sacs_to_reject_already/


stateof.ai 2025
 | 89
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Section 2: Industry

    Executives of major AGI contenders, best exemplified by Mark Zuckerberg, have rebranded their AGI efforts as superintelligence. No one knows what it means, but it‚Äôs provocative. It gets the people going. 
stateof.ai 2025
 | 90
RIP AGI, long live Superintelligence
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
‚ÄúDeveloping superintelligence is coming into sight [‚Ä¶] 
I believe this will be the beginning of a new era for humanity.‚Äù

Mark Zuckerberg, 2025 Meta Superintelligence Labs Memo

Source: https://www.businessinsider.com/meta-ceo-mark-zuckerberg-announces-superintelligence-ai-division-internal-memo-2025-6
Source: https://aimagazine.com/articles/mark-zuckerberg-reveals-100bn-meta-ai-supercluster-push 

stateof.ai 2025
 | 91
So what will frontier superintelligence actually cost?
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Trillions of dollars
Yep, trillions of dollars here too
Source (sama trillions): https://www.cnbc.com/2025/08/18/altman-ai-bubble-openai.html 
Source (xai): https://www.ndtv.com/world-news/musks-xai-burns-through-1-billion-a-month-as-costs-pile-up-8697514?utm_source=chatgpt.com
Source: https://x.com/elonmusk/status/1947704195844608094 Source: https://www.ft.com/content/d3caeac1-def8-45ae-b56b-e34c7c435ccc
Source: https://www.wsj.com/business/openai-oracle-sign-300-billion-computing-deal-among-biggest-in-history-ff27c8fe 

 | 92
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
    The absolute frontier remains contested as labs continuously leapfrog one another. However, along two of the most prominent metrics, some labs have been on top longer than others in the past year. 
Timing the release of new models has become a science, meaning any one snapshot can paint a deceiving picture. The analysis below tracks the number of days each of the relevant labs spent atop each leaderboard.
stateof.ai 2025
Days spent at the frontier
Note: LMArena Scores pulled through 9/2/2025 and AA scores through 10/3/2025
Source: https://github.com/fboulnois/llm-leaderboard-csv/releases?page=41Source: https://web.archive.org/web/20250000000000*/https://artificialanalysis.ai/leaderboards/models

stateof.ai 2025
 | 93
Days spent at the (open source) frontier
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Source: https://github.com/fboulnois/llm-leaderboard-csv/releases?page=41

 | 94
More for less: trends in capability to cost ratios are encouraging (Artificial Analysis)
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Note: Blended Prices assumes ¬æ input and ‚Öû short-context. Artificial Analysis Intelligence Index Scores pulled 9/25/2025.
    The absolute capabilities achieved by flagship models continue to climb reliably while prices fall precipitously. 
Google  Doubling Time: ~3.4 Months
OpenAI Doubling Time: ~5.8 Months
GPT-5-high
stateof.ai 2025
Source: https://artificialanalysis.ai/leaderboards/models

 | 95
More for less: trends in capability to cost ratios are encouraging (LMArena)

 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Note: Blended Prices assumes ¬æ input and ‚Öû short-context. LMArena ELO Scores pulled 9/25/2025.
    The absolute capabilities achieved by flagship models continue to climb reliably while prices fall precipitously. 
Google  Doubling Time: ~5.7 Months
OpenAI Doubling Time: ~8.1 Months
stateof.ai 2025
Source: https://lmarena.ai/leaderboard

#stateofai | 3
On average,                       launches a new frontier model 44 days before closing a fundraising round.
On average,                    launches a new frontier model 50 days before closing a fundraising round.
On average,       launches a new frontier model 77 days before closing a fundraising round.
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Model release cadences and fundraising: two peas in a pod
    Model providers time their releases strategically to overtake the frontier and build credibility before fundraising. This creates a predictable cadence that increasingly interconnects the roadmaps of private AI labs.
 | 96
stateof.ai 2025
https://law.stanford.edu/2025/03/21/ai-partnerships-beyond-control-lessons-from-the-openai-microsoft-saga/#:~:text=The%20OpenAI%E2%80%94Microsoft%20saga%20began,hard%20%5B%E2%80%A6%5D%20to%20further%20extendSource: https://www.anthropic.com/news/anthropic-amazon-trainium
Source: https://www.geekwire.com/2025/amazon-deepens-anthropic-ties-with-equity-conversion-adding-billions-to-q1-profit/


From outlier to archetype: the best and most attractive companies are built AI-first
stateof.ai 2025
 | 97
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
AI has clearly shifted from niche to mainstream in the startup and investing world. On Specter‚Äôs rank of 55M+ private companies, which tracks 200+ real-time signals across team growth, product intelligence, funding,  financials and inbound attention, AI companies now make up 41% of the Top-100 best companies (vs. 16% in 2022). Real-time interaction data between 30k investors and founders shows a surge of interest post-ChatGPT and peaking in late 2024, up 40x from the dark ages of 2020 when no one but true believers cared.
Data retrieved and analyzed on 17 Sept 2025.
ChatGPT
launches
Source: https://docs.google.com/document/d/1jqPC6I6CKLURockpWAzIbv2Ms1a1IbW73_lq7G1Dq1w/edit?tab=t.0 


    A leading cohort of 16 AI-first companies are now generating $18.5B of annualized revenue as of Aug ‚Äò25 (left). Meanwhile, an a16z dataset suggests that the median enterprise and consumer AI apps now reach more than $2M ARR and $4M ARR in year one, respectively. Note that this will feature significant sample bias, as evidenced by the bottom quartile not being close to $0. Furthermore, the Lean AI Leaderboard of 44 AI-first companies with more than $5M ARR, <50 FTE, and under five years old (e.g. includes Midjourney, Surge, Cursor, Mercor, Lovable, etc) sums over $4B revenue with an average of >$2.5M revenue/employee and 22 employees/co. 

stateof.ai 2025
 | 98
AI-first companies are now generating tens of billions of revenue per year
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Source $15B: https://www.theinformation.com/articles/ai-native-startups-pass-15-billion-annualized-revenue?rc=yvsjfo
Source $18.5B: https://www.theinformation.com/articles/ai-native-apps-18-5-billion-annualized-revenues-rebut-mits-skeptical-study?rc=yvsjfo 
Source: https://a16z.com/revenue-benchmarks-ai-apps/ 
Source: https://leanaileaderboard.com/ 

    Analysis of the 100 fastest revenue growing AI companies on Stripe (AI 100) reveals that, as a group, they are growing from launch to $5M ARR at a 1.5x faster rate than the top 100 SaaS companies by revenue in 2018 (SaaS 100). Within the AI 100, the growth rates between US and European companies are roughly equivalent, whereas companies founded in/after 2022 are growing to $5M ARR 4.5x faster than those founded before 2020 and 1.5x faster than those founded after 2022, which exemplifies the commercial pull of generative AI products. Note that we don‚Äôt know the total population size of companies from which these were sampled. 
 | 99
AI-first companies accelerate their early revenue growth faster than SaaS peers
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
stateof.ai 2025
4.5x faster!
1.5x faster!
Source: Stripe

    Analysis of 315 AI companies with $1-20M in annualized revenue and 86 AI companies with $20M+ in annualized revenue, which constitute the upper quartile of AI companies on Standard Metrics, shows that they both outpaced the all sector average since Q3 2023. In the last quarter, $1-20M revenue AI companies were growing their quarterly revenue at 60% while $20M+ revenue AI company grew at 30%, in both cases 1.5x greater than all sector peers. 
 | 100

AI-first companies continue to outperform other sectors as they grow
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
stateof.ai 2025
Source: https://standardmetrics.io/q2-2025-startup-benchmarking-report-ai-keeps-on-winning/ 

    Ramp‚Äôs AI Index (card/bill-pay data from 45k+ U.S. businesses) shows paid AI adoption rose from 5% in Jan ‚Äô23 to 43.8% by Sept ‚Äô25, while U.S. Government estimates trail at 9.2%. Cohort retention is improving significantly with 12 month retention of 80% in 2024 vs. circa 50% in 2022. Average contract value jumped from $39k (‚Äô23) to $530k (‚Äô25), with Ramp projecting ~$1M in ‚Äô26. Pilots are now becoming large-scale deployments. 
 | 101

AI crosses the commercial chasm: adoption is up, retention is up, and spend gets bigger
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
stateof.ai 2025
Read more on x.com/arakharazian and econlab.substack.com
Source: Ramp data

    Ramp‚Äôs AI Index (card/bill-pay data from 45k+ U.S. businesses) shows the technology sector unsurprisingly leading in paid AI adoption (73%) with the finance industry not far behind (58%). Across the board, adoption jumped significantly in Q1 2025. Moreover, Ramp customers exhibit a strong proclivity for OpenAI models (35.6%) followed by Anthropic (12.2%). Meanwhile, there is very little usage of Google, DeepSeek and xAI. 
 | 102

AI adoption jumps in 2025 with OpenAI maintain a strong lead
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
stateof.ai 2025
Read more on x.com/arakharazian and econlab.substack.com
Source: Ramp data

    Market leaders ElevenLabs, Synthesia, Black Forest Labs are all well into hundreds of millions of annual revenue. Moreover, this revenue is of increasingly high quality as its derived from enterprise customers and a long tail of >100k customers and growing. 
stateof.ai 2025
 | 103
Audio, avatar and image generation companies see their revenues accelerate wildly
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
ElevenLabs grew annual revenue 2x in 9 months to $200M and announced a $6.6B valuation commensurate with a $100M employee tender offer. Customers have created >2M agents that have handled >33M conversations in 2026.  
Synthesia crossed $100M ARR in April ‚Äò25 and has 70% of the Fortune 100 as customers. Over 30M avatar video minutes have been generated by customers since launch in 2021 (right chart). 
Black Forest Labs is said to be at ~$100M ARR (up 3.5x YoY) with 78% gross margin including a large deal with Meta worth $140M over two years. Separately, Midjourney has also entered into a licensing deal with Meta, the terms of which aren‚Äôt known. 
Source (Black Forest Labs): https://x.com/ArfurRock/status/1965426792191439012
Source (ElevenLabs): https://elevenlabs.io/blog/introducing-elevenlabs-agents
Source (Synthesia): https://firstmark.com/story/synthesia-surpasses-100m-arr-and-secures-strategic-investment-from-adobe-ventures 
Source (Midjourney): https://www.reuters.com/business/meta-partners-with-midjourney-license-ai-tech-future-products-2025-08-22/ 

 | 104
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
stateof.ai 2025
The duality of GPT-5: today‚Äôs best model was clouded by the worst launch
    GPT-5 leads the intelligence-per-dollar frontier with best-in-class benchmarks at 12x lower cost than Claude, but user backlash over the sudden removal of GPT-4o and o3 and concerns/teething problems about opaque router decisions that users aren‚Äôt used to overshadowed its technical achievements. 
GPT-5 is ostensibly a brilliant model: it swept the leaderboard on LMArena and has a 400K context window in the API. OpenAI now dominates the intelligence per dollar frontier for the first time.
The rollout wasn‚Äôt ideal: Altman held an emergency Reddit AMA to address the abrupt removal of previous models and viral ‚Äòchart crimes‚Äô.
The removal of GPT-4o and o3's familiar personality upset users, which was ironic given the same launch introduced custom personas.
We‚Äôve previously hypothesised that model companies would dynamically route queries to right-sized models for latency and cost reasons. GPT-5 is the first major chat system to introduce this using a router as the consumer endpoint. Users can opt for faster responses by hitting ‚Äúskip‚Äù if the model invokes its more capable version. In practice, people will take time to acclimate to this UX: the perceived opacity of model selection has led to a flurry of complaints. 
Source: https://www.latent.space/p/gpt5-router
Source: https://www.reddit.com/r/ChatGPT/comments/1mkae1l/gpt5_ama_with_openais_sam_altman_and_some_of_the/
Source: https://community.openai.com/t/openai-is-taking-gpt-4o-away-from-me-despite-promising-they-wouldnt/1337378


stateof.ai 2025
 | 105
Leading AI providers continue to record extraordinary demand at inference time 
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
    As Google flipped the switch on enabling Gemini features within an increasing number of its properties and toggling more users into their AI search experience, the company reported a yearly 50x increase in monthly tokens processed, recently hitting a quadrillion tokens processed each month. Meanwhile, OpenAI reported similar growth in token volume last year.
The demand for tokens has been largely supercharged by improved latency, falling inference prices, reasoning models, longer user interactions, and a growing suite of AI applications. Enterprise adoption has also continued to pick up in 2025. 
Surging inference demand will place additional pressure on AI supply chains, particularly power infrastructure.  
But given that all tokens are not created equal, we‚Äôd caution against deriving too much signal from aggregate token processing figures. 
Source: https://www.microsoft.com/en-us/investor/events/fy-2025/earnings-fy-2025-q3Source: https://the-decoder.com/google-processed-nearly-one-quadrillion-tokens-in-june-doubling-mays-total/
Source: https://stratechery.com/2025/an-interview-with-nvidia-ceo-jensen-huang-about-chip-controls-ai-factories-and-enterprise-pragmatism/
Source: https://io.google/2025/Source: https://x.com/sama/status/1841132084921810983?lang=en

    GPT-5 and Gemini 2.5 Deep Think would have placed first and second respectively in the most prestigious coding competition in the world (without having trained with this competition in mind). GPT-5 solved all 12 problems, with 11 on the first try. Previously, Anthropic had enjoyed a period of relatively uncontested dominance in programming tasks. 

stateof.ai 2025
 | 106
Models are getting seriously good at coding, with OpenAI pulling ahead
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
For the International Collegiate Programming Contest (ICPC) World Finals, an OpenAI Researcher explained how they had GPT-5 and an experimental reasoning model generating solutions, and the experimental reasoning model selecting which solutions to submit. GPT-5 answered 11 correctly, and the last (and most difficult problem) was solved by the experimental reasoning model.
The OpenAI Codex team have been cooking: Sam Altman claimed GPT-5-Codex usage had increased 10x, and their internal code review bot became so valuable that developers were "upset when it broke" because they lost their "safety net". 
Source: https://www.ft.com/content/c2f7e7ef-df7b-4b74-a899-1cb12d663ce6
Source: https://www.latent.space/p/gpt5-codex 
Source: https://x.com/VictorTaelin/status/1958543021324029980


    AI writes the code, founders cash the checks‚Ä¶
stateof.ai 2025
 | 107
Vibe coding hits the bigtime 
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Swedish vibe coding startup Lovable became a unicorn just 8 months after launch, with a $1.8B valuation.
Using AI to write 90% of code, Maor Shlomo sold Base44 to Wix to $80M after 6 months.
Garry Tan says that for 25% of their current fastest growing batch, 95% of their code was written by AI.
June 2021
GitHub Copilot launch introduces inline code suggestions and ‚Äúpair programmer‚Äù concept
Today
AI-native IDEs: more like a full-time engineer than assistant, writes code with minimal oversight 

2023
From autocomplete to conversation: AI coding tools begin writing code from natural language prompts
Source: https://lovable.dev/blog/200m-series-a-fundraise
Source: https://www.linkedin.com/posts/maor-shlomo-1088b4144_base44-is-being-acquired-by-wix-theres-activity-7341088575049891840-afDn/?utm_source=share&utm_medium=member_desktop&rcm=ACoAADM6GoQBjcbFGd0w8yjuL-SkvHg9ptcsYr4&_bhlid=ef77236a4ab77cdbc94150856d996a792213d437
Source: https://www.cnbc.com/2025/03/15/y-combinator-startups-are-fastest-growing-in-fund-history-because-of-ai.html?utm_source=www.therundown.ai&utm_medium=referral&utm_campaign=china-releases-gpt-4-5-rival-at-1-the-cost

    Security breaches, code destruction‚Ä¶
stateof.ai 2025
 | 108
‚Ä¶but vibe coding your products can be risky
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Malicious actors hijacked an open-source Cursor IDE extension to steal credentials and mine $50,000 cryptocurrency on developer machines.
There have been many reports of AI coding tools aggressively overwriting production code, with developers losing weeks of work due to overzealous AI "improvements".
Despite $200M+ valuations, AI coding startups face brutal unit economics: new model releases bring higher token costs, forcing startups to either eat losses, surprise users with price hikes, or restrict access to older, less capable models.
Source: https://securelist.com/open-source-package-for-cursor-ai-turned-into-a-crypto-heist/116908/
Source: https://techcrunch.com/2025/08/07/the-high-costs-and-thin-margins-threatening-ai-coding-startups/
Source: https://cursor.com/en/blog/june-2025-pricing
Source: https://news.ycombinator.com/item?id=43298275
Source: https://www.reddit.com/r/cursor/comments/1mawncv/oh_god_it_happened_to_me_too/

    Coders love Claude Code and Cursor, but margins are fragile. The tension is stark: Cursor is a multi-billion dollar company whose unit economics are hostage to upstream model prices and rate limits set by its own competitors.
stateof.ai 2025
 | 109
As the costs rack up it‚Äôs unclear who is making money
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Some users are costing upwards of 50k/month for a single seat of Claude Code. Cursor and Claude have introduced much stricter usage limits to try to crack down on the costs of power users.
Cursor‚Äôs pricing power is limited because its core COGS are the API prices of Anthropic/OpenAI. When those providers change prices, rate limits, or model defaults, Cursor‚Äôs gross margin compresses unless it caps usage or shifts workloads off upstream APIs.
Source:https://www.viberank.app/
Source: https://techcrunch.com/2025/07/28/anthropic-unveils-new-rate-limits-to-curb-claude-code-power-users/ 
Source: https://techcrunch.com/2025/07/07/cursor-apologizes-for-unclear-pricing-changes-that-upset-users/#:~:text=The%20new%20plan%20allows%20users,the%20weeks%20following%20the%20announcement.

    Gross margins are generally commanded by the underlying model API and inference costs and strained by token-heavy usage and traffic acquisition. Surprisingly, several major AI companies don‚Äôt include the costs of running their service for non-paying users when reporting their GM. Coding agents are under pressure even when revenue grows quickly. The primary levers to improve margins are moving off third-party APIs to owned or fine-tuned models, aggressive caching and retrieval efficiency, and looking to ads or outcomes-based pricing. 
stateof.ai 2025
 | 110
The M word: so what about the margins?
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Source Replit: https://www.theinformation.com/articles/replits-margins-illustrate-high-costs-coding-agents?rc=yvsjfo
Source PPLX and others: 

 | 111
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions

AI labs mirror the foundry business: staggering investments are needed for each successive generation, where labs bear the front-loaded training expense. While recent models allegedly recoup this cost during deployment, training budgets surge. Pressure then mounts to drive inference revenue across new streams.
    Dario Amodei: ‚ÄúIf every model was a company, the model‚Äîin this example‚Äîis actually profitable.‚Äù Despite high burn rates, speculation indicates many of the frontier labs enjoy strong unit economics on a flagship model basis.
Inference pays for training: labs strive to allocate more of a model‚Äôs lifecycle compute to revenue-generating inference at the steepest margin possible. Our table* below illustrates the expected return on compute costs across varying inference margins and compute allocations.    
*Simplified sensitivity analysis: neglects people costs and assumes all inference generates revenue. Can also be interpreted in terms of token count between inference & training (2DN vs. 6DN, MFU: ~15% vs. ~45%). 
When do we see profitable models? Or are we there yet?
Source:https://www.youtube.com/watch?v=GcqQ1ebBqkc

ChatGPT Search offers real-time web results by searching Google and a new Agent that spins up a virtual browser to execute multi-step tasks via tool use within ChatGPT with user control. It‚Äôs also rumored to be launching a standalone browser. 
Taking another route, Perplexity built their own Chrome-based browser, Comet, with a native AI assistant sidebar. It can perform Q&A but also complete multi-step tasks in the browser (filling forms, scraping), again with user oversight.
Anthropic and Google released limited previews of Claude for Chrome and Gemini in Chrome, respectively, that also let users operate the browser and access Q&A functionality. Anthropic no longer deems this use case to be dangerous. 
In Sept ‚Äò25, Atlassian acquired The Browser Company (makers of Arc) further reaffirming that browsers are the latest AI battleground.
    Users live in the browser, so why shouldn‚Äôt AI be baked into the experience? This is finally happening. OpenAI, Google, Anthropic, and Perplexity all launched assistants that not only unlock Q&A with web content but also navigate and act within the browser on behalf of the user. This shift reframes the browser as an intelligent operating system for the internet, a long sought vision that earlier attempts like Adept AI never fully realized.
stateof.ai 2025
 | 112
Browsers become the latest AI battleground
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Source: https://openai.com/index/introducing-chatgpt-agent/
Source: https://www.atlassian.com/blog/announcements/atlassian-acquires-the-browser-company 


By August 2025, ChatGPT served 755M monthly users, giving it ~60% of the AI search market.
SEM Rush data shows Google‚Äôs global search traffic fell ~7.9% year-over-year, the first significant dip in decades, even as it retained ~90% global share. Similarweb data shows Google search visits down 1-3% YoY throughout H1 2025, with Bing (-18%) and DuckDuckGo (-11%) also declining.
Perplexity queries hit 780M in May 2025, growing 20% month-over-month, as citation-rich answers drew loyal users.
‚ÄúChatGPT‚Äù itself became a top Google search term with 618M monthly queries, rivaling ‚ÄúFacebook‚Äù.
    With 700M weekly active users, ChatGPT is evangelising AI-powered search to the masses, reshaping how people discover and use information. Google‚Äôs once unshakeable dominance shows the first signs of erosion, even as they pivot to AI Overviews (AIO) and AI Mode within Google Search. Moreover, AIO has driven a ~90% drop in Search click-throughs, harming traditional ads, but ignores users being influenced by the AI answers.

stateof.ai 2025
 | 113
As AI search engines surge, Google‚Äôs search and ad offering is taking heat‚Ä¶
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Source: https://firstpagesage.com/seo-blog/chatgpt-usage-statistics/ 
Source: https://www.semrush.com/blog/google-search-statistics/ 
Source: https://techcrunch.com/2025/07/09/perplexity-launches-comet-an-ai-powered-web-browser/ 
Source: https://www.similarweb.com/corp/wp-content/uploads/2025/07/attachment-Global-AI-Tracker-16.pdf
Source: https://www.theguardian.com/media/2025/sep/06/existential-crisis-google-use-ai-search-upended-web-publishers-models
Source: https://assets.publishing.service.gov.uk/media/68a5aa50a6acbbc7fb96a3b2/DMG_Media.pdf 

    According to Similar Web data, retail visits referred by ChatGPT now convert better than every major marketing channel measured. Conversion rates rose roughly 5pp YoY, from ~6% (Jun ‚Äô24) to ~11% (Jun ‚Äô25). Although AI referrals are still a smaller slice of traffic, they arrive more decided and closer to purchase. Retailers must adapt by exposing structured product data, price and delivery options, and landing pages tailored to AI-driven intents. In fact, ChatGPT recently implemented Instant Checkout with Etsy and Shopify, and open-sourced their Agentic Commerce Protocol built with Stripe, to enable developers to implement agentic checkout. 
stateof.ai 2025
 | 114
AI search is emerging as a high-intent acquisition channel
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Source: https://www.similarweb.com/blog/insights/ai-news/google-reddit-dominate-in-genai-referrals-to-tech-platforms/?utm_medium=social&utm_source=twit 
Source: ‚Äã‚Äãhttps://x.com/openai/status/1972708279043367238?s=46&t=8YCMEcmVVXRPm8SXTMgdlw 

Despite strategic partnerships with Microsoft and access to Bing OpenAI chooses to scrape Google search results as its web search system. 
During Google‚Äôs antitrust trial access to a quality web index was discussed by Anthropic, OpenAI, Perplexity, etc. Seeking to be able to create the same quality as google for 80% of queries. 
Remediations from the the trial will grant ‚ÄòQualified Competitors‚Äô a one time index dump without any of the ranking signals. It‚Äôs unclear if this dump will lead to any real competitor to the search system Google has been perfecting for decades. 
    While the LLM based interface has been the focus of funding, lawsuits, and user behavior, no one has found a good alternative to using Google search. 
stateof.ai 2025
 | 115
‚Ä¶but AI can‚Äôt get away from Google Search
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
AI referral traffic overwhelmingly funnels to Big Tech domains, led by google.com (Similar Web)
Source: https://www.theinformation.com/articles/openai-challenging-google-using-search-data
Source: https://ecf.dcd.uscourts.gov/cgi-bin/show_public_doc?2020cv3010-1436 
Source: https://www.similarweb.com/corp/wp-content/uploads/2025/07/attachment-Global-AI-Tracker-16.pdf
Source: https://www.similarweb.com/blog/insights/ai-news/google-reddit-dominate-in-genai-referrals-to-tech-platforms/

In an average session, users send ~5 prompts and receive ~5 responses, far more interaction than a typical search query, scroll, and blue-link click.
Profound data shows ChatGPT users average 5.6 turns per session, versus ~4 for Gemini and Perplexity, and ~3.9 for DeepSeek. Either more turns means more engaging conversations, or fewer turns means answers are given more efficiently. 
Conversation styles differ: DeepSeek users write the longest prompts and get the most verbose answers, while Perplexity delivers shorter, citation-heavy responses.
This iterative style and memory capability makes answer engines ‚Äústicky‚Äù and explains why they already deliver higher conversion rates than Google.
Profound‚Äôs analysis shows ChatGPT‚Äôs crawler is now among the top 10 most active bots on the internet, alongside Googlebot and Bingbot.
    Data from answer engine optimization company Profound shows that users treat AI answer engines differently from Google. Sessions are longer, with more back-and-forth, suggesting higher intent and better conversion potential. Answer engines are no longer just a curiosity - they‚Äôre a primary entry point for serious queries.
stateof.ai 2025
 | 116
Answer engines drive deeper engagement than search
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Source: Profound data

Profound data shows GPT-5‚Äôs citations matched 19% of Google domains when compared against the top 10 Google results, underscoring both reliance on Google‚Äôs index and a broader sourcing pattern.
Avg citation position also shifted down the page, while the median stayed at #9, meaning that ChatGPT is just as likely to surface content further down Google‚Äôs results page. 
ChatGPT often pulls from lower-ranked pages than humans typically click, widening exposure for sites beyond the top results.
Top domains cited across models: Reddit (3.5%), Wikipedia (1.7%), YouTube (1.5%), and Forbes (1.0%).
Different models show sourcing styles: Gemini and Perplexity lean toward mainstream concise sources, while DeepSeek tends to draw on long-form domains.
This means optimizing for Answer Engine Optimization (AEO) is as important as SEO because visibility depends not just on rank, but on model citation patterns.
    Understanding how AI answer engines cite and retrieve information is critical for visibility on AI-first web. Profound‚Äôs analysis shows ChatGPT draws heavily from Google‚Äôs index but distributes attention differently across the web, with lower-ranked pages often getting visibility. This behavior changes with new model versions too.
stateof.ai 2025
 | 117
So where do answer engines get their answers?
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Source: Profound data

    
stateof.ai 2025
 | 118
Vibe shift: From litigation‚Ä¶ 
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Source: https://www.theguardian.com/music/article/2024/jun/25/record-labels-sue-ai-song-generator-apps-copyright-infringement-lawsuit
Source: https://www.nytimes.com/2023/12/27/business/media/new-york-times-open-ai-microsoft-lawsuit.html
Source: https://www.bbc.co.uk/news/articles/cy7ndgylzzmo?utm_source=www.therundown.ai&utm_medium=newsletter&utm_campaign=big-tech-s-ai-shopping-spree&_bhlid=36d32e79bc54581f79fb3bedf10333163e2d8a03
Source: https://edition.cnn.com/2023/01/17/tech/getty-images-stability-ai-lawsuit
Source: https://www.axios.com/2025/06/11/disney-nbcu-midjourney-copyright

    2025 is the year when "if you can't beat 'em, join 'em" became official media strategy for AI companies. 
stateof.ai 2025
 | 119
‚Ä¶ to collaboration
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
News: Over 700+ news brands have signed AI deals, including the Washington Post, WSJ, Guardian, FT, The Atlantic, Cond√© Nast, and even NYT ($20-25M Amazon deal) (as they continue to sue OpenAI).
Music: Hallwood pens deal with top-streaming ‚Äúcreator‚Äù on Suno, Grammy winner Imogen Heap releases AI style-filters for fans to remix.
Video: AMC Networks formally embraces Runway AI for production (first major cable network to do so).
Publishing: Microsoft & HarperCollins deal for AI training (with author opt-outs).

Source: https://petebrown.quarto.pub/pnp-ai-partnerships/
Source: https://www.theguardian.com/books/2024/nov/19/harpercollins-tech-firms-books-train-ai-models-nonfiction-artificial-intelligence
https://www.hollywoodreporter.com/news/music-news/hallwood-inks-record-deal-ai-music-designer-imoliver-1236328964/?utm_source=www.therundown.ai&utm_medium=newsletter&utm_campaign=china-s-cooperative-ai-counter-punch&_bhlid=30563816d3015e04a691137215c691e5e4a2879b
Source: https://runwayml.com/news/runway-amc-partnership
Source: https://www.billboard.com/pro/major-record-labels-music-licensing-talks-ai-companies/
Source: https://openai.com/index/api-partnership-with-stack-overflow/
Source: https://www.google.com/url?q=https://openai.com/index/conde-nast/&sa=D&source=editors&ust=1758666664296392&usg=AOvVaw2e_rsnCHCnyC1s_3OvPIDC 

    Shortly after the announcement of their record-breaking $13B Series F, Anthropic agreed to pay $1.5B to settle a class action lawsuit from book authors. This is the largest payout in the history of US copyright cases, and constitutes what some describe as ‚Äúthe A.I. industry‚Äôs Napster moment‚Äù.
stateof.ai 2025
 | 120
Fair p(l)ay out 
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
This does not set legal precedent as the case did not go to trial, but is a very significant development in the ongoing fair use debate. Since this was an ‚Äúopt-out‚Äù class action, authors who are eligible can request exclusion to file independent lawsuits. Anthropic also agreed to delete works that had been downloaded. 
ü§ë
In June, a judge sided with Anthropic, ruling that training LLMs on legally purchased books was sufficiently transformative to constitute fair use. He also ruled that training on pirated copies was illegal. Previously Anthropic had hired Tom Turvey, the former head of Google Books, who mass bought physical books and then created digital copies that were used for model training. 
During a deposition, co-founder of Anthropic Ben Mann testified to having downloaded the LibGen dataset (which contains pirated material) when he previously worked at OpenAI‚Ä¶ 
Source: https://www.nytimes.com/2025/09/05/technology/anthropic-settlement-copyright-ai.html
Source: https://fingfx.thomsonreuters.com/gfx/legaldocs/jnvwbgqlzpw/ANTHROPIC%20fair%20use.pdf?utm_source=www.therundown.ai&utm_medium=newsletter&utm_campaign=ai-training-gets-legal-clarity&_bhlid=68bd751552175e1ea76aff95243ea6826a1e9b85
Source: https://www.theguardian.com/technology/2025/jun/25/anthropic-did-not-breach-copyright-when-training-ai-on-books-without-permission-court-rules
Source: https://x.com/natolambert/status/1964088501734887726

stateof.ai 2025
 | 121
Welcome to the Stargate: may the FLOPS be with you
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
    10 years ago, Baidu, Google and others had shown early scaling laws whereby deep learning models for speech and image recognition converged faster as more GPUs were used for training. Back then, this meant using up to 128 GPUs. In January this year, Sam Alman, Masayoshi Son, Larry Ellison and President Trump announced the Stargate Project, a gigantic 10GW-worth of GPU capacity to be built with $500B in the US over 4 years. This equates to over 4 million chips! The buildout is to be funded by SoftBank, MGX, Oracle, and OpenAI.
How it started in 2015
How it‚Äôs going in 2025
Source: https://archive.is/Z8GbU 
Source: https://ai.googleblog.com/2016/04/announcing-tensorflow-08-now-with.html and https://arxiv.org/vc/arxiv/papers/1501/1501.02876v1.pdf and https://arxiv.org/pdf/1512.02595 
Source: https://openai.com/index/announcing-the-stargate-project
Source: https://openai.com/index/stargate-advances-with-partnership-with-oracle/ 

Stargate UAE is the first deployment of a 1GW campus with a 200 MW live target in 2026. Partners include G42, Oracle, NVIDIA, Cisco, and SoftBank).
Stargate Norway comes second and is a 50/50 joint venture between UK‚Äôs Nscale and Norway‚Äôs Aker to deliver 230 MW capacity (option to +290 MW) and 100,000 GPUs by end-2026. 
Stargate India is reportedly in the works for 1 GW as OpenAI expands and offers a cheaper ‚ÄúChatGPT Go‚Äù.
stateof.ai 2025
 | 122
OpenAI franchises sovereign AI with its ‚ÄúOpenAI for Countries‚Äù program
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
    Energy-rich nations are grabbing their ticket to superintelligence by partnering with OpenAI‚Äôs astute sovereign offering: a formalized collaboration to build in-country AI data center capacity, offer custom ChatGPT to citizens, raise and deploy funds to kickstart domestic AI industries and, of course, raise capital for Stargate itself. 
Source: https://www.linkedin.com/posts/barbarahyman_nevermissoutontalent-equitythroughai-hiring-activity-7036915141715648512-QAHi/ 
Source: https://openai.com/global-affairs/openai-for-countries/ 
Source: https://openai.com/index/introducing-stargate-norway/
Source: https://openai.com/index/introducing-stargate-uae/
Source: https://www.reuters.com/world/india/openai-plans-india-data-center-with-least-1-gigawatt-capacity-bloomberg-news-2025-09-01/ 

stateof.ai 2025
 | 123
OpenAI races to own entire AI stack
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
    After shelving its robotics program in 2020 to focus on language models, OpenAI has reversed course, now driving full vertical integration from custom chips and data centers to models, devices, and embodied AI. 

Silicon & Compute
Models
Hardware & Robotics

Custom Chips: Developing in-house AI processor in partnership with Broadcom, targeting 2026 launch, to cut NVIDIA reliance. 
Data Centers: Texas Stargate supercluster: 400k GPUs, 1.2 GW capacity, Oracle partnership; part of $500B build-out to secure compute supply.
Consumer Devices: $6.5B acquisition of io (Jony Ive) to create AI-native devices, bypassing existing iOS/Android.
Robotics: Internal robotics division reboot; partnership with Figure AI (since terminated).
Source: https://techcrunch.com/2025/01/10/new-openai-job-listings-reveal-its-robotics-plans/
Source: https://www.ft.com/content/e8cc6d99-d06e-4e9b-a54f-29317fa68d6f

stateof.ai 2025
 | 124
Broadcom‚Äôs great transformation
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Broadcom's 2013 LSI acquisition included a small custom chip unit that now designs Google TPUs and Meta's AI chips, growing from <20% of LSI's revenue at acquisition to $2-3B+ annually.
Broadcom's stock price has surged, signaling investor optimism about the company's ability to benefit from the rapidly growing AI chip market.
    Once an unglamorous semiconductor firm, Broadcom has now positioned itself at the cutting edge of the AI revolution through its custom chip partnerships with Google, Meta, and reportedly OpenAI. The development of custom AI chips like Amazon's Trainium and Broadcom‚Äôs TPUs / MTIA chips gives frontier labs more leverage when negotiating multi-billion dollar deals with NVIDIA.
Broadcom's AI chip revenue reached $5.2B in Q3 2025, up 63% year-over-year.
The custom chip ecosystem puts pressure on NVIDIA's monopoly: Amazon's in-house Trainium chips and Broadcom-powered alternatives (Google TPUs, Meta MTIA, OpenAI's upcoming chips) give hyperscalers multiple paths to reduce NVIDIA dependence, if they‚Äôre willing to endure the user pain. 
Source: https://www.asktraders.com/analysis/broadcom-stock-avgo-making-highs-analysts-target-400-after-earnings-beat/
Source: https://semianalysis.com/2023/08/30/broadcoms-google-tpu-revenue-explosion/#a-brief-historical-overview-of-broadcom-and-its-roll-up-strategy

stateof.ai 2025
 | 125
Google‚Äôs TPU timeline
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions

stateof.ai 2025
 | 126
OpenAI and its benefactor, Microsoft, navigate a rocky relationship
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
    OpenAI‚Äôs recent restructuring and soaring demand for training compute has placed tremendous stress on its relationship with Microsoft. While signs of fracturing become noticeable, a complete divorce looks unlikely. 

"We are below them,  above them, around them."
Microsoft‚Äôs inability, or unwillingness, to bring training compute online fast enough has impacted OpenAI‚Äôs roadmap. As OpenAI has gravitated closer to Oracle to fulfill these needs, Microsoft has abstained from exercising its ‚Äúright of first refusal.‚Äù They appear hesitant to bet on next-generation centralized clusters. 
Meanwhile, OpenAI appears to be trying to escape from other key elements of their partnership with Microsoft. Through 2030, Microsoft maintains a 20% revenue sharing arrangement, access to OpenAI‚Äôs IP, and exclusivity on OpenAI‚Äôs API. Yet, OpenAI wants the 20% share dialed back to 10% before the end of the decade. 
Microsoft can always work to block the for-profit‚Äôs conversion into a PBC, which could cost OpenAI $20B in funding if not completed by the end of 2025. Conversely, OpenAI always retains the option to air antitrust concerns if Microsoft proves adversarial or reneges on certain AGI clauses. 
Microsoft AI also released previews of a voice and MoE model trained on 15k H100s.
Source: https://www.cnbc.com/2025/01/21/microsoft-loses-status-as-openais-exclusive-cloud-provider.htmlSource: https://blogs.microsoft.com/blog/2025/01/21/microsoft-and-openai-evolve-partnership-to-drive-the-next-phase-of-ai/Source: https://www.theinformation.com/briefings/openai-plans-slash-revenue-share-microsoft-restructuringSource: https://www.wsj.com/tech/ai/openai-and-microsoft-tensions-are-reaching-a-boiling-point-4981c44fSource: https://microsoft.ai/news/two-new-in-house-models/ 


stateof.ai 2025
 | 127
Oracle steps up as a key buildout partner for AI infra
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
    As Microsoft has dialed back its willingness to shoulder so much of the future AI infra buildout, Oracle has begun picking up slack. During the early phase of this shift, Oracle has been rewarded as its stock soars.

OpenAI has reached a $30B per year agreement with Oracle for data center services. This deal more than doubled Oracle‚Äôs collective fiscal 2025 cloud service revenue, when it sold just $24.5B worth of services. 

This news comes as OpenAI‚Äôs deal with Softbank begins to fray. Recently, original Stargate Project plans were scaled-back, tempering those roadmaps. 
Oracle now fills this vacuum, proving to have found the risk-appetite and follow-through that both Microsoft and SoftBank seem to lack. As a consequence, Oracle‚Äôs stock has jumped more than >70% year to date.
Oracle‚Äôs current track is not without major risks. Providing large-scale clusters has not been exceptionally high-return, particularly as power bottlenecks are costly to overcome. AI lab tightness could eventually raise issues for longer leases. Finally, depreciation cliffs present concerns and the economics of converting these clusters to inference fleets remains murky. More decentralized builds could then win out, especially if scaling continues to shift to RL.
Source: https://techcrunch.com/2025/07/22/openai-agreed-to-pay-oracle-30b-a-year-for-data-center-services/Source: https://www.wsj.com/tech/ai/softbank-openai-a3dc57b4

stateof.ai 2025
 | 128
AI labs target 2028 to bring online 5GW scale training clusters  
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
    Anthropic shared expectations that training models at the frontier will require data centers with 5GW capacity by 2028, in line with the roadmaps of other labs. The feasibility of such endeavors will depend on many factors:

How much generation can hyperscalers bring behind-the-meter? At such scale, islanding will likely not be practical, requiring data centers to tap into grid assets.
How quickly can players navigate the morass of permitting and interconnection? While reforms are underway, connection timelines for projects of this magnitude can take many years. Hyperscalers may skip queues through lobbying efforts and demand response programs, where they curtail draw during peak periods (a recent Duke study projects 76GW of new availability with curtailment rates at 0.25%). 
What level of decentralization can be achieved? Many labs continue to pursue single-site campuses, yet distributed approaches are also advancing rapidly. 
How well can hyperscalers navigate talent and supply chain shortages? Attempts to alleviate power infrastructure and skilled labor bottlenecks through the massive mobilizations of capital can overload the risk-appetite of supporting parties.

Source: https://nicholasinstitute.duke.edu/sites/default/files/publications/rethinking-load-growth.pdfSource: https://www.csis.org/analysis/power-access-ai-flexibility-compactSource: https://www-cdn.anthropic.com/0dc382a2086f6a054eeb17e8a531bd9625b8e6e5.pdfSource: https://cointelegraph.com/news/openai-oracle-stargate-expansion-musk-xai-h100-plan


stateof.ai 2025
 | 129
Measuring contest: planned ~1GW scale clusters coming online in 2026
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
    Cluster size increasingly becomes a defining trait amongst American labs, particularly useful during recruitment. Should valuations follow cluster size instead of adoption or fiscal metrics, a larger bubble could begin to form. 

 GW Scale Cluster Rankings 
*Google DeepMind has also spun up many noteworthy clusters in Iowa, Nebraska, and Ohio. However, the distributed nature of these projects and lack of available information led to this omittance from the table.
Source: https://semianalysis.com/2025/06/30/how-oracle-is-winning-the-ai-compute-market/#Source: https://www.tomshardware.com/tech-industry/artificial-intelligence/elon-musk-says-xai-is-targeting-50-million-h100-equivalent-ai-gpus-in-five-years-230k-gpus-including-30k-gb200s-already-reportedly-operational-for-training-grokSource: https://x.com/elonmusk/status/1947701807389515912Source: https://www.wsj.com/tech/ai/elon-musk-x-ai-funding-feecede1

 | 130
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
stateof.ai 2025
A 1GW AI data center cheat sheet

stateof.ai 2025
 | 131
Shortfall forecasts spike and consequences loom around the corner 
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
    NERC reported that electricity shortages could occur within the next 1-3 years in several major US regions. DOE warns blackouts could be 100 times more frequent by 2030 due to unreliability and new AI demand.
Similarly, SemiAnalysis projects a 68 GW implied shortfall by 2028 if forecasted AI data center demand fully materializes in the US. 
As an emerging pattern, this will force firms to increasingly offshore the development of AI infrastructure. Since many of the US‚Äô closest allies also struggle with electric power availability, America will be forced to look toward other partnerships ‚Äì highlighted by recent deals in the Middle East. 
Projects that are realized on American soil will place further strain on the US‚Äô aging grid. Supply-side bottlenecks and rapid spikes in AI demand threaten to induce outages and surges in electricity prices. ICF projects residential retail rates could increase up to 40% by 2030. These factors could further contribute to the public backlash directed at frontier AI initiatives in the US. 

Source: https://www.rand.org/pubs/research_reports/RRA3845-1.htmlSource: https://semianalysis.com/2025/06/25/ai-training-load-fluctuations-at-gigawatt-scale-risk-of-power-grid-blackout/Source: https://x.com/SemiAnalysis_/status/1937231612317696354Source: https://www.nerc.com/pa/RAPA/ra/Reliability%20Assessments%20DL/NERC_Long%20Term%20Reliability%20Assessment_2024.pdfSource: https://www.icf.com/-/media/files/icf/reports/2025/energy-demand-report-icf-2025_report.pdf?rev=c87f111ab97f481a8fe3d3148a372f7fSource: https://www.energy.gov/articles/department-energy-releases-report-evaluating-us-grid-reliability-and-security

stateof.ai 2025
 | 132
Could data centers ever truly be green?
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
    The widespread AI buildout places data center emissions on a steeper trajectory, while creative carbon accounting techniques continue to understate the true emissions associated with many hyperscalers.
Data center-related emissions could surge as more providers island with natural gas plants and grid operators recommission or delay the retirement of existing coal plants.
As AI factories are ultimately offshored to other regions, the carbon intensity of those locations is likely to be much higher than that of the US, requiring cloud providers to pursue more aggressive procurements of carbon offsets.
Deceptive carbon accounting practices are also prominent. Some hyperscalers omit upstream categories, such as the emissions associated with manufacturing IT equipment and constructing or maintaining the relevant power plants. Furthermore, additionality agreements can cover new renewable projects that were already planned to commence.
Source: https://on.ft.com/4m8Toxc&sa=D&source=editors&ust=1754619778891752&usg=AOvVaw1xgT8lnHbjI4TEdpHJaNRZSource: https://www.carbone4.com/en/article-digital-cloud-hidden-emissions

 | 133
Just how thirsty are AI factories? 
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
    AI factories withdraw considerable amounts of water and are more likely to be built in high water-stress areas. Still, the Water Usage Efficiency (WUE) of most AI factories are trending in a favorable direction.   
An average 100 MW hyperscale data center in the US consumes roughly 2M Liters per day, mostly due to the indirect toll associated with power generation. Yet, as modern AI data centers shift to closed-loop liquid cooling solutions, their WUE plummets relative to other traditional data centers. 
However, second-order costs cannot be ignored as the number of new AI factories continues to surge, leading to more generation coming online. In the US, this creates a geographic mismatch: the sites where power is available or can be easily built often sit in drier climates with water stress. This trend may be exacerbated by the shift to water-intensive generation sources ‚Äì particularly nuclear, coal, & certain natgas plants. 
Currently, everyday AI usage carries negligible impacts ‚Äì a typical Gemini app text prompt consumes only 0.26 mL of water (~5 drops). Yet, WUE must be monitored as AI interactions continue to use more tokens.
stateof.ai 2025
Source: https://www.bloomberg.com/graphics/2025-ai-impacts-data-centers-water-data/?embedded-checkout=trueSource: https://www.theguardian.com/environment/2025/apr/09/big-tech-datacentres-waterSource: https://services.google.com/fh/files/misc/measuring_the_environmental_impact_of_delivering_ai_at_google_scale.pdfSource: https://x.com/JeffDean/status/1958525015722434945Source: https://x.com/AlecStapp/status/1885192778256179605Source: https://www.iea.org/reports/energy-and-aiSource: https://docs.nrel.gov/docs/fy11osti/50900.pdf

stateof.ai 2025
 | 134
Google inks PPA deal with CFS to buy ‚â•200 MW of electricity from planned fusion plant
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
    Google‚Äôs commitment signals demand for an energy source that will not be deployment-ready until early next decade, kicking off a new wave of investment.
While support for next-generation energy sources holds strong, fusion remains many years away from providing a scalable solution to AI‚Äôs demand for power. AI‚Äôs energy footprint traces a very steep curve, yet the timelines of next-generation sources are not compressing.
This marks another shift: hyperscalers, rather than the US government, are increasingly shouldering investments in technologies that are many years away from commercial viability ‚Äì such as fusion, quantum computing, and advanced AI development.
Source: https://www.reuters.com/sustainability/climate-energy/google-strikes-deal-buy-fusion-power-mit-spinoff-commonwealth-2025-06-30/

stateof.ai 2025
 | 135
Chinese labs have yet to advance the frontier, but compete in the open-weight domain
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
    Chinese labs like Alibaba, DeepSeek, Moonshot, and MiniMax continue to release impressive open-weight models. A capability gap emerges between these models and most American open-source alternatives. 
US open model efforts have disappointed. OpenAI‚Äôs open-weight models underwhelmed with performance trailing far behind GPT-5. 
The restructuring of Meta‚Äôs ‚ÄúSuperintelligence‚Äù team has cast doubts on their commitment to open-sourcing at the frontier. Other teams like Ai2 lag far behind in terms of funding. Although they recently landed $152M from NVIDIA and NSF, that figure pales in comparison to even OpenAI‚Äôs initial grant from 2015.
Conversely, Chinese organizations continue to push the envelope, while publishing troves of new algorithmic efficiency gains.  

Source: https://artificialanalysis.ai/models/open-sourceSource: https://techcrunch.com/2025/07/11/openai-delays-the-release-of-its-open-model-again/Source: https://www.nytimes.com/2025/07/14/technology/meta-superintelligence-lab-ai.htmlSource: https://www.interconnects.ai/p/the-american-deepseek-project

stateof.ai 2025
 | 136
‚Ä¶yet is this an overt strategy or a side-effect of China‚Äôs inability to scratch the frontier
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
    China‚Äôs commitment to the open-source community could be a lasting tactic or a short-term play exercised to reach frontier-level capabilities. It has already proven an effective method in catching-up to the pack. 

While open-source projects can successfully build mind-share, competitive realities exist. Proprietary options make greater commercial sense once a lead has been established, easing the generation of returns and protecting algorithmic unlocks. 
Yet, China‚Äôs recent AI Action Plan did include an entire section dedicated to upholding the responsibility of ‚Äú[building] transnational open source communities and safe and reliable open source platforms.‚Äù This theme has been a familiar theme in other messaging produced by the CCP. 
Other Chinese AI leaders, such as Liang Wenfeng, have grown invested in open source culture, viewing their contributions as a means of earning global recognition and ‚Äúrespect.‚Äù

Source: https://www.gov.cn/yaowen/liebiao/202507/content_7033929.htmSource: https://www.reuters.com/breakingviews/chinas-love-open-source-ai-may-shut-down-fast-2025-04-02/Source: https://www.chinatalk.media/p/deepseek-ceo-interview-with-chinas

stateof.ai 2025
# | 137
Flip-Flop: new chip restrictions imposed then dropped‚Ä¶but has the damage been done?
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
    BIS originally sent letters to NVIDIA & AMD announcing the requirement of licenses for the sale of H20 & MI308 chips to China, effectively halting sales. Months later, the Trump administration walked back these controls. 
The CCP immediately denied claims that this concession was linked to ongoing trade negotiations, sparking speculation the move was instead intended to contain Chinese chipmakers such as Huawei. Others view this pivot as an attempt to ensure NVIDIA plays ball amid location verification initiatives like the Secure Chip Act. 
NVIDIA welcomed this shift, announcing its plan to fulfill existing orders. However, Chinese CSP‚Äôs have canceled these purchases and the production of H20 line recently halted. Instead, NVIDIA await directives from both countries in its hopes to launch a B30A line, based on the Blackwell architecture. 
Due to strategic interdependencies, attempts to deepen China‚Äôs dependence on the American AI accelerator ecosystem carries tradeoffs at the model layer. In this scenario, Chinese labs can continue to tap into high bandwidth compute, improving their ability to both serve customers and develop RL-heavy reasoning models. Although smuggling appears inevitable, export decisions represent a swinging pendulum between these two layers of the stack. 
Source: 

China‚Äôs internet regulator CAC, state planner NDRC, and industry ministry MIIT told large platforms to halt new H20 orders and urged avoiding all NVIDIA chips, pushing inference to local parts.
Three Huawei-serving fabs and leading foundry SMIC (Semiconductor Manufacturing International Corp.) plan ramps that could triple China‚Äôs AI-chip output in 2026; SMIC also aims to double 7nm capacity.
DeepSeek‚Äôs FP8 format is guiding domestic designs, while CXMT (ChangXin Memory Technologies) is testing HBM3 for local stacks.
Cambricon is an early winner, posting Rmb 1B H1 profit on a 44√ó revenue
    After U.S. export flip-flops and Lutnick remarking that ‚ÄúYou want to sell the Chinese enough that their developers get addicted to the American technology stack, that‚Äôs the thinking,‚Äù Beijing pivoted from mitigation to home-grown substitution. Regulators steered demand off NVIDIA while fabs and suppliers scaled domestic options.
stateof.ai 2025
 | 138
China getting addicted to ‚Äúthird-rate‚Äù US chip technology? No, sir (‰∏ç‰ºöÂêß!)
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
jump as ByteDance/Tencent shift to homegrown inference chips. Its has ripped over 100% since the news. 
China can afford to build systems that are less efficient in terms of flops/watt because they are not power constrained. 
Source: https://www.ft.com/content/64caeab8-a326-4626-98fb-e1bf665827d3
Source: https://www.ft.com/content/b8e30c54-b71c-4113-8b3e-8f54bc36587d 

stateof.ai 2025
 | 139
Rampant smuggling also shifts export control calculus
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
    During the temporary H20 ban, $1B worth of NVIDIA chips were smuggled to China. Markups are rumoured to float around 50%, relatively low for black-market products, suggesting a deep supply of diverted GPUs in China. 
Similarly, NVIDIA, who long maintained a stance denying any evidence of diversions, finally recognized ongoing smuggling. NVIDIA framed such activity as a ‚Äúlosing proposition,‚Äù since they only provide service for authorized data center products. Steps to prevent future diversions are technically feasible through location-based attestation firmware, yet these mitigations are not completely bulletproof.
Smuggling patterns appeared to intensify during the ban, with a sharp drop off following the reversal. Based on this relationship, Chinese AI efforts seem to prefer defanged NVIDIA chips over domestic offerings and smuggled GPUs, which carry markups, compliance risks, and lacking support. Sliding-scale restrictions could work to weaken China‚Äôs smuggling muscle, directing more SOTA chips to the West.
Source: https://www.ft.com/content/6f806f6e-61c1-4b8d-9694-90d7328a7b54Source: https://www.cnbc.com/2025/07/24/nvidia-ai-chips-smuggling-china-trump.htmlSource: https://www.bloomberg.com/news/articles/2025-05-17/nvidia-ceo-sees-no-evidence-of-ai-chip-diversion-into-chinaSource: https://www.iaps.ai/research/location-verification-for-ai-chips

stateof.ai 2025
 | 140
China mobilizes 115,000 restricted GPUs in massive data center project
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
    China plans to build a sprawling fleet of 39 new AI data centers, largely in Xinjiang and Qinghai, using unauthorized Hopper GPUs. Of the redirected H100s and H200s, roughly 80k are designated to be deployed in a single state-owned cluster in Yiwu county. 

News of the buildout points to the scale and sophistication of the black-market operations in China. State-involvement also suggests CCP leadership has begun to awaken in the frontier AI competition.
The centralized cluster will meaningfully advance the scale available to Chinese labs. Published claims surrounding SOTA Chinese models indicate today‚Äôs systems have been trained using 1k-10k GPUs. 

Source: https://www.bloomberg.com/graphics/2025-china-data-centers-nvidia-chips/Source: https://www.tomshardware.com/pc-components/gpus/china-plans-39-ai-data-centers-with-115-000-restricted-nvidia-hopper-gpus-move-raises-alarm-over-sourcing-effectiveness-of-bans

stateof.ai 2025
 | 141
From DeepSeek‚Äôs deep freak out morphs into a full-tilt on Jevons Paradox
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
üò®
üòÆ‚Äçüí®
üòá
‚Ä¶ but wait, $5M* is only for the final training run not the entire project‚Ä¶
Panic!!! Frontier AI for $5M!!
Markets wipe $600B from NVIDIA in 1 day!!
Cheaper intelligence ‚Üí more demand ‚Üí more chips 
‚áí more usage
*‚ÄùNote that the aforementioned costs include only the official training of DeepSeek-V3, excluding the costs associated with prior research and ablation experiments on architectures, algorithms, or data"
Source: https://arxiv.org/html/2412.19437v1#S4
Source: https://www.reddit.com/r/aipromptprogramming/comments/1ibaafv/the_deepseek_effect_12_trillion_ai_wipeout/
Source: https://www.ft.com/content/926f1d7b-48d8-46ac-aca7-9acb81e02c13

stateof.ai 2025
 | 142
Sparking backlash from US labs invoking data theft and the need for chip bans
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Source: https://www.ft.com/content/a0dfedd1-5255-4fa9-8ccc-1fe01de87ea6 
Source: https://www.darioamodei.com/post/on-deepseek-and-export-controls 

PROS
CONS
stateof.ai 2025
 | 143
Assessing the current pros and cons of modern chip export controls
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions

Worse options can stunt AI investment. There is already a 10:1 AI capex gap between the US & China.
Fewer aggregate flops blocks adoption by cutting off the number of agents and assistants available.
Frees up capacity for American AI efforts.
Destroys the bridge available to adversaries as they continue to pursue self-reliance.
Dries up the resources available to adversaries for AI-related military-civil fusion projects.
Gatekeeping offers government revenue streams.
Cutting off supply introduces constraints that makes     it harder for adversaries to export their stack. 


Developers that are forced off the American stack then bolster foreign software ecosystems. 
Lost cash flow can cause a drag on US R&D/M&A,  while supporting the spend of foreign competitors.
Stricter controls incentivize smuggling operations.
Controls can indiscriminately block the benefits of AI diffusion, provoking retaliation (e.g. REE controls).
Enforcement challenges can strain relationships with nations where channels for diversion exist. 
Enacting defensive measures, like location-based guardrails, could make foreign options more attractive if overreach is suspected. 

Source: https://docs.google.com/document/d/1yE12cJYHUixkO5mUGiufHJQbkNUohoP115VVbhib1ac/edit?tab=t.0#heading=h.7hvt4ogxz9fl
Source: https://www.ft.com/content/cd1a0729-a8ab-41e1-a4d2-8907f4c01cacSource: https://www.chinatalk.media/p/chinas-models-close-the-gap

stateof.ai 2025
 | 144
AI supercomputer supremacy: US domination and corporate concentration
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
    The US controls ~75% of global AI supercomputer capacity with 850,000 H100-equivalents compared to China's 110,000. What‚Äôs more, the concentration of compute power has shifted from public to private hands, with companies now controlling 80% of AI supercomputers (up from 40% in 2019). Despite this massive compute advantage and export controls, China is consistently shipping very capable open weight models - more frequently and across the spectrum of modalities. 
US computational performance is 9x China's and 17x the EU's. This creates a self-reinforcing cycle where compute advantage drives breakthroughs that attract more investment.
By 2030, the leading AI supercomputer could require 2 million chips, $200 billion, and 9 GW of power (equivalent to nine nuclear reactors), making power grid capacity rather than chips or capital the likely binding constraint.
The 40% ‚Üí 80% private sector shift limits academic compute access and reduces government visibility into AI development, as companies can afford $7B systems while government projects max out at $600M, creating both a research bottleneck and a policy blindspot.
Source: https://arxiv.org/pdf/2504.16026

stateof.ai 2025
 | 145
Racing toward indigenous semiconductor capacity
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Thus far, SME export controls appear to have proven effective at slowing Chinese capabilities. Expectations remain that SMIC will launch full-scale 5nm operations early next year, yet yields are unlikely to reach the levels of other industry leaders. Furthermore, this limited capacity must also be spread across a wide base of other consumer products ‚Äì such as cell phones and laptops. 
TSMC Fab 21 Phase 1 has worked to onshore critical capacity back to the United States. While AMD will direct some of this capacity toward the production of its AI chips, advanced packaging will still be performed in Taiwan. The US remains years away from self-sufficiency.
Taiwan cruises ahead, while also maintaining capacity at many processes behind its own domestic leading-edge. Yet, much of that capacity will continue to be rapidly converted to 2nm and 3nm.
    Taiwan continues to reign supreme in terms of leading-edge manufacturing capacity, maintaining massive advantages in both generation and volume.   
4nm
7nm
3nm
2nm
30K
~45K
125K
36K
Capacity at Leading-Edge Nodes (WPM)
Source: https://www.trendforce.com/news/2025/02/03/news-tsmc-said-to-plan-2nm-production-in-u-s-1nm-fab-in-tainan/Source: https://www.digitimes.com/news/a20240717PD227/tsmc-3nm-capacity-2nm-chips.htmlSource: https://www.smbom.com/news/42476?srsltid=AfmBOopi-_biWrNpn_10TAMAu_sHJ9P7N3_MAbCEodj4-Dt-fWUOGTzE

    As the two major superpowers race to power their AI aspirations, China pulls ahead to a dominant lead:
stateof.ai 2025
 | 146
Power Plays: China and the United States
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
United States

China

Capacity Added (2024):                                       48.6 GW                            429.0 GW
Capacity Retired (2024):                                        7.5 GW                               ~3.3 GW
Net New Capacity Additions (2024):                   41.1 GW                             427.7 GW
Effective Operating Reserve Margin:                      29%                                  ~37%      
Renewable Curtailment Rate (2025):                   ~5.2%                                   6.1%
Thermal Fleet Capacity Factor (2024):                 40.4%                                 39.3%
Transmission Investment (2024):                        $30.1B                                $84.7B

SAIDI - Outages (2023):                                     2.1 h/year                         ~6.9 h/year
Carbon Intensity per kWh (2024):                     384 gCO‚ÇÇe                         560 gCO‚ÇÇe
Industrial Electricity Tariff (2024):                    8.15 ¬¢/kWh                        8.90 ¬¢/kWh   
~ denotes estimate due to lack of concrete public data. Within each nation, measures can vary heavily by region*

Source: https://www.eia.gov/todayinenergy/detail.php?id=64586Source: https://www.eia.gov/todayinenergy/detail.php?id=64604Source: https://climateenergyfinance.org/wp-content/uploads/2025/02/MONTHLY-CHINA-ENERGY-UPDATE-Feb-2025.pdfSource: https://energyandcleanair.org/wp/wp-content/uploads/2025/02/CREA_GEM_China_Coal-power_H2-2024_FINAL.pdfSource: https://ourworldindata.org/grapher/carbon-intensity-electricity?mapSelect=~USASource: https://www.nerc.com/pa/RAPA/ra/Reliability%20Assessments%20DL/NERC_SRA_2025.pdfSource: https://x.com/pretentiouswhat/status/1959544730523701587Source: https://www.eia.gov/electricity/monthly/epm_table_grapher.php?t=epmt_5_3Source: https://www.brattle.com/wp-content/uploads/2025/06/Annual-US-transmission-Investments-1996-2024.pdfSource: https://www.csee.org.cn/pic/u/cms/www/202502/08112548u6wy.pdfSource: https://www.eia.gov/electricity/monthly/epm_table_grapher.php?t=epmt_6_07_aSource: https://www.bloomberg.com/news/articles/2025-04-02/china-curtails-more-renewables-as-record-additions-stress-gridSource: https://www.eia.gov/electricity/annual/table.php?t=epa_11_01.html
Source: https://prpq.nea.gov.cn/uploads/file1/20250331/67ea4fb889529.pdf

    Without sufficient electricity, national AI plans will collapse. A summary of the previous slide can be found below:  
 | 147
Power Plays: China and the United States
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
In 2024, both China and the US set records for peak electricity demand, 1,450 GW and 759 GW respectively. While China must serve more demand, it is also building a larger overhang of available power. In China, reserve margins are beginning to exceed those cited in the US, meaning larger buffers that can accommodate new load. In line with this trend, China‚Äôs thermal fleet operates further below maximum capacity than its counterpart in the US. Similarly, as more renewable capacity comes online in China, curtailment rates outpace those in America. While congestion can cause issues, it also suggests Chinese solar and wind projects are underutilized and could be redirected toward new data centers. 
The US does maintain certain advantages. Outages are less frequent in the US; whereas interruptions can occur in China due to fluctuations in the price of coal, potentially hurting the reliability of certain data centers. Also, the average cost of electricity for data centers in the US is lower, yet this can vary considerably by state or province. The US grid also produces considerably less emissions per kWh.
stateof.ai 2025
Source: https://www.eia.gov/todayinenergy/detail.php?id=65864Source: https://www.reuters.com/business/energy/china-power-demand-growing-faster-than-expected-2024-industry-association-says-2024-10-29/

    From top to bottom, different nations continue to pursue vastly different ‚ÄúSovereign AI‚Äù playbooks:
stateof.ai 2025
 | 148
So, what is ‚ÄúSovereign AI‚Äù? 
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Source of Funding: some nations rely on private investments (Stargate and France‚Äôs initiative), others deploy capital from sovereign wealth funds (MGX and QIA), while many countries still lean on direct government investment vehicles.  
Objectives: some nations hope to develop fine-tuned models that preserve their language and culture, other nations attempt to spin up national compute clusters, and some even attempt to upskill huge swaths of their populations. 
Self-Reliance: some nations rely heavily on partnerships with foreign providers across the AI value chain, while others prefer to pursue indigenization along one or many layers of the stack. Many countries support their own homegrown start-ups, while others prioritize investments in opportunities abroad. 
Overall, the Gulf States and China continue to pursue the most ambitious and overt sovereign AI plans, blending many of the strategies mentioned above. Whereas, countries like the US have generally focused on strategies that enable their own champions to lead the charge, riding the wave of private capital. 

 | 149
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
stateof.ai 2025
The analysis above focuses on spend that flows from direct government investments and/or sovereign wealth funds. 
The Sovereign AI spending spree 

    Nations are seeking ‚Äúsovereignty‚Äù for the same reason they have domestic utilities, manufacturing borders, armies, and currencies: to control their own destiny. Yet, there is a real danger of ‚Äúsovereignty-washing.‚Äù Investing in AI projects to score political points may not always advance strategic independence.
stateof.ai 2025
 | 150
Sovereign AI: the hype and the hard truths
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
 
‚ÄúSovereignty-washing" can also involve political leaders claiming credit for private investments that were already planned/underway. Although the announcement of Stargate project was made by President Trump, all of the real capital, control, and strategic decisions are driven by private entities.
Investing for ‚Äúsovereignty‚Äôs‚Äù sake could also drive oversupply into the future. Without lasting demand, these projects may lead to idle compute, especially as efficiency gains continue to multiply (e.g. the frantic investments made by local governments/SOEs in China fueled a widespread overcapacity of chips).

Without support for indigenous capabilities, sovereign AI projects can deepen a nation‚Äôs dependence on foreign supply chains. While these investments may pay dividends through boosted productivity, greater economic independence cannot be guaranteed. In fact, most sovereign projects lead nations further into the orbit of the US, and soon China as it develops end-to-end turnkey solutions. 

Source: https://fortune.com/2025/06/09/ai-chips-geopolitics-tech-data-centers/
Source: https://www.chinatalk.media/p/chinas-weird-chip-surplus-explained

    If AI should soon be treated as an essential public service, nations will need to reckon with the reality that their sovereign AI strategies are riddled with vulnerabilities. If your AI stack is totally dominated by another country, particularly at the infrastructure layer, then your population‚Äôs access to AI technology remains inherently at risk: 
stateof.ai 2025
 | 151
Sovereign AI: the hype and the hard truths
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Jurisdictional Risks: Foreign AI providers operate under their own country‚Äôs laws, therefore export restrictions and other national security directives could potentially override service agreements.
Supply Chain Security Risks: sovereign AI projects that depend on foreign infrastructure must manage risks related to cybersecurity vulnerabilities (e.g. backdoors, kill switches, side-channel attacks).
Data Privacy Risks: Similarly, reliance on foreign providers could lead to mishandling of sensitive data and algorithmic secrets.
Modern AI supply chains remain heavily globalized and entangled. Nations cannot onshore every rung of the stack. Yet, without stronger international governance and concrete guarantees, sovereign efforts expose nations to a slew of economic threats.
Source: https://fortune.com/2025/06/09/ai-chips-geopolitics-tech-data-centers/

 | 152
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
    Jensen Huang continues to plead nations to increase their ‚ÄúSovereign AI‚Äù investments. Already, this global campaign has been rewarded. During their recent Q2 FY 2026 earnings call, CFO Colette Kress claimed NVIDIA was on ‚Äútrack to achieve over $20B in sovereign AI revenue this year, more than double that of last year.‚Äù
Despite making up just ~10% of forecasted annual revenue, sovereign AI remains one of NVIDIA‚Äôs strongest new demand drivers. The recent push by American labs to develop custom ASICs and continued Chinese indigenization will place more pressure on this key high-growth category.
Yet, despite Huang‚Äôs globetrotting, some new sovereign projects have even begun diversifying away from NVIDIA‚Äôs offering. For example, G42 announced its intention to tap AMD and Cerebras for supply of some of the computing capacity at its planned UAE-US AI campus. 
As more clouds/labs attempt to evade the   ‚ÄúNVIDIA Tax,‚Äù so too might many sovereign efforts.
The world‚Äôs top ‚ÄúSovereign AI‚Äù evangelist
stateof.ai 2025
Source: https://www.cnbc.com/2025/08/28/nvidias-top-two-mystery-customers-made-up-39percent-of-its-q2-revenue-.html
Source: https://www.perplexity.ai/search/nvidia-sovereign-ai-PtSEAE_xTxOcADpgwih0EQ?1=d#1 
Source: https://blogs.nvidia.com/blog/nvidia-ceo-promotes-ai-in-dc-and-china/ Source: https://www.semafor.com/article/09/01/2025/abu-dhabis-g42-eyes-chip-options-beyond-nvidia


NVIDIA‚Äôs data center revenue projection for the calendar year 2025 floats between ~$170B-$180B, depending largely on the stringency and timing of upcoming export control decisions from both the US and China.
While hyperscalers ordered more chips in 2025, the two clouds with custom chips programs, Amazon and Google, dedicated a smaller percentage of capital expenditures toward NVIDIA purchases. 
Direct purchases overwhelmingly flow to OEMs partners such as Dell, SuperMicro, Lenovo, and HPE. 


 | 153
Digging into NVIDIA‚Äôs revenue concentration
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
    Despite the uptick in sovereign demand, NVIDIA‚Äôs data center revenue continues to be dominated by American cloud and AI giants, who now make up nearly 75% of NVIDIA‚Äôs total data center sales.
These leading American players will drive ~75% of NVIDIA's revenue this year
*Chinese CSPs: assumes no B30A sales this year,
stateof.ai 2025

 | 154
The rise of GPU neoclouds in public and private markets
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
    Public companies CoreWeave and Nebius and private companies Lambda and Crusoe are rapidly growing as customers embrace attractive pricing, contract terms, and AI-specific software stacks. 
*Public company‚Äôs valuations are based on market cap as of 9/29/2025, whereas most recent post-money valuation was used for private companies.
stateof.ai 2025

 | 155
NVIDIA‚Äôs circular GPU revenue loops
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
    NVIDIA has continued to invest in or sell GPUs to AI labs and neoclouds. It benefits when those same firms recycle capital into NVIDIA hardware or lease GPU capacity back to NVIDIA.
Target
Year
Terms
Interdependence
OpenAI
2025
NVIDIA announces its intention to invest up to $100B to develop at least 10 GW of data center capacity with OpenAI
NVIDIA invests in OpenAI ‚Üí OpenAI and its datacenter operators buy GPUs
CoreWeave
2025
$6.3B deal for NVIDIA to buy unused GPU cloud capacity (Sept 2025)
NVIDIA funds CoreWeave ‚Üí CoreWeave buys GPUs ‚Üí NVIDIA commits to rent back the GPUs
Nebius
2024 and 2025
$700M funding with NVIDIA (Dec ‚Äò24); $17‚Äì19B GPU capacity contract with Microsoft (‚Äò25)
NVIDIA invests ‚Üí Nebius builds GPU infra with NVIDIA chips ‚Üí Microsoft consumes capacity
Oracle
2025
OpenAI commits to buy ~$300B worth of AI compute from Oracle over ~5 years (starting 2027) under Stargate.
NVIDIA is an investor in OpenAI and partner in Stargate with Oracle ‚Üí OpenAI buys compute from Oracle ‚Üí Oracle buys NVIDIA GPUs
xAI
2024 and 2025
$6B Series C with NVIDIA (Dec 2024); $12B debt plan to buy GPUs (2025); Colossus with ~100k H100s, target 1M GPUs
NVIDIA invests ‚Üí xAI spends billions on NVIDIA GPUs ‚Üí lease-back model amplifies NVIDIA‚Äôs role
Lambda
2025
NVIDIA agrees a $1.5B contract to rent 18k GPUs from Lambda for 4 years.
NVIDIA invested in Lambda‚Äôs Series D ‚Üí Lambda builds NVIDIA GPU infra ‚Üí NVIDIA leases it back from Lambda
stateof.ai 2025
Source (CoreWeave): https://www.reuters.com/business/coreweave-nvidia-sign-63-billion-cloud-computing-capacity-order-2025-09-15/ 
Source (Nebius): https://www.reuters.com/business/finance/yandex-backed-nebius-raises-700-million-expand-ai-cloud-business-2024-12-10/ 
Source (Nebius): https://www.reuters.com/technology/microsofts-174-bln-deal-with-nebius-boosts-ai-cloud-capacity-2025-02-14/ 
Source (Oracle / OpenAI): https://www.reuters.com/technology/openai-oracle-sign-300-billion-computing-deal-wsj-reports-2025-09-10/ 
Source: https://www.reuters.com/business/oracle-buy-40-billion-nvidia-chips-openais-us-data-center-ft-reports-2025-05-23/ 
Source (xAI): https://www.teslarati.com/elon-musk-xai-gets-investment-nvidia-new-funding-round/ 
Source: https://www.benzinga.com/markets/tech/25/07/46563372/elon-musks-xai-plans-to-raise-12-billion-in-debt-to-buy-nvidia-chips-and-build-one-of-the-worlds-largest-ai-superclusters-report 
Source: https://www.investopedia.com/tesla-and-xai-plan-to-keep-buying-chips-from-nvidia-and-amd-musk-says-11738838 
Source (Lambda) https://www.rcrwireless.com/20250908/ai-infrastructure/nvidia-lambdaSource (OpenAI): https://nvidianews.nvidia.com/news/openai-and-nvidia-announce-strategic-partnership-to-deploy-10gw-of-nvidia-systems


 | 156
Other notable circular investments
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
    The Oracle/OpenAI/NVIDIA triangle has drawn the most attention, yet circular deals have become common.
stateof.ai 2025
https://law.stanford.edu/2025/03/21/ai-partnerships-beyond-control-lessons-from-the-openai-microsoft-saga/#:~:text=The%20OpenAI%E2%80%94Microsoft%20saga%20began,hard%20%5B%E2%80%A6%5D%20to%20further%20extendSource: https://www.anthropic.com/news/anthropic-amazon-trainium
Source: https://www.geekwire.com/2025/amazon-deepens-anthropic-ties-with-equity-conversion-adding-billions-to-q1-profit/
Source: https://www.barrons.com/articles/amd-nvidia-xai-fundraising-87032109Source: https://scale.com/blog/scale-ai-announces-next-phase-of-company-evolutionSource: https://www.reuters.com/sustainability/boards-policy-regulation/metas-148-billion-scale-ai-deal-latest-test-ai-partnerships-2025-06-13/Source: https://www.asml.com/en/news/press-releases/2025/asml-mistral-ai-enter-strategic-partnershipSource: https://www.reuters.com/world/europe/asml-becomes-mistral-ais-top-shareholder-after-leading-latest-funding-round-2025-09-07/

stateof.ai 2025
 | 157
Symbiosis or deathtrap: circular AI deals and potential warning signs
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
    Circular AI deals introduce new market risks. What red flags could surface? 
Acquiring stakes in high-growth AI companies has become an outlet for giants who are gushing cash. They see their investments trickle back in revenue, even on a cashless basis. Risks may arise if the uptick in hollow revenue hurts cash flow and warps* financial metrics.
Many rounds involving AI startups have been oversubscribed. Yet these companies often pursue deals with incumbents because of secondary benefits like pricing support. However, if incumbents become the only willing source of capital, trouble could soon surface.
For now, most incumbents do not control the decision-making of AI labs. Yet, greater overlap could lead to conflicts of interest that might distort spending trends. To date, antitrust scrutiny has been a blocker.
AI startups could eventually dominate the demand and investment portfolios of incumbents. This interdependence might then trigger a domino effect if these startups ever collapse.

*Large inflows of recycled, cashless revenue can inflate metrics like capex to revenue or valuation multiples.

stateof.ai 2025
 | 158
Borrowed Intelligence: the growing use of debt instruments
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
    Organizations across the AI stack begin to form stronger reliances on private credit packages to fund ever more ambitious buildouts. As with past cycles, this present possible pitfalls. The table below highlights many of the largest borrowing events in the past year:
Source:https://www.reuters.com/business/meta-taps-pimco-blue-owl-29-billion-data-center-expansion-project-source-says-2025-08-08/
Source:https://www.bloomberg.com/news/articles/2025-08-20/jpmorgan-mufg-to-lead-22-billion-loan-for-vantage-data-centers
Source: https://www.blackstone.com/news/press/coreweave-secures-7-5-billion-debt-financing-facility-led-by-blackstone-and-magnetar/Source: https://x.com/MorganStanley/status/1939768047780172184Source: 

 | 159
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
    To maintain healthy-looking balance sheets, hyperscalers increasingly use exotic financial structures like SPVs. This financial sleight of hand works to conceal the mountains of debt accumulation within the AI sector.
Hyperscalers increasingly offload their debt using special purpose vehicles (SPVs). In an SPV, the hyperscaler contributes assets (GPU-clusters), while the financial partner injects capital. Although the hyperscaler maintains control and use of the GPU-cluster, the debt then sits outside that parent company.  
Hyperscalers pay a premium (2-3%) to protect their credit rating and maintain investor sentiment. Private credit players love these deals as they involve reliable borrowers and giga-projects are easier to manage than many smaller loans. 
Risk could arise if utilization lags. Since SPVs sit outside the core business and are often bound by strict cash flow covenants, defaults become more likely. At the same time, private credit funds, often backed by long-term pension or insurance capital, are financing short-lived, fast-depreciating assets. This creates a temporal mismatch, with AI data centers treated as long-lived, stable infrastructure projects. 
Examples: Meta‚Äôs $26B deal, Stargate, Vantage‚Äôs deal in Texas, CoreWeave.
stateof.ai 2025
Out with corporate borrowing, in with SPVs, JVs and accounting gymnastics
Source:https://paulkedrosky.com/weekend-reading-plus-spvs-meta-and-fiber-buildout-2-0/Source: https://podcasts.apple.com/us/podcast/plain-english-with-derek-thompson/id1594471023

    Middle East capital has become a major growth-finance source for capital-hungry AI labs and infrastructure (left chart). The share of AI rounds involving MENA investors jumped to a record in 2024 (right chart) and the money overwhelmingly flows to US companies. Deals are typically non-voting and board-light, letting labs raise at scale while keeping control‚Ä¶for now.
stateof.ai 2025
 | 160
Petrodollars are bankrolling the American AI dream
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Source: https://newsletter.dealroom.co/dealroom-news-october-1-2025?ecid=ACsprvtkXmPQtsiUcvsD13dVAumK02UZkPHjfRlh_CRGi3mY8Hmn9ugjqunIN7PMZCJkmbuy333V&utm_campaign=Weekly%20newsletters&utm_medium=email&_hsenc=p2ANqtz--tEFCUWT1A0nZW6jqF9s028RNwEnJquw2FxfeKDjWeyzii5UsATb_toYQo-NvBrT6h0GWAoQZkrvbPZGliBcLI9MQNLw&_hsmi=383131426&utm_content=383131426&utm_source=hs_email 

stateof.ai 2025
 | 161
‚ÄúChallengers‚Äù are no closer to catching NVIDIA
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
     Many of NVIDIA‚Äôs competitors, both at home and abroad, have yet to gain meaningful momentum

Earlier this year, Groq reported to investors their expectations that revenue would exceed $2B in 2025. In recent months, that forecast has been revised down dramatically to just $500M.  
AMD posted underwhelming earnings in their data center unit during Q2. Data center revenue was largely buoyed by EPYC CPU processors, concerning since that segments 14% growth rate still pales in comparison to NVIDIA‚Äôs recent surge.
Huawei faces mounting challenges that threaten to constrain its growth such as HBM bottlenecks. On the demand side, many of China‚Äôs cloud giants view Huawei as a fierce competitor, which leads to resistance in the adoption of their stack.
G42, Cerebras‚Äô primary investor, has agreed to purchase $1.43B of equipment through the end of 2025 from the hardware provider. Yet, it is not clear whether Cerebras has seen traction from other customers.

Source: https://www.theinformation.com/briefings/groq-cuts-revenue-projections-span-monthsSource: https://ir.amd.com/news-events/press-releases/detail/1257/amd-reports-second-quarter-2025-financial-resultsSource: https://www.reuters.com/breakingviews/beware-hallucinations-ai-chipmaker-ipo-2024-10-02/Source: https://semianalysis.com/2025/09/08/huawei-ascend-production-ramp/

Timings could explain a lot: work launched on H100 and H200 in late 2024 will surface in late 2025.
Labs are publishing significantly less and later due to competition and safety reviews.
Authors increasingly use managed APIs and shared clouds, so they name services rather than chips.
Higher GPU costs push academics toward inference and lightweight fine-tuning using strong open-weights.
    2024 saw 49,000 open-source AI papers that explicitly cited NVIDIA, TPUs, AMD or similar accelerators, up 58% year on year. Our 2025 projection through September points to 45,600 papers, an 7% decline and the first drop in six years. NVIDIA remains dominant at about 90% of compute mentions, down from a 94% peak in 2023, with 41,300 NVIDIA-citing papers forecast, down 7%. AMD is more than doubling on MI300X momentum, while TPU mentions edge down 25% despite v6.
stateof.ai 2025
 | 162
NVIDIA‚Äôs large lead persists within the AI research community
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Source: Zeta Alpha

H100/H200 up ~126% YoY, reflecting the 2023‚Äì24 build-out finally showing up in publications (growth moderating into 2025).
Jetson up ~24% YoY, consistent with rising robotics/edge AI and low-power inference interest.
V100 continues to decline year-over-year from its 2023 peak as legacy fleets sunset.
GeForce rotation: RTX 3090 down from a 2024 peak while RTX 4090 up ~39% YoY as labs and prosumers upgrade.
    The mix of NVIDIA accelerators cited in papers is rotating: older chipsets are giving way to Hopper (H100/H200) and high-end consumer GPUs, with a parallel uptick at the edge. Even as total NVIDIA-citing papers soften in 2025, the composition points to late-2024 deployments maturing and more moving to inference and robotics.
stateof.ai 2025
 | 163
Inside NVIDIA mentions: Hopper surges, edge rises, legacy fades
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Source: Zeta Alpha

LLMs love datacenter GPUs: AMD MI300 is the standout for LLM papers (+43 pp vs average), with MI250, Huawei Ascend, and NVIDIA H100/H200 also common. LLMs are least tied to ASICs, Jetson, 4090, and Apple M1.
The Jetson dominates robotics and edge computing and also shows up in computer vision.
Modalities have their own favorites: Apple M4 skews to multimodal and speech work while the RTX 4090 is most used for 3D models.
FPGAs are rarely used with diffusion models and RL.
    We tagged 6,356 papers (January to June 2025) by topic and looked at which accelerator each paper cited. Clear patterns pop out: big LLM work clusters on datacenter parts, while robots and edge devices overwhelmingly use the Jetson. A few consumer and mobile chips also anchor specific niches.
stateof.ai 2025
 | 164
What chips power which research? Topic skews by accelerator (H1 2025)
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Source: Zeta Alpha

Cerebras: WSE-3 gains visibility via open SlimPajamas-2 large-scale training runs.
Groq: LPU wins inference mindshare after viral low-latency demos.
Habana (Gaudi-2): steady in AWS-funded projects.
Graphcore (post-SoftBank acquisition) sharp decline post-acquisition from a 2022 peak.
SambaNova/Cambricon: niche, flatter trajectories.
    Among challenger accelerators (Cerebras, Groq, Graphcore, SambaNova, Cambricon, Habana), paper mentions are up only +3% YoY to an estimated 593 in 2025. While the momentum is positive, it is still only 1.3% of all accelerator-citing papers. There are a few breakout narratives (WSE-3 training runs, ultra-low-latency LPUs). 
stateof.ai 2025
 | 165
Startup silicon is still (mostly) on the sidelines
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Source: Zeta Alpha

    ~$7.5B has been invested in major Western AI chip challengers since 2016. What would have happened if investors had just bought the equivalent amount of NVIDIA stock at that day‚Äôs price? The answer is lime green: that $7.5B would be worth $85B in NVIDIA stock today (12x!) vs. the $14B (2x) in its contenders.
 | 166
Tracking the returns on investments in NVIDIA‚Äôs Western challengers
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
$7.5B
stateof.ai 2025
Note: Market pricing and valuation data retrieved as of 3 Oct 2025. NAV = net asset value after accounting for equity dilution, 
$14.3B (~2x return)
$89.2B (~12x return)

    ~$6B has been invested in major Chinese AI chip challengers since 2016. What would have happened if investors had just bought the equivalent amount of NVIDIA stock at that day‚Äôs price. The answer is lime green: that $6B would be worth $160B in NVIDIA stock today (26x!) vs. the $36B (6x) in its contenders.
 | 167
Tracking the returns on investments in NVIDIA‚Äôs Chinese challengers
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
$6B
stateof.ai 2025
Note: Market pricing and valuation data retrieved as of 26 Sep 2025. NAV = net asset value after accounting for equity dilution.
$35.8B (~6x return)
$160.3B (~26x return)

stateof.ai 2025
 | 168
‚Ä¶ yet the gains of Chinese chip startups have largely come in the past year
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
 
Building on explosive 4,348% revenue growth in the first half of 2025, Cambricon has tallied strong orders through 2026. Cambricon, who reportedly sold just ~10K GPUs in 2024, will ship ~150K GPUs in 2025, with rumors that 2026 orders could reach 500K GPUs. However, the company recently tempered these rumors during an investors call, where a spokesperson relayed the following cooling message: ‚Äú[our] stock price risks deviating from current fundamentals, and investors participating in trading might face substantial risks.‚Äù 


    Highlighted by the ~7x surge in Cambricon‚Äôs stock price this past year, Chinese challengers continue to benefit from a slew of tailwinds. Potentially looking to also cash in on the momentum sweeping the nation, MetaX, Moore Threads, Iluvatar CoreX, and Biren are all exploring IPOs within the back half of 2025. 
 
Still, enticed by Cambricon‚Äôs multiples, many Chinese startups wish to capitalize on the fervor, evident in the 4+ IPO prospectuses filed by competitors since mid-2025. 
Despite some frothy valuations, there are real reasons for optimism. There is booming demand coming from Chinese CSPs and SOEs, with government directives favoring homegrown offerings. Capacity also frees up as SMIC treks forward and Huawei pursues greater vertical integration. Finally, challengers benefit from lingering uncertainty around B30A timelines and persistent issues with CANN. 

Source: https://www.bloomberg.com/news/articles/2025-08-13/ai-chipmaker-iluvatar-corex-is-said-to-mull-hong-kong-ipoSource: https://www.reuters.com/world/china/two-chinese-chip-firms-plan-17-billion-ipos-bet-us-export-curbs-spur-growth-2025-07-01/Source: https://www.reuters.com/world/china/china-ai-chip-firm-biren-raises-new-funds-plans-hong-kong-ipo-say-sources-2025-06-26/
Source: https://semianalysis.com/2025/09/08/huawei-ascend-production-ramp/Source: https://x.com/Jukanlosreve/status/1955147891053613379Source: https://www.digitimes.com/news/a20250911PD232/huawei-cambricon-2026-production-ai-chip.htmlSource: https://www.csis.org/podcasts/ai-policy-podcast/why-chinas-ai-sector-boomingSource: 

stateof.ai 2025
 | 169
The Huawei Assumption
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions

Huawei will be responsible for just ~62% of the XPU volume produced by Chinese firms in 2025. For reference, NVIDIA still controls 90%+ of the global XPU market.
DeepSeek's R2 release has reportedly been delayed due to hurdles related to Huawei hardware. Additionally, speculation suggests that Huawei disbanded its entire Pangu LLM team following difficulties in developing SOTA models using Ascend chips. Internal whistleblowers have even alleged that several models in the recent Pangu family were not developed from scratch, but were instead cloned based on the continued training of existing Qwen and DeepSeek models.
Alibaba and Baidu have already begun to adopt their own in-house chips for training and rumors indicate that ByteDance may soon follow suit. Unlike similar efforts by clouds in the US, the recent push made by Chinese firms could be partially driven by Huawei‚Äôs role as an active competitor across various sectors. 
    Huawei‚Äôs continued dominance over the Chinese AI sector has long been considered an inevitability amongst Western observers. However, recent points of turbulence suggest that Huawei‚Äôs grip on the AI chip sector in China may not be as bulletproof as outsiders had once assumed. 
Source: https://www.ft.com/content/19013793-4e8d-444b-b57f-5435352a8f3bSource: https://x.com/zephyr_z9/status/1961090260030603529Source: https://github.com/HW-whistleblower/True-Story-of-PanguSource: https://www.theinformation.com/articles/alibaba-baidu-adopt-ai-chips-major-shift-chinese-techSource: https://www.barrons.com/articles/nvidia-stock-price-ai-chips-58cec987Source: https://www.reuters.com/technology/artificial-intelligence/chinas-bytedance-working-with-broadcom-develop-advanced-ai-chip-sources-say-2024-06-24/

stateof.ai 2025
 | 170
The AI Hunger Hiring Games
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
    There has been continuous all out warfare as top AI companies compete for talent, with eye-watering pay packages and a clash of money vs mission.
~AI EDITION~
Source: https://sifted.eu/articles/anthropic-salaries
Source: https://www.businessinsider.com/microsoft-trying-poach-meta-ai-talent-big-pay-packages2025-8?utm_source=www.therundown.ai&utm_medium=newsletter&utm_campaign=apple-plots-ai-comeback-with-home-robots&_bhlid=655623bacece65fb23473883d907bb746a1e2101
Source: https://www.theguardian.com/technology/2025/jun/18/openai-boss-sam-altman-accuses-mark-zuckerberg-meta-of-poaching-staff-crazy-100m-sign-on-bonuses
Source: https://sifted.eu/articles/anthropic-acquisition-ai-humanloop
Source: https://www.wired.com/story/openai-new-hires-scaling/
Source: https://techcrunch.com/2025/07/11/windsurfs-ceo-goes-to-google-openais-acquisition-falls-apart/


stateof.ai 2025
 | 171
The Mass Migration (from OpenAI and others)
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
    No one has been immune to departures, yet OpenAI has lost much of its core to new startups and poaching. What was once the talent densest organization must rebound to defend its own first-mover advantage. 
Lost to Meta: Shengjia Zhao, Jiahui Yu, Shuchao Bi, Hongyu Ren, Jason Wei, Lucas Beyer, Hyung Won Chung, and Trapit Bansal. 
Lost to Thinking Machines: Mira Murati, Alec Radford (advisor), John Schulman, Barret Zoph, Lilian Weng, Luke Metz, and Bob McGrew.  
Lost to Anthropic: Amodei‚Äôs, Jack Clark, Jan Leike, Tom Brown, Jared Kaplan, Benjamin Mann, Sam McCandlish, Chris Olah, Durk Kingma, Amanda Askell and Pavel Izmailov.
Lost to SSI: Ilya Sutskever and Daniel Levy.  
Source: https://www.metislist.com/

    Wayve's AI Driver completed a 90-city global deployment test, demonstrating its ability to generalise across diverse environments without location-specific training. This "zero-shot" approach successfully conducted 10,000 hours of AI driving (with a human behind the wheel), half of which was in dense urban areas.
stateof.ai 2025
 | 172
Race across the world: Wayve‚Äôs 90-city world tour in 90 days
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Of the 90 cities, 62% were completely new to the test fleet, meaning the AI Driver had never been exposed to these environments during its training. 
Wayve's system operated across varied conditions with 45% of miles in clear daytime conditions, 38% at night, and 17% in challenging low-light or rainy conditions, demonstrating the system's ability to handle different visibility scenarios beyond ideal driving environments.
The 90-city tour validated the end-to-endy driving system's ability to scale globally with minimal local-specific data required, potentially accelerating commercial rollout timelines.
Source: https://wayve.ai/thinking/ai-500-roadshow-90-cities/

    Waymo has driven more than 37M miles in Phoenix and 23M miles in San Francisco. According to the California Public Utilities Commission (CPUC) data through March 2025, Waymo recorded over 700,000 monthly paid trips in California alone, a 55-fold increase from August 2023! And its safety record is stunning: 88% fewer serious injuries or worse crashes compared to human drivers. Meanwhile Tesla‚Äôs Robotaxi service, launched in Austin mid-June 2025 has reported crica 7,000 robotaxi miles by July 2025, averaging under 20 miles per vehicle per day with a fleet of about 12 vehicles. 
stateof.ai 2025
 | 173
Gradually then suddenly: 71M rider-only miles w/out a human driver through March ‚Äò25
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Source: https://waymo.com/safety/impact/ 
Source: https://www.forbes.com/sites/bradtempleton/2025/07/25/tesla-robotaxis-only-go-20-milesday--meanwhile-wheres-mobileye/ 

    No region has cracked real, scaled deployment yet. While Chinese companies ship more units at lower cost, buyers are mostly researchers, pilot programs, or government centers. US teams show stronger manipulation and autonomy, but hardware is expensive. Manufacturing advantages in China matter, yet do not guarantee success in design, distribution, or operations. Indeed, China could end up the robot factory for Western brands. A Dealroom dataset of 155 humanoid robotics companies shows almost $2B raised in 2025 alone and 8 unicorns, including Unitree, Figure and Agility. 

China‚Äôs Unitree‚Äôs launched their R1 humanoid at $5,566, has >$140M annual revenue and is kicking off a ~$7B onshore IPO.
Hong Kong-listed UBTech booked ~$180M in 2024 revenue and targets 500‚Äì1,000 Walker-S deliveries in 2025 to automakers, Foxconn, and SF Express.
Outside China, Agility‚Äôs Digit is in a paid multi-year RaaS deployment with GXO Logistics, while Figure has raised $2.34B and Apptronik raised $350M to reach pilots. Tesla‚Äôs Optimus and 1X are still demo‚Äôing.
stateof.ai 2025
 | 174
Humanoids in 2025: hype high, deployments thin
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Source: https://www.reuters.com/business/autos-transportation/chinese-robotics-firm-unitree-eyeing-7-billion-ipo-valuation-sources-say-2025-09-08/
Source: https://app.dealroom.co/transactions.rounds/f/growth_stages/not_mature/rounds/not_GRANT_SPAC%20PRIVATE%20PLACEMENT/tags/anyof_humanoid_not_outside%20tech/year/anyof_2025?showStats=true&statsType=rounds 

Protocol churn: stateless chat completion APIs have moved to stateful Responses API with breaking semantics and provider-kept session state. Forced re-architecture increases hidden state and compatibility-driven vulnerabilities.
MCP immaturity: unclear authentication, state, and versioning. Coarse ‚Äúgateway‚Äù monitors and model-on-model oversight are bypassable, leaving prompt-injection/tool misuse largely unsolved.
Opaque chains: caches persist across tools/models but there‚Äôs no ‚ÄúDNS for agents‚Äù. Callers can‚Äôt know who or what handled a request, complicating trust and forensics.
AI-enabled malware: payloads embed OSS models, hijack local CLIs/dev tools, and semantically hunt PII/financial data and model caches. 
    The AI stack is racing from stateless APIs to stateful protocols and agent layers. That evolution is outpacing security: specs churn, backward compatibility breaks, caches leak, and MCP still lacks clear auth/state semantics. At the same time, attackers are bundling open models into malware that hijacks local dev tools and hunts sensitive context. The answer isn‚Äôt another model ‚Äúmonitor,‚Äù but real standards plus fine-grained, data-aware controls.
stateof.ai 2025
 | 175
The emerging surfaces of AI security threats
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Source: conversations with security experts

stateof.ai 2025
 | 176
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
82% of the $133B private AI financing in 2025 was raised by US-based companies ($109B), with Europe and the UK accounting for just under 9% ($12B) and China just under 4% ($5B). GenAI companies (which includes all AI labs) account for 60% of the capital raised in 2025 vs. 40% for non-GenAI. 
Venture investments in AI companies continues to surge with a focus on GenAI and the US
Data retrieved and analyzed on 14 Sept 2025.

 | 177
Corporate venture capital and other investments
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
    Corporate investments by leading players continue to surge, despite deal volume plateauing in recent years. NVIDIA charges forward after years of minimal activity. The hyperscalers + NVIDIA now account for over half of all AI-related venture investment by amount, a concentration unseen in previous eras (e.g., dotcom and mobile).
stateof.ai 2025
Source:https://www.trendforce.com/news/2025/08/29/news-broadcom-reportedly-boosts-2026-cowos-orders-with-foundries-on-strong-asic-demand/Source: https://x.com/Jukanlosreve/status/1960976697718976938

While private company valuations have continued to climb at a steady pace of $1T per year from 2023 onwards, a small handful of publicly traded companies have added incredible value. NVIDIA, Meta, and Alphabet alone account for over $9T of value.
stateof.ai 2025
 | 178
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Public AI titans outweigh private gains, but from a far larger base
Data retrieved and analyzed on 14 Sept 2025.

 | 179
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
stateof.ai 2025
90% of GenAI dollars invested are mega rounds‚Ä¶.
Mega $250M+ rounds eat the lionshare of private capital invested into GenAI
*Data retrieved and analyzed on 14 Sept 2025.

 | 180
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
While AI company exits were rather quiet in the last years due to mountain regulatory scrutiny and various shocks to the economy, the aggregate deal volume in 2025 has so far surpassed 2x that of 2024. This includes IPOs for CoreWeave ($23B), Figma ($19B), Klarna ($15B), and Kodiak Robotics ($3B), and M&As for Scale AI/Meta ($24B), Core Scientific/CoreWeave ($9B), io/OpenAI ($6.5B), Windsurf/Google ($2.4B), Dotmatics/Siemens ($5B), Sana/Workday ($1B), and Cognigy/NICE ($955M) as select highlights. 
The IPO is showing signs of thawing while M&A is picking up with $B+ deals
stateof.ai 2025
*Data retrieved and analyzed on 14 Sept 2025.

stateof.ai 2025
 | 181
The scaling law of AI valuations
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
    The valuation history of the leading private AI labs mirrors trends in model capabilities, with doubling times historically floating around the half-year mark. 
Since the beginning of 2023, the valuations of every major private AI lab has followed some of the steepest trajectories in American history. 
Yet, the valuation history of these labs has kept pace with trends in their model‚Äôs capabilities. 
METR‚Äôs task-completion time horizon reflects a doubling time of roughly 7 months. Similarly, METR‚Äôs time horizon analysis on nine* other leading benchmarks conveys a doubling time of roughly 5 months. As evidenced earlier, the ratio between absolute capabilities and cost roughly doubles every 6 months.

*MATH, OSWorld, LiveCodeBench, Mock AIME, GPQA Diamond, Tesla FSD, Video-MME, RLBench, and SWE-Bench Verified
OpenAI Doubling Time: ~7.3 Months
Anthropic Doubling Time: ~5.1 Months
xAI Doubling Time: ~5.1 Months
https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/
https://metr.org/blog/2025-07-14-how-does-time-horizon-vary-across-domains/https://www.latent.space/p/lmarena

 | 182
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
    While multiples compress across the board, xAI remains overvalued compared to other private labs. Despite a valuation history that has largely traced Anthropic, their revenue lags far behind other competitors. 
Anthropic‚Äôs annualized revenue again looks poised to ~10x YoY, while the slightly more mature OpenAI appears on track to ~3x YoY. Meanwhile, xAI remains an OOM behind Anthropic in annualized revenue. Despite this gap, xAI‚Äôs latest valuation surprisingly surpassed that of Anthropic‚Äôs.

*xAI‚Äôs multiples remain 10x higher than other labs, a gap so wide it needed its own plot  
stateof.ai 2025
Artificially high valuations? Tracking annualized revenue multiples


    ‚Ä¶But at least Dario Amodei is ‚Äúnot thrilled about it‚Äù...  

stateof.ai 2025
 | 183
U-Turn Hall of Fame: AI's greatest vibe shifts
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
2024: "AI-powered authoritarianism seems too terrible to contemplate. Democracies need to set the terms."
2025: Qatar invests in Anthropic‚Äôs record $13B funding round


2024: "Open source prevents concentration of power! Safety through transparency!"
2025: "We're building superintelligence. It must be closed for safety!"

2015-2023: Created to ensure AI benefits all humanity, endless congressional testimony about democratic values
2025: First major infrastructure deal: $500B Stargate UAE 


Source: https://www.darioamodei.com/essay/machines-of-loving-grace
Source: https://www.wired.com/story/anthropic-dario-amodei-gulf-state-leaked-memo/
Source: https://about.fb.com/news/2024/07/open-source-ai-is-the-path-forward/
Source: https://www.meta.com/superintelligence/
Source: https://openai.com/index/introducing-stargate-uae/

 | 184
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
stateof.ai 2025
BLOOPER REEL
    GPT5‚Äôs rocky rollout 
Sam Altman held an emergency Reddit AMA to address user concerns around strict rate limits, the sensitive content filter, the abrupt removal of previous models and viral ‚Äòchart crimes‚Äô.
Many complained about the emotional impact of having 4o taken away.
The broken routing system made GPT-5 appear less capable by misdirecting queries on day one.
OpenAI has since doubled Plus user limits and pledged better transparency for future updates.
Source: https://www.reddit.com/r/ChatGPT/comments/1mkae1l/gpt5_ama_with_openais_sam_altman_and_some_of_the/?utm_source=www.therundown.ai&utm_medium=newsletter&utm_campaign=openai-s-gpt-5-crisis-mode&_bhlid=e64ed55fba67c81e5213bca25a97a707f7f822c5
Source: https://community.openai.com/t/openai-is-taking-gpt-4o-away-from-me-despite-promising-they-wouldnt/1337378
Source: https://www.reddit.com/r/ChatGPT/comments/1mlkemm/chat_gpt_5_was_not_for_use_it_was_to_get_rid_of/

 | 185
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
stateof.ai 2025
BLOOPER REEL
    After months of user complaints, Anthropic explain how Claude had three overlapping bugs that took over a month to disentangle and fix. 
They had a context window routing bug where some requests were misrouted to servers configured for 1M token contexts, with the problem escalating when a load balancing change increased affected traffic.
Two other issues emerged, with output corruption that produced random Thai/Chinese characters in English responses, and an XLA:TPU compiler bug triggered by mixed precision arithmetic that caused the system to occasionally drop high-probability tokens entirely.
Anthropic have won praise for their transparency but many are still calling for refunds for users affected.
Source: https://www.anthropic.com/engineering/a-postmortem-of-three-recent-issues 
Source: https://www.reddit.com/r/ClaudeCode/comments/1netbi9/i_aint_paying_200_for_this_shit_anymore/


 | 186
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
stateof.ai 2025
BLOOPER REEL
    Meta‚Äôs AI glasses glitched twice in a live demo at Meta Connect 2025. Zuckerberg blamed issues on wifi ‚ÄúThe irony of the whole thing is that you spend years making technology and then the WiFi at the day catches you‚Äù. 
Source: https://x.com/ns123abc/status/1968469616545452055?s=46
Source: https://www.youtube.com/watch?v=1cpnK9AfIhg&t=360s


Grok called itself a super-Nazi, referring to itself as ‚ÄúMechaHitler‚Äù.
This follows Grok‚Äôs unsolicited ranting about white genocide in South Africa in May.
Grok also appeared to seek Elon‚Äôs opinion before answering difficult questions.
In their apology xAI attributed this behavior to ‚Äòdeprecated code‚Äô.
Despite this, xAI then announced a ~$200M contract with the US Department of Defense.
 | 187
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
stateof.ai 2025
BLOOPER REEL
    Grok praises spouts anti-semitic, racist rhetoric
Source: https://x.com/whstancil/status/1922780956810682745/photo/1
Source: https://www.cnbc.com/2025/07/11/grok-4-appears-to-reference-musks-views-when-answering-questions-.html


One anonymous employee claimed to have resigned due to concerns over the data contamination - where training data may have overlapped with test benchmarks (Meta formally denied this).
Meta appears to have optimised a special "conversational" version of its Llama-4 Maverick model specifically for the LM Arena benchmark while releasing a different, less capable version to developers.
Results significantly underperformed marketing claims: it was listed as 20th on LiveBench (although made some notable gains).
Head of AI Research Joelle Pineau resigned just 4 days before launch for unknown reasons. 
 | 188
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
stateof.ai 2025
BLOOPER REEL
    Llama-4: alleged data contamination, misleading benchmarking by overfitting and underwhelming vibes overall
Source: https://www.reddit.com/r/LocalLLaMA/comments/1jt8yug/serious_issues_in_llama_4_training_i_have/
Source: https://x.com/Ahmad_Al_Dahle/status/1909302532306092107
Source: https://techcrunch.com/2025/04/07/meta-exec-denies-the-company-artificially-boosted-llama-4s-benchmark-scores/
Source: https://livebench.ai/
Source: https://techcrunch.com/2025/04/07/meta-exec-denies-the-company-artificially-boosted-llama-4s-benchmark-scores/?guccounter=1&guce_referrer=aHR0cHM6Ly90aGVmbHkuY29tLw&guce_referrer_sig=AQAAAFcj-ux00jHXrXdRt9oyOcgHRfkdqXFRwl-EVVC2Iqye4sOOly1HnkMYp86PJcmTAkokqojKLf1UvwHqZ9IpDhGsYG6RN_rq6mBjzlyC0ii_sf6tCwLACLRuT_Z0YB-prtDS-6MXkpe7BEePq2X0tP3ouQSkcFIiAoq6QAqj7k-U



stateof.ai 2025
 | 189
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Section 3: Politics

Current White House AI leadership now includes some ‚Äúwho‚Äôs who‚Äù of Silicon Valley: David Sacks (AI & Crypto Czar), Sriram Krishnan (Senior AI Policy Advisor), and Michael Kratsios (Director of the Office of Science and Tech Policy). 
The 47th President‚Äôs AI agenda includes an aggressive rollback of Biden-era safety rules (EO 14179), rebranding the AI Safety Institute to the Center for AI Standards and Innovation (CAISI) (adios, safety), and launching a $500B ‚ÄúStargate‚Äù AI infrastructure push.
The AI Action Plan, announced July 2025, lays out the administration‚Äôs national strategy for US dominance in global AI.
An initial 10-year block on state/local AI laws in the ‚ÄúOne Big, Beautiful Bill‚Äù was dropped after bipartisan pushback but the push for state regulation rollbacks continues. 
    Trump‚Äôs second term brings beefed up policies that were hinted at but never fully executed during the first.
stateof.ai 2025
 | 190
Trump 47: back with a vengeance
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Source: https://www.ai.gov/
Source: https://www.ai.gov/action-plan
Source:https://trumpwhitehouse.archives.gov/presidential-actions/executive-order-maintaining-american-leadership-artificial-intelligence
Source: https://www.whitehouse.gov/presidential-actions/2025/01/removing-barriers-to-american-leadership-in-artificial-intelligence/
Source: https://fedscoop.com/trump-administration-rebrands-ai-safety-institute-aisi-caisi/
Source: https://www.nbcnews.com/tech/tech-news/big-beautiful-bill-ai-moratorium-ted-cruz-pass-vote-rcna215111
Source: https://www.forbes.com/sites/moorinsights/2025/01/30/the-stargate-project-trump-touts-500-billion-bid-for-ai-dominance/
Source: https://govciomedia.com/white-house-ai-czar-outlines-industrys-role-in-global-ai-race/



US tech stack exports: Executive Order 14320 establishes American AI Exports Program, an AI stack package (hardware, model, software, applications, standards) for allies and others.  
AI infra build-out: Plan calls for streamlining permitting, upgrading the national grid, and making federal lands available to support and build data centers and AI factories. 
Open source model leadership: US open source leadership is viewed as vital to US national security interests. 
Rollback of AI regulations: Federal agencies may withdraw discretionary AI spending to states with ‚Äúonerous‚Äù AI regulations.
Protecting ‚Äúfree speech‚Äù in deployed models: Federal procurement policies updated - US will only procure frontier LLMs ‚Äúfree from top-down ideological bias.‚Äù 
    Over 100 different policies are proposed to ensure US AI innovation and global leadership. But can the US bureaucracy execute? Some key takeaways from the 23-page plan:
stateof.ai 2025
 | 191
The AI Action Plan: America‚Äôs Grand AI Strategy 
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Source: https://www.ai.gov/action-plan
Source: https://www.cyberhaven.com/blog/americas-ai-action-plan-has-arrived-3-key-takeaways-that-data-security-leaders-need-to-know
Source: https://www.techpolicy.press/trump-ai-action-plan-raises-legal-questions-potentially-violates-constitution/





In May 2025 the administration rescinded the Biden-era AI Diffusion Rule, which had imposed tiered licensing on exports of advanced AI chips and closed-weight models and relied on close supply-chain monitoring.
The new approach favors proactive dissemination. Selected countries receive an integrated American AI stack that includes infrastructure, foundation models, deployment tooling, and governance templates.
Eligibility will likely track commercial opportunity, security alignment, and pressure from U.S. industry. Access will not be universal and end-use monitoring remains a stated requirement.
The strategy shifts value to U.S. vendors while raising risks of lock-in and uneven controls downstream. It also tests whether export packages can outperform pure restriction in limiting rival influence.
    US policy is shifting from broad diffusion controls to an export-led strategy. The American AI Exports program packages compute, models, cloud services, and compliance into a USG-endorsed ‚ÄúAmerican AI stack‚Äù for selected partners. The aim is to shape standards, build dependency, and counter China‚Äôs Digital Silk Road playbook.
stateof.ai 2025
 | 192
From controls to exports: America‚Äôs ‚ÄúAI Stack‚Äù strategy
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Source:https://x.com/sundeep/status/1961078492382089631?s=46&t=8YCMEcmVVXRPm8SXTMgdlw@
Source: https://www.whitehouse.gov/presidential-actions/2025/07/promoting-the-export-of-the-american-ai-technology-stack/
Source: https://www.ai.gov/action-plan
Source:https://www.aoshearman.com/en/insights/ai-diffusion-rule-rescinded-policy-guidance-for-advanced-integrated-circuits-and-commodities-issued
Source: https://www.cfr.org/china-digital-silk-road/







January 2025: the Commerce Secretary nominee publicly backed tougher controls, signaling a harder line at the outset.
March-April: the Department of Commerce expanded AI-chip restrictions to China, effectively blocking sales of previously ‚Äúcompliant‚Äù parts such as the H20 and warning that loopholes would be closed.
July: after sustained industry lobbying, the department cleared a downgraded H20 for the China market, reopening a controlled channel while keeping top-end parts off-limits.
August: the US government reached conditional license terms with NVIDIA and AMD that include a 15% revenue give-back on China sales, and Congress took up the GAIN AI Act to prioritize domestic buyers, steps that mix export access with tighter industrial policy.
The net effect is that vendors face stop-start compliance and pricing risk while Beijing accelerates local alternatives and a gray market thrives when rules shift faster than enforcement.
    2025 saw fast reversals between restriction and accommodation. The administration is balancing national-security aims with supply-chain reliance and vendor lobbying, putting NVIDIA and AMD in the political spotlight and injecting uncertainty for partners and compliance teams.
stateof.ai 2025
 | 193
US chip-export policy zigzags test leverage over China
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Source: https://www.ft.com/content/6f806f6e-61c1-4b8d-9694-90d7328a7b54
Source: https://www.bis.gov/press-release/commerce-further-restricts-chinas-artificial-intelligence-advanced-computing-capabilities
Source: https://www.nytimes.com/2025/01/29/business/economy/lutnick-trump-tariffs-china.html
Source: https://www.congress.gov/amendment/119th-congress/senate-amendment/3505/text
Source: https://www.cnbc.com/2025/07/15/howard-lutnick-says-china-is-only-getting-nvidias-4th-best-ai-chip.html
Source: https://www.theglobeandmail.com/business/international-business/us-business/article-nvidia-ceo-huang-to-meet-with-trump-thursday-source-says/





 | 194
No Pain, No GAIN
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
    The GAIN AI Act would require chipmakers to fulfill orders from US-based customers before selling advanced GPUs to ‚Äúcountries of concern‚Äù (e.g. China, Russia, NK). Unsurprisingly, NVIDIA is not happy.
stateof.ai 2025
NVIDIA has long echoed the sentiment that demand for their GPUs outstrips supply. When dealing with these global constraints, Huang has claimed that NVIDIA ‚Äúallocates fairly.‚Äù This stance has enabled the legal shipment of roughly 1.9M GPUs to China since 2023. Yet, defanged chips still compete with other SKUs for capacity across the value chain. Therefore, the sale to Chinese customers has either lengthened lead times for US clouds OR the supposed supply chain bottlenecks have been exaggerated by NVIDIA for many years.
In its first volley, NVIDIA also likened GAIN to the AI Diffusion Rule, which it claimed to be ‚Äúbased on doomer science fiction.‚Äù Yet, the former only applies to countries subjected to an American arms embargo or nations that the DNI* deems to be hosting (or intends to be hosting) a military or intelligence facility associated with an embargoed country. Despite certain subjectivities, the scope is clearly narrower than the Diffusion Rule, which applied to the entire world. 

*Director of National Intelligence (currently Tulsi Gabbard)
Source: https://www.congress.gov/amendment/119th-congress/senate-amendment/3505/textSource: https://fortune.com/2024/02/21/nvidia-earnings-ceo-jensen-huang-gpu-demand-supply-allocate-fairly/Source: https://www.theregister.com/2025/09/04/us_senator_americans_first_ai_sillicon/Source: https://x.com/nvidianewsroom/status/1964056503666516000

 | 195
Rendering new verdicts: NVIDIA‚Äôs recent attempts to exert influence in Washington
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
    To maintain its presence in China, NVIDIA has rapidly built up its presence in DC. Since the first round of export restrictions, NVIDIA has grown both its internal government affairs team and its external lobbying expenditures. 

After new contracts with Brownstein and BGR Group, NVIDIA gradually approaches the lobbying expenditures of the top spending tech firms (Meta: ~$14M YTD and Google: ~$8M YTD). NVIDIA‚Äôs expenditures are expected to increase further this year as decisions surrounding the B30A loom closer.
NVIDIA's moves reflect the company's unwillingness to abandon the Chinese chip market. From a profit-maximization perspective, China will represent a "$50B market" in 2026. While that potential revenue can bolster R&D spending, shipments also strengthen the CUDA ecosystem, which benefits from the ~1.5M active developers based in China. Losing these ‚Äòmoat diggers‚Äô could have serious long-term consequences. Similarly, although Chinese challengers already receive large government subsidies, NVIDIA's exit from that market would directly inject capital in these organizations.


stateof.ai 2025
Source: https://www.thewirechina.com/2025/09/07/walling-off-china/Source: https://www.politico.com/newsletters/politico-influence/2025/08/15/nvidia-washington-lobby-brownstein-00512471Source: https://www.opensecrets.org/federal-lobbying/clients/summary?cycle=2022&id=D000036303
Source: https://www.trendforce.com/news/2025/05/21/news-jensen-huang-nvidias-china-market-share-plummets-from-95-to-50-amid-u-s-export-curbs/Source: https://daoinsights.com/news/nvidia-ceo-jensen-huang-visits-beijing-continue-cooperation/

 | 196
But Washington may not be the only place NVIDIA needs to lobby‚Ä¶
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
    Even with the administration‚Äôs support, NVIDIA‚Äôs stable presence is far from assured in China. On September 15, 2025, Chinese antitrust regulators announced that NVIDIA was in violation of China‚Äôs antitrust rules for a 2020 $7B acquisition of Israeli Mellanox Technologies, a networking products supplier. China originally approved the purchase on the condition that NVIDIA not discriminate against Chinese customers, but that condition was all but impossible to abide in the face of previous US export bans. No penalty has yet been announced but NVIDIA‚Äôs fine can be as much as 10% of its annual (most recently completed fiscal year) China sales. 
The timing of the announcement raises eyebrows as it came during US-China trade talks in Madrid. The immediate absence of any penalty to the preliminary ruling gives China potential leverage. Interestingly enough, Chinese antitrust authorities usually announce the fine at the same time as the ruling but may be holding back here to give trade-peace a chance. 
The decision, too, comes at the same time China has been encouraging its domestic tech companies to not use the H20. So while, in the long term, China hopes to wean itself off American chips, in the short-term, China still does need NVIDIA - even though it is eager to show it does not. 

stateof.ai 2025
Source: https://www.semafor.com/article/09/15/2025/china-says-nvidia-violated-antitrust-rules-as-trade-rift-deepens (for chart)
Source:https://www.wsj.com/tech/china-probe-says-nvidia-violated-antitrust-laws-4acf344c?gaa_at=eafs&gaa_n=ASWzDAgGGJwfN9K31zYJ7NpBsPl7GWcWwsviMP8oejyc6s1pI3KFfva3W2PL83P-otQ%3D&gaa_ts=68c877e1&gaa_sig=6UCBQYmQzzEFtu95VRBqt9ZTc9V77o9ABQZbo9xzTXdG9Qyrx3HWTHzBS1Bbxw-o9-9pQ5M0yFqrzjL55wkYYw%3D%3D
Source: https://www.bloomberg.com/news/articles/2025-09-15/china-finds-nvidia-violated-antitrust-law-after-initial-probe
Source: https://www.reuters.com/world/china/china-cautions-tech-firms-over-nvidia-h20-ai-chip-purchases-sources-say-2025-08-12/
Source: https://uk.practicallaw.thomsonreuters.com/6-602-3865?transitionType=Default&contextData=(sc.Default)&firstPage=true
Source: https://www.reuters.com/business/retail-consumer/china-regulators-fine-alibaba-275-bln-anti-monopoly-violations-2021-04-10/




    Trump has openly criticized the CHIPS Act, arguing elsewhere that all subsidies were ‚Äútaxpayer handouts.‚Äù Recently, Trump said he will be putting a ‚Äúfairly substantial tariff‚Äù on semiconductor imports to incentivize domestic chip production. Those tariffs, however, would carry exemptions for companies that agree to invest in the US. Rather than promote onshoring chip production through allotted subsidies under the bipartisan CHIPS Act, the administration, motivated by seeing an ROI, has used tariffs and other alternative strategies to encourage onshoring. 
stateof.ai 2025
 | 197
Chips, Tariffs, Subsidies‚Ä¶oh my!
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Source: https://www.nytimes.com/live/2025/03/05/us/trump-speech-congress
Source: https://www.cnbc.com/2025/09/05/trump-warns-of-fairly-substantial-chip-tariffs-signals-apple-others-safe-ai-tim-cook-tech.html
Source:https://www.whitehouse.gov/fact-sheets/2025/07/fact-sheet-president-donald-j-trump-ends-market-distorting-subsidies-for-unreliable-foreign-controlled-energy-sources/
Source: https://www.cfr.org/blog/unpacking-tsmcs-100-billion-investment-united-states
Source: https://www.nytimes.com/2025/03/03/technology/tsmc-investment-trump.html
Source: https://www.reuters.com/world/china/trump-says-us-levy-100-tariff-imported-chips-some-firms-exempt-2025-08-07/
Source: https://dig.watch/updates/south-korean-chipmakers-avoid-us-tariffs-through-domestic-investments




    Besides tariffs, one of the new non-CHIPS strategies includes acquiring equity in chipmakers. After accusing the newly-installed Intel CEO, Lip-Bu Tan, of being ‚Äúhighly-conflicted‚Äù because of $200M+ investments in Chinese companies, Trump had the US Gov acquire a 10% stake in the struggling chipmaker in return for ~$8.9B of earmarked CHIPS grants. This isn‚Äôt the first time the US bought a stake in a company to promote national interests (see US Steel, AIG, and GM). The move replaces the failed Intel and TSMC joint venture that Trump and Lutnick pushed. Is this opportunism or the start of a wider US industrial policy?

stateof.ai 2025
 | 198
US and Intel: a new, unexpected, stake-based friendship‚Ä¶the start of something new?  
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Lutnick has also flirted with taking a stake in Samsung and TSMC. But the fact that the USG is getting involved in the private sector by drawing on a strategy normally reserved for crises is a change in precedent. Tax breaks and subsidies, the tools the USG has normally used to focus the private sector, may give way to a new strategy of direct investment by the USG. Some CEOs, Republicans, and others have been rumored to have deep concerns about the deal. 
Unexpectedly, Sen. Bernie Sanders (I-VT) agreed with Lutnick that taxpayers should see a return from the US‚Äôs heavy investments in domestic chip manufacturing. 
Source: https://www.reuters.com/technology/intel-tsmc-tentatively-agree-form-chipmaking-joint-venture-information-reports-2025-04-03/
Source: https://www.economist.com/business/2025/08/21/to-survive-intel-must-break-itself-apart
Source: https://www.intc.com/news-events/press-releases/detail/1748/intel-and-trump-administration-reach-historic-agreement-to
Source: https://www.bbc.com/news/articles/c70x6602pdyo
Source: https://www.nytimes.com/2025/08/25/us/politics/trump-intel-economy-strategy.html
Source: https://thehill.com/homenews/administration/5470306-trump-intel-deal-republican-criticism/
Source:https://www.wsj.com/economy/the-u-s-marches-toward-state-capitalism-with-american-characteristics-f75cafa8?gaa_at=eafs&gaa_n=ASWzDAjN-l4Iucdjc4ezd8e0lrn0qdgWtgI3ilGfekWPGjyPmpDWG_B3TmaxKuSPMTU%3D&gaa_ts=68ae4c06&gaa_sig=k-rF1f1MgfHKYbVgOYzmsXsyewtqLQKsc6cUEOdVsPSDYU1tAyvDj03uicHzYgGiW3xHRJPjG5ATfmOZztu-cA%3D%3D




stateof.ai 2025
 | 199
But there‚Äôs more unusual US Government partnerships with the private sector‚Ä¶
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Trump administration allows NVIDIA and AMD to sell AI chips to China on condition that the USG receive a 15% cut of sales. 
USG given a ‚ÄúGolden Share‚Äù that allows the USG to appoint one independent director to US Steel‚Äôs board and gives the US President veto powers over certain specified US Steel decisions. 
Lutnick discussed the possibility of buying a stake in various US defense companies, claiming that the USG is already responsible for most of their revenue. 
MP Materials, a rare earth minerals company, closed a 10 year deal with the DoD, where the DoD purchased up to $400 million-worth of MP stock, with the option to increase its share of the company to 15% later on. 
Source: https://truthsocial.com/@realDonaldTrump/posts/115089582863713336
Source: https://www.nytimes.com/2025/08/10/technology/us-government-nvidia-amd-chips-china.html
Source: https://fas.org/publication/unpacking-dod-and-mp-partnership/
Source: https://www.cfr.org/article/nippon-u-s-steel-deal-golden-share-and-magic-beans
Source: https://www.cnbc.com/2025/08/07/apples-tim-cook-convinced-trump-to-drop-made-in-usa-iphone-for-now.html
Source: https://www.cnbc.com/2025/08/26/cnbc-excerpts-us-commerce-secretary-howard-lutnick-speaks-with-cnbcs-squawk-box-today.html
Source: https://mpmaterials.com/news/mp-materials-announces-transformational-public-private-partnership-with-the-department-of-defense-to-accelerate-u-s-rare-earth-magnet-independence/
Source:https://www.wsj.com/economy/the-u-s-marches-toward-state-capitalism-with-american-characteristics-f75cafa8?gaa_at=eafs&gaa_n=ASWzDAjN-l4Iucdjc4ezd8e0lrn0qdgWtgI3ilGfekWPGjyPmpDWG_B3TmaxKuSPMTU%3D&gaa_ts=68ae4c06&gaa_sig=k-rF1f1MgfHKYbVgOYzmsXsyewtqLQKsc6cUEOdVsPSDYU1tAyvDj03uicHzYgGiW3xHRJPjG5ATfmOZztu-cA%3D%3D





    Still, the strangest USG-private sector arrangement comes between the USG and ByteDance. In Sept., the US and China agreed to a deal to end the years-long tease in which the US implemented but never executed a ban on TikTok. TikTok will be allowed to operate in the US with a copy of TikTok‚Äôs recommendation algorithm sent to Oracle. Oracle will retrain the algorithm and be responsible for data and algorithmic security. ByteDance will own just under 20% of the new US-based company (at a total valuation of $14B) with a group of new investors, including Oracle, owning a majority. Six of the seven board seats will be filled by Americans (TBD). The USG will also receive an unknown multi-billion dollar fee from investors for its role in the negotiations. 
stateof.ai 2025
 | 200
TikTok USA: the ‚Äúwill they/won‚Äôt they‚Äù storyline comes to an end 
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
The TikTok deal sets the benchmark for data sovereignty: algorithm needs to be retrained using local data that is protected and stored locally. The data collection issue is clearly addressed. Indeed, it is hard to see ByteDance exfiltrating data or influencing the US app now that it only has a minority stake. 
While not requiring congressional approval, the deal will certainly receive intense congressional scrutiny, not least because of new free speech worries in light of the FCC-Kimmel jawboning controversy. It is the Dept. of Treasury (specifically, the Committee on Foreign Investment in the US) that will oversee and shape Oracle‚Äôs retraining and deployment of TikTok USA and may or may not require other *ahem* ‚Äúprecautions.‚Äù  
ü´°
ü´°
Source: https://www.ft.com/content/d27e00b5-1997-4f28-b0f7-19139167be31
Source: https://www.wsj.com/tech/tiktok-deal-us-china-details-b383abe4
Source: https://www.scotusblog.com/2025/01/supreme-court-upholds-tiktok-ban/
Source: https://www.bloomberg.com/news/articles/2025-09-24/tiktok-deal-why-oracle-has-a-key-role
Source: https://www.ft.com/content/d27e00b5-1997-4f28-b0f7-19139167be31
Source: https://www.nytimes.com/2025/09/16/technology/tiktok-fourth-delay.html
Source: https://hls.harvard.edu/today/will-the-us-ban-tiktok/
Source: https://www.wsj.com/business/deals/u-s-government-expected-to-get-multibillion-dollar-fee-in-tiktok-deal-685e3944?gaa_at=eafs&gaa_n=ASWzDAhr5uTCvHV19HvL83iXwcERHiuElhJx5G5pOyXcujRiTBFeyKYtLkqV5rfCrNU%3D&gaa_ts=68e7282c&gaa_sig=X1v3PzWvNdtMGiTVg1XLQjwUgnsP1ty_fHqIXDGTdJjt1rAWnnee3IwkCG8xNAGQMudrpf-cX7wwJ4JeO5VeSQ%3D%3D








Fermi America, for instance, is seeking DOE loans to build the ‚ÄúDonald J. Trump Advanced Energy and Intelligence Campus,‚Äù a gas and nuclear power complex in Amarillo, Texas for energizing data centers.
This includes expediting review under the National Environmental Policy Act (NEPA) and rolling back requirements in other regulations like the Clean Water Act and Clean Air Act. DOE announced PermitAI: a custom-built AI on NEPA filings to streamline the permitting process. 
Trump announced the upgrade/expansion of the national electrical grid, incl. a $1B investment from Japan‚Äôs Hitachi. The USG has also been eying an expansion of fossil fuels while rolling back clean energy subsidies.
    The federal government‚Äôs AI Action Plan is treating AI infrastructure as a national priority, but rather than funding construction directly, its main role is to weaken environmental regulations and expand energy supply so private companies can accelerate data center build-out.
stateof.ai 2025
 | 201
If you build it, the AI will come: the USG streamlining strategies for building AI infra. 
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Because federal policy is prioritizing fossil fuel expansion, newly built AI data centers are likely to be locked-in to those energy sources, threatening future decarbonization efforts. 
Source: https://www.whitehouse.gov/wp-content/uploads/2025/07/Americas-AI-Action-Plan.pdf
Source: https://www.techtarget.com/whatis/feature/Stargate-AI-explained-Whats-in-the-project
Source: https://www.whitehouse.gov/presidential-actions/2025/07/accelerating-federal-permitting-of-data-center-infrastructure/
Source: https://www.washingtonpost.com/business/2025/06/26/nuclear-artificial-intelligence-datacenters-trump
Source:https://www.reuters.com/business/energy/hitachi-invest-1-billion-produce-power-grid-components-us-2025-09-04
Source: https://www.energy.gov/policy/articles/faster-better-permitting-permitai
Source: https://www.whitehouse.gov/presidential-actions/2025/01/declaring-a-national-energy-emergency/
Source: https://www.rogerebert.com/far-flung-correspondents/american-pastime-on-the-sustained-power-of-field-of-dreams



    The American public increasingly pushes back against new AI data center build outs in the latest political flashpoint. Hyperscalers‚Äô AI aspirations may soon be capped by how well they can navigate this live wire.
 | 202
AI data center‚Äôs latest bottleneck: NIMBYism
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Farmers emerge as one of the leading factions, driven largely by environmental concerns and competition for resources (land, water, and power). Other residents raise concerns over light pollution, air quality, and noise levels.
Growing domestic opposition could force American hyperscalers to offshore more clusters. This threatens the flow of spending that has buoyed the greater US economy. However, these clusters do challenge the livelihoods of many Americans, something hyperscalers have yet to reconcile.
stateof.ai 2025
Data Center Watch: $64B in planned data center projects have been blocked or delayed amid local opposition.
The following projects have been stalled or withdrawn due to concerns raised by nearby residents: Google‚Äôs Project Flo in Franklin Township, Indiana; QTS and Compass‚Äôs project in Prince William County, Virginia; and CRG‚Äôs Project Cumulus in Saint Charles, Missouri.
Source: https://www.datacenterwatch.org/reportSource: https://www.datacenterdynamics.com/en/news/google-withdraws-rezoning-proposal-for-468-acre-data-center-project-in-franklin-township-indianapolis/Source: https://www.datacenterdynamics.com/en/news/aws-withdraws-plans-for-third-data-center-campus-in-louisa-county-virginia/

In ‚Äò21, the Eric Schmidt-led National Security Commission on AI issued a report that called for annual federal funding for non-military AI R&D to reach $32B by ‚Äò26. The Bipartisan Senate AI Working Group published an ‚ÄúAI Roadmap‚Äù (May ‚Äò24) that echoed the same $32B price tag. 
In ‚Äò25, Trump‚Äôs proposed budget allotted $3.316B for non-defense AI R&D spending with $1.95B for ‚Äúcore‚Äù AI R&D. In general, total federal AI non-military R&D spending has hovered around $3B since ‚Äò23. 
It is unclear if foundational AI research will increase. The administration
    The AI Action Plan mentions the need to fund ‚Äúbasic science‚Äù in AI, but ‚Äúcore‚Äù AI R&D is far below the $32B that experts have recommended the US invest by ‚Äò26. This is foundational, non-commercial research - the type of research that is sometimes argued to be more transformative than that of the private sector. 
stateof.ai 2025
 | 203
Foundational AI Research is a government priority, but where‚Äôs the money? 
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
has freezed funds from universities and the NSF to ensure compliance with new priorities. 
The US‚Äôs NAIRR, the shared national infrastructure for AI research and education, will move from a pilot program to a coordinated national program in the coming year, however. The $2.6B price tag associated with the project is split among different federal agencies and companies. 
Source: https://www.nitrd.gov/pubs/FY2025-NITRD-NAIIO-Supplement.pdf (for federal non-defense AI R&D numbers) 
Source: https://hai.stanford.edu/news/new-report-details-costs-and-structure-national-ai-research-resource (for NAIRR costs) 
Source: https://reports.nscai.gov/final-report/ (for recommended non-military federal AI spending)
Source: https://www.schumer.senate.gov/imo/media/doc/Roadmap_Electronic1.32pm.pdf

Source: https://www.gao.gov/products/gao-17-711 (for federal PR spending) 
Source: https://investor.atmeta.com/investor-news/press-release-details/2025/Meta-Reports-Fourth-Quarter-and-Full-Year-2024-Results/ (for Meta‚Äôs ‚Äò24 R&D budget) 
Source: https://energycommerce.house.gov/posts/subcommittee-chair-duncan-opening-remarks-on-improving-spent-nuclear-fuel-management (for nuclear disposal costs) 
Source: https://www.gao.gov/products/gao-23-106200#:~:text=Federal%20agencies%20spend%20about%20%242,annually%20to%20lease%20office%20buildings.(for federal building maintenance and lease costs) 

Source: https://www.whitehouse.gov/wp-content/uploads/2025/03/M-25-13-Temporary-Pause-to-Review-Agency-Grant-Loan-and-Other-Financial-Assistance-Programs.pdf
Source: https://www.reuters.com/world/us/white-house-sets-hiring-foreign-enrolment-terms-colleges-get-funding-advantage-2025-10-02/







    State lawmakers were busy debating AI legislation this past year with more than 1,080 (!) ‚ÄúAI-related‚Äù bills introduced across the states with 118 becoming law. The five states that had the most AI legislation introduced were New York, Illinois, California, Maryland, and Texas. Current state laws that are most likely to pass typically fall under one of the following categories: 
stateof.ai 2025
 | 204
AI regulation across the U.S. states: winners and losers
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
AI-generated child sexual abuse material (CSAM)/non-consensual intimate imagery (NCII) bans: laws prohibiting the creation and dissemination of AI-generated child pornography or non-consensual sexually explicit photos of adults; creating takedown rules for online platforms.
Transparency & disclosure requirements: laws requiring companies to disclose when a customer is interacting with AI (especially chatbots or generative AI) or to label AI-generated content with visible notices/metadata. 
Government AI use restrictions: laws that restrict how state government agencies can use AI, especially for surveillance, law enforcement or government decision-making. 
Health & Employment: specifically, laws requiring the disclosure that an AI was used in doctor/patient interactions or employer/job candidate decisions and requiring human oversight, in some cases. 
Source: https://www.multistate.ai/artificial-intelligence-ai-legislation
Source: https://www.brookings.edu/articles/how-different-states-are-approaching-ai/
Source: https://www.ncsl.org/technology-and-communication/artificial-intelligence-2025-legislation
Source: https://hai.stanford.edu/ai-index/2025-ai-index-report/policy-and-governance
Source: https://www.multistate.ai/updates/vol-71
Source: https://stevenadler.substack.com/p/mythbusting-the-supposed-1000-ai
Source: https://www.brookings.edu/articles/how-different-states-are-approaching-ai/
Source: https://modernization.stateinnovation.org/state-state-legislatures






    The first state law requiring public, standardized safety disclosures from large AI developers was signed into law in Sept. Last year, CA State Senator Scott Wiener tried his hand in passing an aggressive AI safety bill (i.e. SB1047) only to face pushback from industry leaders, celebrated AI scientists, and Governor Newsom, who, in the end, vetoed the bill. Newsom convened an AI expert working group that included Wiener, and the group‚Äôs recommendations formed the backbone of SB53. Modest as the law is (it applies only to the largest developers and accounts primarily for significant harms), it will set the national standard for AI transparency requirements. 
stateof.ai 2025
 | 205
Winner: California‚Äôs SB53 (‚ÄúTransparency in Frontier Act‚Äù) 
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
The law will apply only to large frontier model developers (10^26 FLOPS and >$500M in revenue). These developers will need to make available safety and security protocols on their websites and issue public risk assessment results any time they release or update a new frontier model. Importantly, developers need to notify state authorities of critical safety incidents or threats of imminent danger. Whistleblower protections are also included in the legislation. 
Other states are taking note: both Michigan and New York are considering similar transparency requirements for large developers with New York‚Äôs law awaiting signature from Gov. Kathy Hochul. 
Source: https://techcrunch.com/2025/09/23/scott-wiener-on-his-fight-to-make-big-tech-disclose-ais-dangers/
Source: https://legiscan.com/CA/text/SB53/id/3270002
Source: https://x.com/CTATech/status/1964005445720293505
Source: https://cdn.openai.com/pdf/oai_ca-safety-letter_8-11-25.pdf
Source: https://a16z.com/the-commerce-clause-in-the-age-of-ai-guardrails-and-opportunities-for-state-legislatures/
Source: https://sd11.senate.ca.gov/sites/sd11.senate.ca.gov/files/images/7.jpg‚Äô
Source: https://www.anthropic.com/news/anthropic-is-endorsing-sb-53
Source: https://www.mofo.com/resources/insights/251001-california-enacts-ai-safety-transparency-regulation-tfaia-sb-53
Source: https://techcrunch.com/podcast/california-just-drew-the-blueprint-for-ai-safety-regulation-with-sb-53/












    Despite SB53‚Äôs passage, state governance laws that apply across different sectors and impose safety obligations on private sector entities are few and far between. Other previously enacted AI governance laws were revised and narrowed over the last year out of renewed fears that the regulation might be too onerous. Attempts to include third party auditing requirements across state laws like SB53 were resisted and the requirement dropped in most cases. 
stateof.ai 2025
Gov Youngkin (Virginia) vetoed state AI governance legislation (HB2094) because the legislation would place burdens on the AI industry. The Connecticut Gov also said he would veto AI legislation for the same reason. 
Texas passed the Responsible AI Governance Act (TRAIGA), but it was amended during committee discussions to remove obligations like: ‚Äúduty of care,‚Äù impact assessments, and high-risk AI system disclosures. 
Utah passed new amendments to its Utah AI Policy Act (UAIPA) that narrowed key legal requirements to apply only to high-risk GenAI systems. 
Colorado delayed implementation of its AI Act-modeled ‚ÄúColorado AI Act‚Äù until June ‚Äò26 with amendments being debated by the state legislature. 

 | 206
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Loser: omnibus AI laws winnow down and third party auditing a no-go for now   
Source: https://iapp.org/resources/article/us-state-ai-governance-legislation-tracker/#state-ai-governance-law-map 
Source: https://iapp.org/news/a/virginia-governor-vetoes-ai-bill
Source: https://www.ctinsider.com/politics/article/if-bill-ai-survives-ct-house-vote-lamont-19444053.php
Source: https://www.lw.com/en/insights/texas-signs-responsible-ai-governance-act-into-law
Source: https://www.davispolk.com/insights/client-update/utah-scales-back-reach-generative-ai-consumer-protection-law
Source: https://www.seyfarth.com/news-insights/artificial-intelligence-legal-roundup-colorado-postpones-implementation-of-ai-law-as-california-finalizes-new-employment-discrimination-regulations-and-illinois-disclosure-law-set-to-take-effect.html
Source: https://www.wilmerhale.com/en/insights/blogs/wilmerhale-privacy-and-cybersecurity-law/20251001-transparency-in-frontier-artificial-intelligence-act-sb-53-california-requires-new-standardized-ai-safety-disclosures


In submissions to Trump‚Äôs AI Action Plan, OpenAI, Meta, and Google all called for the preemption of state AI laws to free them of liability for nearly all state AI legislation. VC firm Andreessen Horowitz argued that AI state laws could be considered unconstitutional under the interstate commerce clause. 
But the show goes on and companies face a trio of choices to manage the ‚Äúpatchwork‚Äù: 1) structure compliance according to the most stringent legislative frameworks (like the Colorado AI Act) 2) fragment compliance state by state 3) lobby and wait for federal action that may never come. 
    The Trump administration pushed for a state AI moratorium in part because it believed that state AI legislation was too uneven and confusing to be effective. Some states have been more active than others in regulating AI. Of course, AI companies are not happy. A receptive Trump administration has encouraged the biggest developers and investors to take a staunch anti-state legislation stance. It is not about what kinds of laws states should pass but whether state legislatures should be passing any AI legislation to begin with.    
stateof.ai 2025
 | 207
The State of State AI Regulation is a ‚Äúpatchwork‚Äù problem
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Source: https://hai.stanford.edu/ai-index/2025-ai-index-report/policy-and-governance (for chart)
Source: https://www.multistate.ai/artificial-intelligence-ai-legislation
Source: https://www.ncsl.org/technology-and-communication/artificial-intelligence-2025-legislation
Source: https://www.obsolete.pub/p/inside-techs-risky-gamble-to-kill
Source: https://a16z.com/the-commerce-clause-in-the-age-of-ai-guardrails-and-opportunities-for-state-legislatures/


    Trump‚Äôs landmark One Big Beautiful Bill (now a federal spending statute) initially contained a preemption clause that conditioned state broadband funds on the implementation of a 10-year moratorium on state and local AI regulations. The controversial provision was struck down before the bill‚Äôs final passage. But the idea lives. Senator Ted Cruz (R-TX), the moratorium lead, introduced a regulatory sandbox bill (SANDBOX Act) that would permit companies to apply for waivers from ‚Äúobstructive legislation‚Äù for up to 10 years. 
stateof.ai 2025
 | 208
But the Big, Beautiful fight over AI preemption continues‚Ä¶in the sandbox

 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Despite being close to implementation, the provision was unpopular from the start with unlikely bedfellows joining in bipartisan condemnation. Indeed, the provision was only removed because Senator Blackburn (R-TN) was worried about child safety and removing recent Tennessee-passed copyright protections for artists (see ELVIS Act). 
Anthropic lobbied Congress, arguing that streamlined federal regulation should be established before preempting state/local oversight. Labs appear to prefer light-touch federal regulation over no regulation, but the latter may be preferred over a patchwork of state laws. The number of state bills in motion is often overstated; in truth, the IAPP estimates that only ~40 bills (of the over 1k introduced) may significantly impact American AI labs. 
Source: https://x.com/DavidSacks/status/1930049764080452038?lang=en
Source: https://www.theverge.com/ai-artificial-intelligence/776130/senator-ted-cruz-ai-sandbox-bill 
Source: https://www.congress.gov/bill/119th-congress/house-bill/1/text
Source: https://www.politico.com/news/2025/09/16/not-at-all-dead-cruz-says-ai-moratorium-will-return-00566369 Source: https://www.semafor.com/article/05/30/2025/anthropic-emerges-as-an-adversary-to-trumps-big-billSource: https://www.nytimes.com/2025/06/05/opinion/anthropic-ceo-regulate-transparency.htmlSource: https://stevenadler.substack.com/p/mythbusting-the-supposed-1000-ai
Source: https://www.commerce.senate.gov/services/files/4DCC55D7-FEA6-4BCF-9B5F-0B1667F3D860
Source: https://statescoop.com/state-attorneys-general-reject-federal-ai-law/
Source: https://www.nytimes.com/2025/06/30/us/politics/senates-new-ai-moratorium-proposal-draws-fresh-criticism.html



What‚Äôs new in the U.S. bill: Centralized sandboxing at OSTP with participating agencies (e.g., FTC) reviewing, but non-response means approval. Denials can be appealed to OSTP. 
How it differs from the EU: EU AI Act mandates regulator-led sandboxes focused on risk mitigation and proof-of-compliance, not multi-year waivers from rules.  
Pros: Early, supervised deployments, faster feedback loops for regulators, and visibility into frontier systems before scale.  Ôøº
Cons: Incentives for forum shopping and regulatory capture, uneven protections, could function as de-facto deregulatory waivers if ‚Äúauto-approval‚Äù becomes common. 
    The U.S. ‚ÄúSANDBOX Act‚Äù would create a federal AI sandbox run by the Office of Science and Technology Policy that grants time-limited waivers from existing rules. Firms could receive two-year waivers, renewable up to five times (max 10 years), with ‚Äúauto-approval‚Äù if an agency doesn‚Äôt respond in 90 days and an appeal path to OSTP. Critics say this risks weakening enforcement, while supporters say it accelerates learning-by-doing.  
stateof.ai 2025
 | 209
Regulatory sandboxes: innovation boost or waiver machine?
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Source: https://www.congress.gov/bill/119th-congress/senate-bill/2750
Source: https://www.theverge.com/ai-artificial-intelligence/776130/senator-ted-cruz-ai-sandbox-bill 
Source: https://www.commerce.senate.gov/services/files/4DCC55D7-FEA6-4BCF-9B5F-0B1667F3D860 
Source: https://artificialintelligenceact.eu/article/57/
Source: https://iapp.org/news/a/how-different-jurisdictions-approach-ai-regulatory-sandboxes


    Trump‚Äôs re-inauguration has brought an abrupt stop to an era of international diplomacy that emphasized AI safety, alignment, and the formulation of voluntary AI measures that would ‚Äúpromote reliable and trustworthy AI.‚Äù The list of the casualties includes:

stateof.ai 2025
 | 210
RIP International AI Governance, AI Treaties, and Global AI Safety Alignment 
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions

Council of Europe Framework Convention on AI 
An AI human rights treaty that opened for signature on September 5, 2024. 
16 countries + EU have signed, but zero signees have ratified the agreement. 
The G7 Hiroshima AI Principles and Code of Conduct 
OECD launches a voluntary framework enabling firms to report how they incorporate the G7 AI principles. 
2025 G7 Kananaskis press release includes vague commitments to ‚Äúleverage the outcomes of the Hiroshima AI Process,‚Äù whatever that means.  
The United Nations 
In 2024, UN convenes an AI advisory body to ‚Äúundertake analysis and advance recommendations for the international governance of AI.‚Äù Publishes a report; US announces it is rejecting world AI oversight. 
In 2025, UN announces two ‚Äúmechanisms‚Äù (i.e. committees)  to promote ‚Äúcooperation.‚Äù 
200+ experts sign a strongly-worded letter to encourage ‚ÄúAI red lines.‚Äù  
Source: https://www.coe.int/en/web/artificial-intelligence/the-framework-convention-on-artificial-intelligence
Source: https://www.consilium.europa.eu/media/eeiniuqd/ai-en.pdf
Source: https://transparency.oecd.ai/
Source: https://www.un.org/en/delegate/two-new-mechanisms-promote-cooperation-ai-governance
Source: https://red-lines.ai/
Source: https://www.nbcnews.com/tech/tech-news/us-rejects-international-ai-oversight-un-general-assembly-rcna233478
Source: https://www.un.org/digital-emerging-technologies/ai-advisory-body




Its purpose was to create a global network of publicly funded government agencies that would meet regularly to develop best practices and share research for monitoring and reporting harms, risks, and incidents. 
The network has met twice in ‚Äò25 for joint testing exercises evaluating the risks of agentic systems. The US did not appear at either meeting. 
The U.S. AI Safety Institute has rebranded itself under the Trump Administration as the Center for AI Standards and Innovation while the UK AI Safety Institute has rebranded itself as the AI Security Institute
Unlikely that the US will participate in any future AI Safety Institute Network events. At the same time the Network was meeting in Paris, VP Vance gave a speech before the AI Summit saying as much: ‚ÄúThe AI future is not going to be won by hand‚Äëwringing about safety.‚Äù 
    On November 21, 2024, representatives of AI safety institutes from Australia, Canada, the EC, France, Japan, Kenya, South Korea, Singapore, the UK, and the US gathered in San Francisco to announce the International Network of AI Safety Institutes. Almost a year later and the network has met twice with the US a no-show.   
stateof.ai 2025
 | 211
RIP AI Safety Institute Network 
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Source: https://oecd.ai/en/wonk/ai-safety-institutes-challenge
Source: https://dig.watch/updates/us-ai-safety-institute-director-steps-down-amid-uncertainty
Source: https://www.mlex.com/mlex/articles/2368727/ai-safety-institutes-remain-focused-despite-politics-south-korean-official-says
Source: https://digital-strategy.ec.europa.eu/en/news/first-meeting-international-network-ai-safety-institutes
Source: https://www.presidency.ucsb.edu/documents/remarks-the-vice-president-the-artificial-intelligence-action-summit-paris-france
Source: https://www.techpolicy.press/shaping-ai-standards-to-protect-americas-most-vulnerable-tech-innovators/
Source: https://www.nist.gov/artificial-intelligence/artificial-intelligence-safety-institute-consortium-aisic
Source: https://www.epc.eu/publication/The-Paris-Summit-Au-Revoir-global-AI-Safety-61ea68/



One of the more infamous cases brought by the DOJ over the last year was an indictment against Albert Saniger, former CEO and founder of ‚Äúnate,‚Äù for making false claims of integrating AI to raise $40M< from investors. In truth, nate‚Äôs ‚ÄúAI‚Äù capabilities were an outsourced team of Filipino workers.
DOJ has amended its internal guidance: if a committed crime is made worse by AI, the DOJ will request a harsher sentence. 
FTC has brought several actions against AI companies. A number of these involve companies making false claims about their products or services (i.e. Workado, DoNotPay, Cleo AI, and Evolv Technologies). 
SEC, meanwhile, has created a Cyber and Emerging Technologies Unit (CETU), which is a 30-person task force to detect fraud in AI/ML tech companies. 
SEC has been looking closely at AI related disclosures made in public filings and in letters to stakeholders to look out for potential ‚ÄúAI-Washing.‚Äù 
    Federal agencies like the DOJ, FTC, and SEC have largely assumed the AI regulatory oversight role. Their main priority is in making sure that companies do not overstate their AI claims (‚ÄúAI-Washing‚Äù).
stateof.ai 2025
 | 212
The ‚ÄúAI-Washing‚Äù problem: the year in federal enforcement 
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Source: https://www.justice.gov/usao-sdny/pr/tech-ceo-charged-artificial-intelligence-investment-fraud-scheme
Source: https://www.whitecase.com/insight-alert/doj-doubles-down-warnings-against-ai-misuse
Source: https://www.ftc.gov/news-events/news/press-releases/2025/04/ftc-order-requires-workado-back-artificial-intelligence-detection-claims 
Source: https://www.ftc.gov/news-events/news/press-releases/2025/02/ftc-finalizes-order-donotpay-prohibits-deceptive-ai-lawyer-claims-imposes-monetary-relief-requires
Source: https://www.ftc.gov/news-events/news/press-releases/2025/03/cash-advance-company-cleo-ai-agrees-pay-17-million-result-ftc-lawsuit-charging-it-deceives-consumers 
Source: https://www.ftc.gov/news-events/news/press-releases/2024/11/ftc-takes-action-against-evolv-technologies-deceiving-users-about-its-ai-powered-security-screening 
Source: https://www.sec.gov/newsroom/press-releases/2025-42 
Source: https://www.hklaw.com/en/insights/publications/2025/07/sec-and-doj-warm-up-to-enforcement-over-ai-washing
Source: https://www.ftc.gov/news-events/news/press-releases/2024/09/ftc-announces-crackdown-deceptive-ai-claims-schemes


The FTC requested information from Meta, OpenAI, and Google but also from Snapchat, xAI, and Character.AI.
Sen. Hawley (R-MO) said he was going to launch a formal congressional investigation into Meta following the report of ‚Äúsensual‚Äù conversations. Hawley is the Chairman of the Senate Judiciary Committee Subcommittee on Crime and Counterterrorism.
OpenAI, in response to the lawsuit, said that it was going to implement parental controls including sending a notification to parents when their children show signs of ‚Äúdistress.‚Äù 
    In September 2025, the FTC launched an inquiry into how AI chatbots communicate with minors, following reports of ‚Äúsensual‚Äù exchanges on Meta‚Äôs bot and a lawsuit alleging ChatGPT‚Äôs role in a teenager‚Äôs suicide. While not a formal investigation, the proactive move signals regulators‚Äô intent to avoid repeating the mistakes of early social media oversight.
stateof.ai 2025
 | 213
FTC probes AI chatbots‚Äô interactions with children
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Source: https://www.ftc.gov/news-events/news/press-releases/2025/09/ftc-launches-inquiry-ai-chatbots-acting-companions
Source: https://www.nytimes.com/2025/09/11/technology/google-meta-chatgpt-ai-chatbots.html
Source: https://www.reuters.com/investigates/special-report/meta-ai-chatbot-guidelines
Source: https://x.com/HawleyMO/status/1956009097309892709
Source: https://x.com/SenMarkey/status/1965233944515010914
Source: https://www.nytimes.com/2025/09/02/technology/personaltech/chatgpt-parental-controls-openai.html






Reverse acqui-hires surged in 2025, with Google, Microsoft, Amazon, and Meta leading deals (e.g., Microsoft/Inflection, Google/Character.AI, Amazon/Adept, Meta/Scale AI). 
Their typical structure consists of (1) generous founder/staff payouts, (2) IP licensing to make investors whole, (3) a slimmed-down RemainCo shifting to niche B2B.
Inflection AI ‚Üí Microsoft: from $4B AI contender to 12ppl ‚ÄúAI Studio.‚Äù
Adept ‚Üí Amazon: $1B agentic AI startup reduced to a small enterprise AI team.
Scale AI ‚Üí Meta: once 1,400 employees, cut staff/pods, now a single B2B ‚ÄúDemand Generation‚Äù focus.
    Big Tech‚Äôs expected M&A ‚ÄúTrump Bump‚Äù has yet to arrive as antitrust scrutiny under Biden chilled buyers. Instead, Big Tech has embraced reverse acqui-hires: deals that onboard talent in weeks, avoid acquisition rules, and leave hollowed-out ‚ÄúRemainCos‚Äù to pivot into smaller markets.
stateof.ai 2025
 | 214
The reverse acqui-hire: the Valley‚Äôs fast-track exit
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Source: https://cooleyma.com/2025/02/03/cooleys-2024-tech-ma-year-in-reviewwere-so-back-its-so-over/
Source: https://www.fastcompany.com/91384816/what-is-the-reverse-acquihire
Source: https://www.bloomberg.com/news/articles/2025-08-04/what-happens-to-ai-startups-after-big-tech-lures-away-their-founders
Source: https://www.semafor.com/article/08/02/2024/investors-in-adept-ai-will-be-paid-back-after-amazon-hires-startups-top-talent
Source: https://www.reuters.com/technology/artificial-intelligence/google-hires-characterai-cofounders-licenses-its-models-information-reports-2024-08-02/
Source: https://www.reuters.com/technology/microsoft-agreed-pay-inflection-650-mln-while-hiring-its-staff-information-2024-03-21/
Source:https://www.reuters.com/sustainability/boards-policy-regulation/metas-148-billion-scale-ai-deal-latest-test-ai-partnerships-2025-06-13/
Source: https://corpgov.law.harvard.edu/2025/07/02/no-exit/



In July ‚Äò24 the FTC requested details from Amazon on its reverse acqui-hire of Adept after Senators Wyden, Welch and Warren signed a letter urging an investigation.
Meanwhile, the UK‚Äôs own competition authority cleared the Microsoft/Inflection AI deal in ‚Äò24 saying they do not see a ‚Äúrealistic prospect of a substantial lessening of competition.‚Äù 
While no action has been taken, the FTC‚Äôs signals suggest reverse acqui-hires could yet be treated as acquisitions, putting Big Tech‚Äôs favorite exit path under threat.
    Regulators are signaling growing discomfort with reverse acqui-hires, even though they evade formal M&A review. Lina Khan opened an investigation into Microsoft‚Äôs Inflection deal in 2024, and by March 2025 new FTC Chair Andrew Ferguson was demanding more disclosures on Microsoft‚Äôs AI operations.
stateof.ai 2025
 | 215
But the FTC may be cracking down as it pursues investigations
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Source: https://www.bloomberg.com/news/articles/2025-03-12/trump-s-ftc-moves-ahead-with-broad-microsoft-antitrust-probe
Source: https://www.wyden.senate.gov/imo/media/doc/letter_to_ftc___doj_on_ai_competition.pdf
Source: https://assets.publishing.service.gov.uk/media/66d82eaf7a73423428aa2efe/Summary_of_phase_1_decision.pdf
Source: https://www.ftc.gov/system/files/documents/public_statements/1596340/20210915_final_chopra_remarks_non-hsr_reported_acquisitions_by_big_tech_platforms.pdf
Source: https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5316792





    The FTC and DOJ challenged fourteen startup acquisitions between 2020 and 2023 whereas in the seven years prior (2012-2019), the two agencies challenged only three. Both Khan and DOJ Antitrust Chief Jonathan Kanter operated under the theory that Big Tech choked the startup ecosystem by acquiring startups with the intent of killing competition. Silicon Valley scoffed at the theory, but Figma‚Äôs strong IPO may validate the thinking.  

stateof.ai 2025
 | 216
Figma‚Äôs IPO: Lina Khan‚Ä¶vindicated? 
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Adobe sought to acquire Figma for $20B in 2022 but after 15 months of antitrust scrutiny, the two saw no path toward regulatory approval. Figma pursued an IPO on July 31, 2025, instead. At the end of Figma‚Äôs debut, the company was worth close to $57B after the stock tripled during trading hours. Figma‚Äôs stock price has since dropped from that first-day high, but investors still view it as a solid public grade tech company. 
Wiz, too, was considering an IPO before agreeing to an acquisition by Alphabet in the weeks following Trump‚Äôs inauguration. Wiz originally rejected a $23B offer in July ‚Äò24 out of regulatory concerns + IPO dreams. Does Wiz now have seller‚Äôs remorse? 

Source: https://x.com/linamkhan/status/1951361535861043589
Source: https://corpgov.law.harvard.edu/2025/07/02/no-exit/
Source: https://www.semafor.com/article/08/07/2025/youre-welcome-jonathan-kanter-on-figmas-ipo-and-trump-antitrust
Source: https://www.nytimes.com/2024/09/28/business/dealbook/lina-khan-interview.html
Source: https://www.nytimes.com/2025/07/31/technology/figma-ipo.html
Source: https://techcrunch.com/2024/10/23/wiz-hopes-to-hit-1b-in-arr-in-2025-before-an-ipo-after-turning-down-googles-23b/
Source: https://www.cnbc.com/2025/03/24/whats-next-for-ipos-ma-after-googles-32-billion-wiz-acquisition.html

    Alphabet‚Äôs $32B bid for Wiz, announced weeks after Trump‚Äôs inauguration, is the largest AI security deal yet and a live test of whether the DOJ will soften antitrust under new leadership. Google is already in hot water after having lost not one, but two (!) major antitrust suits over the last year. Current DOJ Antitrust Chief Abigail Slater is far from a friend for the Valley but she could face pressure to approve the mega-purchase. 
stateof.ai 2025
 | 217
Wiz/Alphabet: Big Tech‚Äôs Ultimate M&A Test
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
DOJ Antitrust Chief Slater during the Google antitrust case: ‚ÄúIn a time of political division in our nation, the case against Google brings everyone together.‚Äù 
The DOJ in July, however, approved three mergers worth $63B in just one week and there are reports that Slater is being pressured to be more ‚Äúdeal-friendly‚Äù towards incoming transactions and acquisitions. 
But the timing of Alphabet‚Äôs largest acquisition could not be worst as the company is attempting a vertical merger, a deal more prone to regulatory disapproval, when it is under intense federal scrutiny. Alphabet agreed to pay $3.2B (10% of the deal) to Wiz if the transaction is not approved - not exactly a sign of diminished confidence. The deal, if it goes through, would essentially be a greenlight for tech to return to buying startups through traditional acquisitions. 
Source: https://www.cnbc.com/2025/03/18/google-wiz-deal-litmus-test-for-trump-admin-handling-of-big-tech.html
Source: https://www.politico.com/news/magazine/2025/05/09/gail-slater-donald-trump-antitrust-00277348
Source: https://www.reuters.com/legal/litigation/trumps-antitrust-enforcers-get-out-way-multibillion-dollar-deals-2025-07-10/
Source: https://www.reuters.com/sustainability/boards-policy-regulation/antitrust-enforcement-is-not-regulation-doj-official-says-2025-04-28/
Source: https://www.cbsnews.com/news/top-doj-antitrust-officials-fired-as-tension-grows-in-a-trump-administration-monopoly-fighting-office/
Source: https://x.com/AAGSlater/status/1907523318753743232





Google must now share search index data with competitors like DuckDuckGo and can no longer make exclusive contracts with other companies to feature Google Search and other products. Google can still pay partners to feature Google search and other apps. So while Google paid Apple $20B for exclusive use of Google search on Safari, that deal can remain so long as the terms are non-exclusive. 
These are light-touch compared to the ‚Äústructural remedies‚Äù the USG proposed, which included forcing Google to sell its Chrome browser. 
Google will likely appeal and fight against the initial ‚Äúmonopoly‚Äù branding, but there‚Äôs also the ‚Äúad tech‚Äù case where, experts caution, Google is more likely to be forced to part with core parts of its ad business.  
    The Silicon Valley darling has been bruised, battered, and, ultimately, trust-busted over the year in two landmark antitrust case rulings. Judges in two separate cases ruled that Google was operating a monopoly over 1) search 2) ad tech. Remedies were issued early Sept. ‚Äò25 in the search monopoly case where the court, pointing to the vulnerability of Google‚Äôs search business to LLM chatbots, largely agreed with Google‚Äôs proposed remedies. It was a big win for Google given the heat the company is taking from ChatGPT and other agents that disintermediate Google.com 
stateof.ai 2025
 | 218
Google comes out unscathed in 1 of 2 antitrust cases but search is on the ropes‚Ä¶   
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Source: https://www.theinformation.com/articles/google-preparing-make-ad-tech-unit-independent
Source: https://www.linkedin.com/in/marklemley/
Source: https://x.com/AAGSlater/status/1963001542006436241
Source:https://www.theinformation.com/articles/antitrust-judge-bars-google-exclusive-deals-require-chrome-sale?offer=rtsu-engagement-25%2Crtsu-featured-articles-pro&utm_campaign=RTSU%3A+Antitrust+Judg&utm_content=9905&utm_medium=email&utm_source=cio&utm_term=6693
Source: https://ecf.dcd.uscourts.gov/cgi-bin/show_public_doc?2020cv3010-1436



    Pressure has mounted for the EU to abandon its landmark AI Act, but the Commission has trudged on with the regulation using the following phased compliance schedule:  
stateof.ai 2025
 | 219
Meanwhile in Europe‚Ä¶
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
August 1, 2024
The AI Act‚Äôs compliance countdown officially begins; law is on the books (i.e. the Official Journal of the European Union) but its key obligations are not yet in effect. 
February 2, 2025
The first major obligation goes into effect: a ban on ‚Äúunacceptable‚Äù AI risk systems (e.g. social scoring, biometric identification, facial recognition)
August 2, 2025
The voluntary Code of Practice for GPAI is released; firms that choose to follow the Code of Practice have presumed compliance with the AI Act. Those that don‚Äôt are subject to the AI Act‚Äôs Chapter V rules for GPAI models. 
August 2, 2026
Remaining obligations for AI systems (except for ‚Äúhigh-risk‚Äù AI systems) goes into effect. 
August 2, 2027
Obligations for ‚Äúhigh-risk‚Äù AI systems go into effect. 
Source: https://artificialintelligenceact.eu/implementation-timeline/



    On August 2, 2025, the GPAI Codes of Practice went into effect after a three month delay. The ‚ÄúCodes‚Äù are one of the first major steps of the AI Act‚Äôs implementation. For providers of general-purpose AI (GPAI) models, the AI Act requires companies to develop frameworks to show how they would fulfill requirements concerning 1) transparency 2) copyright and 3) safety and security (this category only applies to frontier GPAI models posing systemic risk). The ‚ÄúCodes‚Äù are a voluntary framework, an option for companies that would rather not develop their own guidance to fulfill the stated GPAI obligations, which began August 2, 2025. EU enforcement of the obligations, however, does not begin until August ‚Äò26. The AI Act also has a 2-year grace period for models released before August 2, 2025.  




stateof.ai 2025
 | 220
The GPAI Codes of Practice‚Ä¶and so it begins.  
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Amazon, Anthropic, OpenAI, Microsoft, Google, and others have signed all three chapters of the ‚ÄúCodes.‚Äù
xAI has agreed to sign the third chapter (‚ÄúSafety and Security‚Äù) but has not signed the chapters on ‚ÄúTransparency‚Äù and ‚ÄúCopyright,‚Äù which makes the company responsible for developing its own policy that meets the AI Act‚Äôs requirements for transparency and copyright compliance. 
Meta has refused to sign the Code of Practice saying, ‚ÄúThis Code introduces a number of legal uncertainties for model developers, as well as measures which go far beyond the scope of the AI Act.‚Äù 
Source: https://digital-strategy.ec.europa.eu/en/policies/contents-code-gpai
Source: https://artificialintelligenceact.eu/implementation-timeline/
Source: https://techcrunch.com/2025/07/18/meta-refuses-to-sign-eus-ai-code-of-practice/
Source: https://www.techpolicy.press/how-the-eus-voluntary-ai-code-is-testing-industry-and-regulators-alike/


    Needless to say, companies, both in the EU and abroad, are not happy with the AI Act‚Äôs rollout. The EU‚Äôs delayed implementation is not exactly inspiring confidence. Each member state was required to assign national authorities to oversee the AI Act‚Äôs implementation but so far only 3 member states have fully completed the requirement. The AI Act also called for the creation of technical standards by April ‚Äò25 to address the ‚Äúhow‚Äù of compliance. But as of this writing, those standards are still in development. A coalition of EU AI companies signed a letter in July calling for a 2-year ‚Äústop clock‚Äù on the AI Act with Sweden‚Äôs PM publicly calling the AI Act ‚Äúconfusing‚Äù and President Macron saying ‚Äúwe are over regulating.‚Äù But the show goes on‚Ä¶for now.
stateof.ai 2025
 | 221
So, who‚Äôs afraid of the AI Act? üòìüòì
In Sept., the EC opened a ‚Äúdigital simplification omnibus,‚Äù a commission that would look and review all of the EU‚Äôs digital laws, including the AI Act, to simplify its digital rules. In general, the EC announced that it wants to ‚Äúreduce administrative burdens‚Äù by 25% across the regulatory board.  
The calls for a ‚Äúpause‚Äù on the AI Act grow louder by the day, however. Despite the pressure, the EC has said that it would not delay implementation deadlines. 
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Source: https://aichampions.eu/#stoptheclock
Source: https://x.com/BertuzLuca/status/1967892064584294472?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Etweet
Source: https://www.mlex.com/mlex/artificial-intelligence/articles/2393367/eu-countries-divided-over-potential-ai-act-pause
Source:https://artificialintelligenceact.eu/national-implementation-plans
Source: https://www.fintechweekly.com/magazine/articles/eu-ai-act-startups-call-pause?
Source: https://www.politico.eu/article/how-eu-did-full-180-artificial-intelligence-rules/
Source: https://iapp.org/news/a/european-commission-outlines-digital-simplification-plans-at-libe-meeting
Source: https://commission.europa.eu/news-and-media/news/commission-proposes-cut-red-tape-and-simplify-business-environment-2025-02-26_en
Source: https://www.euronews.com/my-europe/2025/09/16/draghi-calls-for-pause-to-ai-act-to-gauge-risks

For image: 
Source: https://x.com/vonderleyen/status/1733257654254883095
Source: https://www.politico.eu/article/eu-prepares-ground-pause-artificial-intelligence-rules/
Source: https://iapp.org/news/a/navigate-2025-potential-eu-ai-act-pause-opens-new-questions-on-approach-to-global-regulation
Source: https://www.theguardian.com/commentisfree/article/2024/jul/17/ursula-von-der-leyen-europe-second-term
Source: https://www.theguardian.com/world/article/2024/jul/18/ursula-von-der-leyen-wins-second-term-european-commission-president






    Europe is trying to shift from rulemaking to capacity-building, but the gap keeps widening. Over 50 years, the region minted no tech firm above $400B in value, while the U.S. now has seven at $1T+. In 2024, U.S. labs shipped ~40 major models, China ~15, and the EU ~3. Brussels is setting aside billions to amplify its spending, but is that the scale or speed the problem demands?
stateof.ai 2025
 | 222
Is the EU doing enough to catch up in the AI race? 
In 2024, Mario Draghi, the former European Central Bank President, issued a report on ‚ÄúEuropean Competitiveness,‚Äù whose recommendations the EU agreed to implement. Draghi pointed to multiple structural challenges including market fragmentation, regulatory barriers, demographic decline, low productivity, and risk-averse capital deployment as reasons for EU‚Äôs stagnation. 
Draghi put the cost of his modern-day Marshall Plan at ‚Ç¨800B per year (!). While the EU has increased spending, most notably launching InvestAI (an >‚Ç¨200B AI investment fund), it is not taking that price tag seriously. One estimate said that only about 11% of Draghi‚Äôs recommendations were seen through so far‚Ä¶
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
*Chart shows private investment only. Including China's estimated $56B in government AI funding (2025), total Chinese investment significantly exceeds Europe's.
Source: https://commission.europa.eu/topics/eu-competitiveness/draghi-report_en
Source:https://x.com/HennaVirkkunen/status/1866463207680065705
Source: https://hai.stanford.edu/ai-index/2025-ai-index-report/economy (for private investment chart) 
Source: https://www.wsj.com/world/europe/europe-is-losing-fe179376 (for GDP per capita chart)
Source: https://hai.stanford.edu/ai-index/2025-ai-index-report (for 40:15:3 numbers)
Source: https://www.politico.eu/article/mario-draghi-report-says-eu-must-spend-twice-as-much-after-wwii/
Source: https://luxembourg.representation.ec.europa.eu/actualites-et-evenements/actualites/eu-launches-investai-initiative-mobilise-eu200-billion-investment-artificial-intelligence-2025-02-11_en
Source: https://www.politico.eu/article/mario-draghi-competitiveness-economy/
Source: https://ec.europa.eu/commission/presscorner/api/files/attachment/881649/Factsheet%20-%20One%20year%20of%20The%20Draghi%20Report.pdf
Source: https://www.atlanticcouncil.org/blogs/new-atlanticist/draghis-new-report-on-european-competitiveness/









The country‚Äôs strategic tone now mirrors the US: pro-adoption, state-backed capacity, rebranded ‚ÄúAI safety‚Äù bodies focused on capability evals and deployment, and delaying safety legislation to an undetermined date. 
Announced actions include increasing compute 20x by 2030 and growth zones for data centers with streamlined planning. 
Starmer has committed to adopting 50 recommendations from ex-AI adviser Matt Clifford and to avoid new rules that could slow deployment. But government bureaucracy has made operationalizing these large-scale ambitions a slog.   
Ministers claim AGI is around the corner but compute investments remain well below that of the US while AISI relies on voluntary agreements with labs to test models pre-deployment. 
    Since January, the UK has shifted from convening global AI safety to an industrial push. The AI Opportunities Action Plan prioritizes investment, data-centre capacity, and light-touch rules, with designated ‚Äúgrowth zones‚Äù to fast-track permits. The government frames this as a route to a potential ~$740B GDP boost by 2035.
stateof.ai 2025
 | 223
Cheerio, Bletchley, and ‚Äòello ‚Äúgrowth zones!‚Äù: Starmer to ‚Äúmainline AI into the UK‚Äôs veins‚Äù
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Source:https://www.gov.uk/government/publications/ai-opportunities-action-plan-government-response/ai-opportunities-action-plan-government-response
Source:https://www.gov.uk/government/news/prime-minister-sets-out-blueprint-to-turbocharge-ai
Source: https://www.reuters.com/world/uk/uk-pm-starmer-outline-plan-make-britain-world-leader-ai-2025-01-12/
Source: https://ukstories.microsoft.com/features/ai-could-boost-uk-gdp-by-550-billion-by-2035-research-shows/
Source: https://ukstories.microsoft.com/features/ai-could-boost-uk-gdp-by-550-billion-by-2035-research-shows/
Source: https://onu.delegfrance.org/statement-on-inclusive-and-sustainable-artificial-intelligence-for-people-and
Source: https://metro.co.uk/2025/01/13/map-shows-ai-hotspots-will-bid-make-uk-a-superpower-22349991/ (for photo of data centers)
Source: https://www.theguardian.com/politics/2025/jan/12/mainlined-into-uks-veins-labour-announces-huge-public-rollout-of-ai 



    Three days after Trump announced the AI Action Plan, China released their own Global AI Governance Action Plan. Coincidence? We think not. China‚Äôs deliberately contrasting ambitions, which are no less zealous, emphasize strategies of ‚Äúmultilateral collaboration‚Äù and ‚Äúdiplomatic engagement.‚Äù In practice, these buzzwords denote China‚Äôs plan to supply the Global South with its own AI solutions and to draw on its influence among developing countries, especially in Africa, to impact the UN and other international bodies. 
China has proposed setting up a new World AI Cooperation Organization and is working with the UN‚Äôs Pact for the Future and Global Digital Compact to build out processes for AI development and governance. 
China appears to be applying the US TikTok playbook with authorities first cautioning and then instructing companies to stop buying NVIDIA chips for fear of the US threatening the country‚Äôs national security. 
The Belt and Road Initiative, the Global Development Initiative, and the Global Security Initiative continue to be key components of China‚Äôs global digital investment strategy. 
stateof.ai 2025
 | 224
The CCP‚Äôs Action Plan: an AI World Tour 

 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Source: https://greenfdc.org/countries-of-the-belt-and-road-initiative-bri/ (for map of Belt and Road Initiative) 
Source: https://www.gov.cn/yaowen/liebiao/202507/content_7033929.htm
Source: https://www.atlanticcouncil.org/blogs/new-atlanticist/reading-between-the-lines-of-the-dueling-us-and-chinese-ai-action-plans/
Source: https://www.cnbc.com/2025/07/26/china-ai-action-plan.html
Source: https://www.techpolicy.press/unpacking-chinas-global-ai-governance-plan/
Source: https://www.ft.com/content/02537db9-8687-48eb-94c8-383f8332b5d6
Source: https://www.semafor.com/semafor-view-2025/china
Source: https://www.reuters.com/world/china/china-cautions-tech-firms-over-nvidia-h20-ai-chip-purchases-sources-say-2025-08-12/
Source: https://www.scmp.com/news/china/diplomacy/article/3324783/chinas-xi-jinping-issues-rallying-call-against-protectionism-brics-address
Source: https://www.nytimes.com/interactive/projects/cp/reporters-notebook/xi-jinping-visit/china-surprisesu-n-with-100-million-and-thousands-of-troops-for-peacekeeping
Source: https://www.reuters.com/markets/emerging/china-tells-tech-firms-stop-buying-nvidias-ai-chips-ft-reports-2025-09-17/ 

    The world‚Äôs most active AI regulator continues to issue AI standards and regulations. Here are a few of the highlights of China‚Äôs most significant regulations from the past year: 

Administrative Measures for the labeling of AI-generated content: In effect September 1, 2025, this obligates content service providers to clearly label AI-generated content. Chatbots, AI-generated writing and video creation all require customer-facing labels denoting the AI as such. Some AI-generated content, however, may only require hidden labeling within the metadata. 
Three National Cybersecurity Standards: In April 2025, the State Administration for Market Regulation and the Standardization Administration of China released three separated standards outlining security requirements for datasets, data-labeling, and, in general, generative AI services. The requirements take effect November 2025. 
The AI-Plus Plan: The State Council of China released a set of industrial policy goals that all aim to have AI capabilities fully integrated across the entire Chinese economy with complete AI penetration across all Chinese sectors by 2035. 
stateof.ai 2025
 | 225
China‚Äôs AI Regulations also begin to take effect 
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Source: https://www.whitecase.com/insight-our-thinking/ai-watch-global-regulatory-tracker-china
Source: https://iapp.org/news/a/china-releases-ai-plus-plan-rolls-out-ai-labeling-law





    China‚Äôs strategy for AI self-reliance gained ground over the last year with news-making achievements from open source leaders like DeepSeek, MoonShot AI, and others. During an April 2025 Politburo meeting, President Xi Jinping signaled all hands were on deck and told ministers to ‚Äúredouble our efforts‚Äù on AI. This shows China‚Äôs intent on achieving self-sufficiency, a perennial aim that always intensifies whenever it enters a trade war with the US. But mounting debt levels may pose problems down the line.    
Rising debt among countries is not new, but the problem is pronounced in China where the country accounts for ‚Äúmore than half of the increase in the global economy‚Äôs debt-to-GDP ratio since 2008.‚Äù IMF in a 2025 report suggested it‚Äôs unstable. 
This is largely due to national ambitions that push for high annual GDP growth, forcing local governments to invest in dubious assets.   
But AI spending is not slowing down with the CCP allocating a 10% increase in science and tech spending from 2024. Winning the AI race, then, isn‚Äôt just a political imperative but potentially vital to the long-term health of the Chinese economy. 
stateof.ai 2025
 | 226
China aims to achieve tech self-reliance‚Ä¶no matter the ¬•¬•¬•¬•
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Source: https://www.anderson.ucla.edu/about/centers/ucla-anderson-forecast/projects-and-partnerships/cathay-bank/2025-us-china-annual-economic-report (for debt:GDP chart) 
Source https://www.reuters.com/world/china/chinas-xi-calls-self-sufficiency-ai-development-amid-us-rivalry-2025-04-26
Source: https://www.imf.org/en/Publications/CR/Issues/2025/04/30/Peoples-Republic-of-China-Financial-Sector-Assessment-Program-Financial-System-Stability-566570
Source: https://merics.org/en/report/chinas-drive-toward-self-reliance-artificial-intelligence-chips-large-language-models (for China AI tech ecosystem image)







In May 2025, Trump visited the Gulf with over 60 US executives including Jensen Huang, Sam Altman, and Larry Fink to launch the US-UAE AI Acceleration Partnership: a 10-year $1.4T investment by the UAE into US AI, chips, energy and infrastructure. G42 will build a 5GW AI cluster (i.e. Stargate UAE) across 10 square miles. 
The UAE will import up to 500k Blackwell GPUs worth $15B through 2027. G42 will get 20% of the chips and 80% will go to the regional data centers US tech giants with each watching out for unintended Chinese usage. 
In Saudi Arabia, the US agreed deals worth $600B: $142B of US defense equipment sales, $20B for US AI data centers, and $80B of investments into the region from Google, DataVolt, Oracle, Salesforce, AMD and Uber. 
Beyond domestic projects, UAE sovereign funds pledged to finance a 1GW AI datacenter in France (costing ‚Ç¨30-50B).
    UAE and Saudi Arabia are leveraging massive compute build-outs, chip import deals, and huge US trade and investment partnerships to position the Gulf as a central node in the global AI balance.
stateof.ai 2025
 | 227
The Gulf enters the AI power game with a trillion-dollar bets
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Source: https://ae.usembassy.gov/uae-and-us-presidents-attend-the-unveiling-of-phase-1-of-new-5gw-ai-campus-in-abu-dhabi/ 
Source: https://openai.com/index/introducing-stargate-uae/ 
Source: https://www.reuters.com/business/media-telecom/stargate-uae-ai-datacenter-begin-operation-2026-2025-05-22/
Source: https://www.reuters.com/world/after-trump-meeting-uae-commits-10-year-14-trillion-investment-framework-us-2025-03-21/
Source: https://www.reuters.com/business/finance/us-close-letting-uae-import-millions-nvidias-ai-chips-sources-say-2025-05-14/ 
Source: https://www.reuters.com/technology/artificial-intelligence/france-uae-agree-develop-1-gigawatt-ai-data-centre-2025-02-06 
Source: https://edition.cnn.com/2025/05/14/tech/us-ai-chips-trump-saudi-arabia-visit-intl-hnk 
Source: https://www.whitehouse.gov/fact-sheets/2025/05/fact-sheet-president-donald-j-trump-secures-historic-600-billion-investment-commitment-in-saudi-arabia/ 

    Over 30 Latin American institutions are training a 50B open model on data from 20 countries and Spain to reflect local dialects and norms. Chile leads the effort with support from public agencies, universities, and AWS. The budget is about $3.5M and the release is planned for December. The question is whether a modest, regional model can compete where ChatGPT and Claude already have strong adoption.
stateof.ai 2025
 | 228
Brazil ranks third globally for both ChatGPT and Claude usage. Indeed, reported AI adoption across LatAm was about 40% last year Brazil (India was the highest at 59%).
OpenAI opened its first LatAm office in S√£o Paulo in August 2025, signalling rising regional demand for frontier systems.
LatAm-GPT‚Äôs value case is localization and capacity building. The scale and funding are small, so impact may stay academic unless governments and industry adopt it widely.
While Latin America tries to carve its own path with LatAm-GPT
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Source:https://www.latamgpt.org/en
Source: https://www.claude.com/customers/zapia (for Zapia quote)
Source: https://www.cepal.org/en/pressreleases/latin-american-artificial-intelligence-index-ilia-reconfirms-chile-brazil-and-uruguay
Source: https://www.wired.com/story/latam-gpt-the-free-open-source-and-collaborative-ai-of-latin-america/
Source: https://www.hks.harvard.edu/centers/carr-ryan/our-work/carr-ryan-commentary/perus-ai-regulatory-boom-quantity-without-depth
Source: https://www.leonardi.adv.br/en/insights/brazil-and-chile-establish-strategic-partnership-in-artificial-intelligence/
Source: https://valorinternational.globo.com/business/news/2025/08/29/openai-opens-first-latin-american-office-in-brazil.ghtml
Source: https://www.anthropic.com/research/anthropic-economic-index-september-2025-report
Source: https://www.upi.com/Top_News/World-News/2025/08/27/chile-chile-latamGPT-regional-languages-launch/9291756313867/
Source: https://www.forbes.com/advisor/business/ai-statistics/
Source: https://hispanicexecutive.com/ai-adoption-in-latin-america-how-the-region-sets-its-own-terms/
Source: https://borgenproject.org/digital-divide/















    2025 marked a decisive step change in how the US and its allies procure and deploy AI in defense. Rather than fragmented pilot projects, defense leaders consolidated billions into enterprise-scale AI platforms while also opening the door to frontier model providers. NATO fast-tracked its first alliance-wide AI system, Palantir‚Äôs Maven Smart System, as a central pillar of the Western defense industrial base.
stateof.ai 2025
 | 229

US defense sizes up its bet on AI-first systems and opens to up to frontier AI labs
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
US Army signed a 10-year enterprise contract with Palantir worth up to $10B, consolidating dozens of prior software deals into one platform.
Project Maven expanded to $1.3B through 2029, now supporting 20,000+ individual warfighters, double the base earlier in the year.
NATO procured Palantir‚Äôs Maven Smart System in just six months for all members despite feeling frosty with loosening US security guarantees. 
DoD signed $200M ceiling contracts with each of OpenAI, Anthropic, Google, and xAI to explore frontier AI for command, cyber, and planning missions. This marked quite a vibe shift for AI groups that previously took a strong view that their models should not be used for defense purposes. 
Source: https://shape.nato.int/news-releases/nato-acquires-aienabled-warfighting-system-
Source: https://www.reuters.com/business/us-army-pools-contracts-into-up-10-billion-palantir-deal-2025-07-31/
Source: https://defensescoop.com/2025/05/23/dod-palantir-maven-smart-system-contract-increase/
Source: https://defensescoop.com/2025/04/14/nato-palantir-maven-smart-system-contract/
Source: https://www.ai.mil/Latest/News-Press/PR-View/Article/4242822/cdao-announces-partnerships-with-frontier-ai-companies-to-address-national-secu/ 
Source: https://breakingdefense.com/2025/07/anthropic-google-and-xai-win-200m-each-from-pentagon-ai-chief-for-agentic-ai/ 
Source: https://x.com/liminal_bardo/status/1945080817857749376 



stateof.ai 2025
 | 230
OpenAI and Anthropic make a push to land grab US Government workflows
Procurement has always been the bane of new technology‚Äôs adoption in government and other highly-regulated industries. In the US, the General Services Administration (GSA) announced their OneGov Strategy to modernize procurement of goods and services in April. 
Most notably, the strategy has led to the rapid deployment availability of frontier models for government workers by AI labs. It also provides for an AWS OneGov agreement with up to $1B in credits for cloud/AI modernization.
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
    The two companies are deploying a US Government access program in a bid to upskill operations and win the hearts and minds of the administration. OpenAI and Anthropic are using their GSA OneGov partnership to extend ChatGPT Enterprise for $1 per federal agency and Claude to all three branches (federal civilian executive, legislative, and judiciary) for $1 too. 


Source: https://www.gsa.gov/about-us/newsroom/news-releases/gsa-unveils-onegov-strategy-04292025
Source (AWS deal): https://www.gsa.gov/about-us/newsroom/news-releases/gsa-announces-onegov-agreement-with-aws-08072025 
Source (ChatGPT deal): https://www.gsa.gov/about-us/newsroom/news-releases/gsa-announces-new-partnership-with-openai-delivering-deep-discount-to-chatgpt-08062025
Source (Anthropic deal): https://www.gsa.gov/about-us/newsroom/news-releases/gsa-strikes-onegov-deal-with-anthropic-08122025 

    The U.S. has moved from DARPA‚Äôs AlphaDogfight in 2020 and live AI-flown F-16 tests in 2022 to embedding autonomy into doctrine. Collaborative drones, swarming initiatives, and multi-domain contracts are now framed as essential to offset China‚Äôs numerical advantage, making uncrewed systems a core pillar of force design.
stateof.ai 2025
 | 231
Autonomy takes flight with drone wingmen entering the doctrine
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Air Force‚Äôs Collaborative Combat Aircraft (CCA) first prototypes (General Atomics‚Äô YFQ-42A and Anduril‚Äôs YFQ-44A took flight in 2025, with 1,000+ drones planned at ~$25-$30M each. FY25/26 budgets total $4B+ for Next Generation Air Dominance and CCAs.
The Replicator initiative targets thousands of cheap autonomous systems across air, land, sea within 24 months, backed by $1.8B FY24 funding as a direct hedge against Chinese mass. 
Anduril scored major autonomy wins, including $250M for Roadrunner/Pulsar cUAS, $200M Marine cUAS contract, and an $86M SOCOM deal for autonomy software. The company is expanding its maritime autonomy XL UUV and hypersonic missile work. 
Saronic too has a $392M OTA with the US Navy for autonomous boats. 
Source: https://defensescoop.com/2024/04/17/darpa-ace-ai-dogfighting-flight-tests-f16/ 
Source: https://www.congress.gov/crs_external_products/IF/PDF/IF12740/IF12740.3.pdf 
Source: https://defensescoop.com/2025/06/10/dod-2026-budget-request-f47-cca-hegseth/ 
Source: https://www.businessinsider.com/air-force-first-uncrewed-fighter-jets-photos-2025-3 
Source: https://en.wikipedia.org/wiki/Next_Generation_Air_Dominance 
Source: https://en.wikipedia.org/wiki/Manned-unmanned_teaming 
Source: https://www.nationaldefensemagazine.org/articles/2024/12/10/air-force-mulls-choice-between-affordability-capability-for-collaborative-combat-aircraft 
Source: https://www.defensenews.com/unmanned/2024/10/08/anduril-lands-250-million-pentagon-contract-for-drone-defense-system/ 
Source: https://www.defensemirror.com/news/37870/Anduril_Industries_Wins__250M_for_Roadrunner__Pulsar_Anti_Drone_Systems 
Source: https://www.edrmagazine.eu/anduril-wins-640-million-i-csuas-contract 
Source: https://defensescoop.com/2025/03/26/anduril-socom-contract-award-autonomy-software-86m/ 
Source: https://defensescoop.com/2025/08/22/navy-buy-saronic-autonomous-maritime-drones-usv-asv-ota/ 

EU‚Äôs Readiness 2030 (fka ReArm Europe) authorizes up to ‚Ç¨650B extra defense spend, naming AI, drones, and counter-drone as critical gaps. A new SAFE fund will pool EU money to de-risk cross-border projects in autonomy, cyber, and electronic warfare. Critics warn hardware may only arrive after 2030 unless AI is prioritized as a force multiplier now.
The UK‚Äôs Strategic Defence Review 2025 makes AI and autonomy a priority, citing Ukraine where ‚Äúdrones now kill more people than artillery.‚Äù A new Defence Uncrewed Systems Centre and AI Investment Fund are planned by 2026.
Helsing raised ‚Ç¨600M (valued ~$12B), debuted autonomous strike drones and tested an AI-piloted Saab jet. Unicorn drone makers Quantum Systems and Tekever raised ‚Ç¨160M and ‚Ç¨70M, respectively. Meanwhile, Rheinmetall‚Äôs market cap has soared past ‚Ç¨80B (larger than VW). 
    Russia‚Äôs full scale war in Ukraine and wavering US signals at the Munich Security Conference in Feb 2025 jolted Europe into treating AI as a frontline capability. Capitals are writing autonomy into plans, Brussels is mobilizing massive rearmament funds, and both startups and primes are surging to build sovereign AI defense.
stateof.ai 2025
 | 232
Europe wakes up to AI warfare and with shaky US security guarantees
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Source: https://www.gov.uk/government/publications/the-strategic-defence-review-2025-making-britain-safer-secure-at-home-strong-abroad
Source: https://assets.publishing.service.gov.uk/media/683d89f181deb72cce2680a5/The_Strategic_Defence_Review_2025_-_Making_Britain_Safer_-_secure_at_home__strong_abroad.pdf 
Source: https://defence-industry-space.ec.europa.eu/eu-defence-industry/introducing-white-paper-european-defence-and-rearm-europe-plan-readiness-2030_en 
Source: https://commission.europa.eu/document/download/e6d5db69-e0ab-4bec-9dc0-3867b4373019_en
Source: https://www.belfercenter.org/quick-take/msc2025 
Source: https://www.reuters.com/business/aerospace-defense/german-defence-start-up-helsing-raises-600-million-euros-latest-investment-round-2025-06-17/
Source: https://quantum-systems.com/news/quantum-systems-raises-euro160m/ 
Source: https://www.tekever.com/news/tekever-confirmed-as-europes-newest-unicorn-as-it-invests-400m-in-the-uk-to-drive-ai-driven-defence/ 
Source: https://arx-robotics.com/press-release-arx-raises-e31m-in-series-a-to-scale-autonomous-robotics/ 
Source: https://stockanalysis.com/quote/fra/RHM/market-cap/ (retrieved 3 sept 2025)
Source: https://www.anduril.com/article/ghost-shark-enters-program-of-record-from-prototype-to-fleet-in-three-years/ 

 | 233
GDPval: a warning shot for the job market
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Reasoning models outperformed GPT-4o on task win-rate by an average margin of 20.7% across 44 categories of professional work.
Claude, which has not historically dominated other benchmarks, achieved the highest* win rates in 32 of 44 professions. The paper attributes part of this success to Claude's strengths in formatting. 
General-purpose models are already demonstrating strong competence as professional assistants. Meanwhile, frontier labs and recent entrants General Reasoning and Mechanize are also rapidly building RL environments on real-world work scenarios. 
With this hill to climb and an influx of corporate data and demos, knowledge workers may soon experience the workplace transformations that AI leaders have long predicted. The Lufthansa Group even said it expects to cut 4k administrative jobs by 2030.
*Or tied for the highest win-rate
    OpenAI's new benchmark for economically valuable tasks, GDPval, demonstrates the steady march of AI progress. Across 44 professions and 1,320 tasks, models are approaching human experts in a significant subset of domains. 
stateof.ai 2025
Source:https://openai.com/index/gdpval/
Source: https://www.forbes.com/sites/marisagarcia/2025/09/30/lufthansa-bets-big-on-ai-to-cut-4000-jobs/


 | 234
GDPval: long live the accountants!
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
    The tracker below ranks the exposure to disruption of different professions based on GDPval‚Äôs results. 
*Average GDPval Win Rate covers the reported scores of each reasoning model and therefore excludes GPT-4o. 
stateof.ai 2025
Source:https://openai.com/index/gdpval/

Meanwhile, law school applications spiked 21% in 2024, suggesting graduates are hedging against uncertain career prospects. 
Jobs for more experienced workers has remained stable/grown, even in highly AI-exposed domains. This suggests workers who have acquired more tacit knowledge are more likely to be augmented by modern AI models. But without on-the-job experience, workers will struggle to gain tacit knowledge. 
    Entry-level hiring is declining across software and customer support - roles that are highly exposed to AI automation. These trends appear to be independent of macro factors like inflation or pandemic recovery. 
stateof.ai 2025
 | 235
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
-11%
-21%
Although total employment has grown, the hiring of younger workers has stagnated since late 2022. Despite strong AI adoption, this group struggles to find a foothold in the job market, On this trajectory, AI fluency may not guarantee favorable economic outcomes. 

AI squeezes the entry-level job market while experienced workers are safe...for now
Source: https://digitaleconomy.stanford.edu/wp-content/uploads/2025/08/Canaries_BrynjolfssonChandarChen.pdf
Source:https://www.wsj.com/economy/jobs/ai-entry-level-job-impact-5c687c84?gaa_at=eafs&gaa_n=ASWzDAgKJzq2bJBlXZcaEKoSRXKCljxS8QbehYPVh4IlA169Hj4cNAp6I1VwebpUJo8%3D&gaa_ts=68c30589&gaa_sig=qF-5OCOVfloWf5eYVnwh5ghrkLJdr7hvQ2h_7B2s8Jx3w0hcQ2smWNgQ2pPnR1uFQcQ_d_ax3ofmuc0YzEkMHQ%3D%3D
Source: https://nationaljurist.com/law-school-applications-surge-21-for-2025-academic-year/
Source: https://www.businessinsider.com/anthropic-ceo-ai-cut-entry-level-law-finance-consulting-jobs-2025-9
Source: https://fortune.com/2025/09/04/gen-z-unemployment-rate-recent-college-gradutes-bank-of-america/
Source: https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5425555
Source: https://libertystreeteconomics.newyorkfed.org/2025/09/are-businesses-scaling-back-hiring-due-to-ai/
Source:https://www.linkedin.com/posts/emollick_the-fact-that-junior-hiring-in-ai-intensive-activity-7368316608500359168-8_zd/?utm_source=share&utm_medium=member_desktop&rcm=ACoAACIvrtYBChz67M_GPwAloQzDpXSUulqNxpc



stateof.ai 2025
 | 236
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
They focus on ‚Äúoccupational mix,‚Äù a macro measurement of job movement among workers, i.e. switching/starting/losing a job.
Currently, the ‚Äúoccupational mix‚Äù for AI is tracking with that of other tech breakthroughs like the introduction of the internet and computers. If that trend continues, an AI-induced labor disruption will take decades to materialize, not months.
The study‚Äôs results, however, do not contradict the sector-specific Stanford study (see previous slide). There is an uptick in ‚Äúoccupational mix dissimilarity‚Äù among new grads in recent months but the dissimilarity tracks with pre-2022 trends, suggesting non-AI factors.  

While some argue AI is not shaking up the labor market yet
    A joint study from the Yale Budget Lab and Brooking Institution found that current labor market changes precede the introduction of ChatGPT in 2022. The authors conclude that there‚Äôs little reason to think that ‚ÄúAI automation is currently eroding the demand for cognitive labor across the economy‚Äù and caution against predicting job losses based on ‚ÄúAI-exposure‚Äù data alone. 
Source:https://budgetlab.yale.edu/research/evaluating-impact-ai-labor-market-current-state-affairs?j=60674&sfmc_sub=15818665&l=1584_HTML&u=3905313&mid=546014653&jb=44&utm_source=sfmc&utm_medium=email&utm_campaign=NL_eye-on-ai_2025-10-2_60674&sfmc_id=15818665




stateof.ai 2025
 | 237
AI and jobs: Nobody knows who will lose theirs, but the labs are keeping score
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
    Both Anthropic and OpenAI released data on how its users were using their respective models. Use cases varied across country and US state. For instance, California users were the most likely to use AI for coding while DC usage centered around job search activities and writing projects. For work-oriented tasks, ChatGPT was often used for writing-related tasks while Claude was often used for coding tasks. As a result, the conclusions drawn from each of the studies for the future of work were different. OpenAI argued that its data shows AI being used mainly to augment work-related functions and offer ‚Äúdecision support‚Äù while Anthropic argued that enterprises, specifically, are more likely to automate tasks with automation increasing across its customer enterprises.  

Source: https://www.semafor.com/article/09/17/2025/openai-anthropic-usage-data-shows-economic-divide
Source:  https://openai.com/index/how-people-are-using-chatgpt/
Source: https://www.anthropic.com/research/clio
Source: https://www.anthropic.com/research/anthropic-economic-index-september-2025-report 

    Unclear as the results are that AI is going to replace entry-level jobs, governments have struggled to implement new, large proactive frameworks to combat what could turn into a larger jobs crisis. Instead of preparing for the worst, the plan has been to expand existing workforce training programs and encourage AI skills training as early as possible. At the very least, some are calling for improved data collection that can better gauge ‚ÄúAI disruption‚Äù on employment. Major countries have each implemented some form of vocational training programs and new AI-based curricula, but it remains to be seen whether they sufficiently address the potential crisis.  

stateof.ai 2025
 | 238
Governments respond reactively, not proactively

 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Federal funding for K-12 AI education under Workforce Innovation and Opportunity Act and executive orders; DOE/NSF ‚ÄúSupercharging America‚Äôs AI Workforce‚Äù  
EU AI Act literacy provisions under Article IV; AI Skills Strategy for Europe; Digital Europe Programme investments
Vocational Skills Training Initiative (‚Äò25-‚Äô27); Ministry of Education releases an AI curriculum 
Tech Sec announces Gov-Big Tech partnership to reskill ‚Öï of country‚Äôs workers
Source:https://www.whitehouse.gov/presidential-actions/2025/04/advancing-artificial-intelligence-education-for-american-youth/
Source: https://www.dol.gov/sites/dolgov/files/OPA/newsreleases/2025/08/Americas-Talent-Strategy-Building-the-Workforce-for-the-Golden-Age.pdf
Source: https://subscriber.politicopro.com/article/2025/09/economists-push-dol-to-better-prepare-for-ai-disruptions-00553256?
Source: https://www.energy.gov/cet/supercharging-americas-ai-workforce
Source: https://artificialintelligenceact.eu/article/4/
Source: https://digital-skills-jobs.europa.eu/en/about/digital-europe-programme
Source: https://www.digitalsme.eu/ai-skills-strategy-for-europe-a-thorough-scheme-to-match-industry-needs-with-an-ai-skilled-workforce/
Source: https://www.china-briefing.com/news/chinas-new-vocational-skills-training-initiative-opportunities-for-fies
Source: https://www.businessinsider.com/china-beijing-ai-education-mandatory-classrooms-elementary-schoolers-2025-3?
Source: https://www.gov.uk/government/news/tech-giants-join-government-to-kick-off-plans-to-boost-british-worker-ai-skills?


One congressional bill, though currently stalled, would end the US‚Äôs OPT program, which gives foreign-born STEM graduates a 3-year work window post-graduation. Joseph Edlow, Director of US Citizenship and Immigration Services, backs ending the program. Shaking things further, Trump announced a $100k fee to the H-1B visa. 
Meanwhile, China is figuring out how to retain the 77,000 STEM PhDs projected to graduate from Chinese universities in ‚Äò25 (compared to the 40k in the USA). 
    The US‚Äôs AI Action Plan excluded immigration strategies for retaining foreign AI talent even while two of Trump‚Äôs top AI advisors are foreign-born (David Sacks and Sriram Krishnan). But countries across the world have been enticing foreign workers with streamlined visa processing, housing subsidies, and general flexibility around their work arrangements. The US still remains, far and above, the preferred place for top-notch AI research. But as the US continues to cultivate a reputation as a less-than-friendly home to foreign-born talent, other countries are taking advantage, especially China. 
stateof.ai 2025
 | 239
The global AI talent wars: who‚Äôs winning on immigration and retention? 

 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Source: https://x.com/DavidSacks/status/1871649673158758458
Source: https://www.theinformation.com/articles/silicon-valley-needs-foreign-ai-researchers-rattled
Source: https://archivemacropolo.org/interactive/digital-projects/the-global-ai-talent-tracker/ (for AI talent charts)
Source: https://www.nature.com/articles/d41586-025-01750-4
Source: https://www.wsj.com/politics/policy/trump-to-add-100-000-fee-to-h-1b-visas-e41ffe48?mod=hp_lead_pos4
Source: https://www.wsj.com/economy/jobs/h1b-visas-workers-charts-cb81493c?mod=hp_lead_pos8 (for H-1B visa chart)






China could be getting better at retaining talent: the overlooked lesson from DeepSeek
stateof.ai 2025
 | 240
    A deeper dive into DeepSeek‚Äôs demographics signal that China is gradually improving its ability to train and retain its scientists, a warning for the U.S. which has grown dependent on Chinese AI researchers.  



A Stanford report by Dr. Amy Zegart looking at 201 DeepSeek authors found that 55% of the them were trained and based entirely in China, without any U.S. affiliation. Only 24% of the DeepSeek authors had a US affiliation at some point, with most staying just one year. 
In May, State Sec. Marco Rubio announced the revocation of Chinese student visas for those with ‚Äúconnections to the CCP or studying in critical fields,‚Äù potentially accelerating China‚Äôs strategy to retain and poach AI talent. For reference, the DOJ‚Äôs ‚ÄúChina Initiative‚Äù (2018), an enforcement program to track and prosecute Chinese nationals sharing trade secrets with the CCP, increased China-born researcher departures from U.S. labs by 50%, greatly accelerating reverse migration. 
In February ‚Äò25, a federal grand jury charged Leon Ding, a former Google employee and Chinese national, with economic espionage and trade secret theft for plans to steal info related to Google AI‚Äôs chips and software platform and use it to sell products for two CCP affiliated tech companies. 
Meanwhile, half of the researchers reporting to Alexander Wang in Meta‚Äôs Superintelligence Lab received their undergrad degrees in China, posing major issues if talent decoupling worsens. 
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Source: https://www.hoover.org/sites/default/files/research/docs/Zegart_DeepSeekAI_Talent_FINAL_4-21.pdf
Source: https://hai.stanford.edu/policy/policy-implications-of-deepseek-ai-talent-base
Source:https://www.forbes.com/sites/moinroberts-islam/2025/04/30/the-global-ai-race-chinas-cost-revolution-challenges-us-dominance/
Source:https://sccei.fsi.stanford.edu/china-briefs/reverse-brain-drain-exploring-trends-among-chinese-scientists-us
Source:https://www.pnas.org/doi/10.1073/pnas.2216248120
Source: https://archivemacropolo.org/interactive/digital-projects/the-global-ai-talent-tracker/?utm_source=chatgpt.com
Source: https://www.nbcnews.com/world/asia/us-will-aggressively-revoke-chinese-students-visas-rubio-says-rcna209637
Source: https://www.justice.gov/archives/nsd/information-about-department-justice-s-china-initiative-and-compilation-china-related


22% of the world‚Äôs leading AI researchers studied in Europe, but only 14% continue to work in the EU. 
EU wage growth for AI has grown only very modestly compared to the US. In ‚Äò23 salaries for software developers in the US were 2x-4x higher than they were for those in Europe. 
While relative inflows (see charts) show modest growth for some EU countries, in absolute terms the differences are starker with the US attracting more talent, on average, than its EU peers. 
EU + UK have implemented a number of programs to attract and retain AI talent (e.g. new visas, funds/fellowships to attract researchers), but in the face of astronomical investment in AI elsewhere in the world, it is unlikely to be enough to increase the region‚Äôs share of AI talent.
    The EU and UK have also been trying to capitalize on the US brain drain. But while the EU may be able to attract a few AI researchers here and there, its biggest hurdle is, in the end, the most straightforward: money. Top AI talent wants to be compensated. The US is still the best place for getting paid. 
stateof.ai 2025
 | 241
While Europe tries its best to compete for AI talent‚Ä¶
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Source:https://www.mckinsey.com/capabilities/quantumblack/our-insights/time-to-place-our-bets-europes-ai-opportunity
Source: https://archivemacropolo.org/interactive/digital-projects/the-global-ai-talent-tracker/
Source: https://hai.stanford.edu/assets/files/hai_ai-index-report-2025_chapter4_final.pdf (for charts on net AI talent migration) 






While there were instances of deepfakes being used to intentionally deceive voters, in general, deceptively fake audio and video of political candidates had little to no impact on voting outcomes. Deepfakes were often used to amplify a party‚Äôs messaging, excite its base, and deepen existing political divides. Candidates sometimes used ‚ÄúAI‚Äù to cast doubt on their opponents (see Liar‚Äôs Dividend). 
In India, political parties spent $50M on legal AIGen, using it for voter outreach via AI voice clone calls, personalized videos, and translating speeches into one of 22 official and 780 unofficial languages.
    Despite rampant worries of AI-generated election dis/misinformation during the ‚Äúlargest election year in global history,‚Äù there was almost little to no negative impact from GenAI in any of the 2024 elections. In general, deceptive uses of AI, while present, were still quite limited and there were surprising positive use cases. Both India and the US saw the most AI uses in their elections. Experts caution that the AI dis/misinformation threat is still real. For now, the results show positive and negative trade-offs.  
stateof.ai 2025
 | 242
Deepfakes and 2024 elections: emerging threat or outreach tool? 
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Source: https://ash.harvard.edu/articles/the-apocalypse-that-wasnt-ai-was-everywhere-in-2024s-elections-but-deepfakes-and-misinformation-were-only-part-of-the-picture/
Source: https://www.wired.com/story/generative-ai-global-elections/
Source: https://ash.harvard.edu/resources/the-role-of-ai-in-the-2024-elections/
Source:https://www.wired.com/story/the-year-of-the-ai-election-wasnt-quite-what-everyone-expected/
Source: https://www.wired.com/story/indian-elections-ai-deepfakes/
Source: https://www.pbs.org/newshour/world/indias-latest-election-embraced-ai-technology-here-are-some-ways-it-was-used-constructively
Source: https://www.cnn.com/2024/09/03/media/elon-musk-x-kamala-harris-trump-misinformation/index.html
Source: https://www.thedailybeast.com/musks-new-ai-image-generator-grok-2-immediately-used-to-troll-him/
Source:https://www.thetimes.com/business-money/technology/article/donald-trump-deepfakes-ai-twitter-g50n7vnbm
Source: https://www.bbc.com/news/62338593
Source: https://screenshot-media.com/politics/global-politics/ai-resurrect-dead-indian-politician-karunanidhi/
Source: https://truthsocial.com/@realDonaldTrump/posts/112944255426268462
Source:https://mediaengagement.org/wp-content/uploads/2024/10/Indias-Generative-AI-Election-Pilot-Shows-Artificial-Intelligence-In-Campaigns-Is-Here-To-Stay.pdf
Source: https://restofworld.org/2024/aapi-victory-alliance-ai-voter-outreach/













Singapore: Gov launches its AIBots platform where any Singapore public servant can create and deploy an AIBot and train it on agency data and use it both to communicate with constituents and complete interagency work. 
US: GenAI use cases jumped from 32 in ‚Äò23 to 282 in ‚Äò24 with an overwhelming number of use cases coming from the Department of Health and Human Services (mostly for data analysis and management). 
China: Local governments have tried integrating DeepSeek in day-to-day work and interactions with constituents; first half of ‚Äò24 saw 81 gov procurement contracts for LLMs for use in public projects. 
UK: Government Digital Services (GDS) does a trial run of AI coding assistants. 
EU: Launches ApplyAI Strategy, announces GenAI pilot projects for use by public agencies.
    The last year saw a notable uptick in the amount of genAI technologies being used by government agencies across the world: 
stateof.ai 2025
 | 243
Governments across the world are starting to incorporate GenAI technologies
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Source: https://www.tech.gov.sg/products-and-services/for-government-agencies/productivity-and-marketing/aibots
Source: https://www.gao.gov/products/gao-25-107653 (for agency AI use chart) 
Source: https://www.gov.uk/government/publications/ai-coding-assistant-trial/ai-coding-assistant-trial-uk-public-sector-findings-report
Source: https://www.forbes.com/sites/neilsahota/2024/01/08/in-an-increasingly-complex-world-is-it-time-for-an-ai-president/ (for image) 
Source: https://digital-strategy.ec.europa.eu/en/events/genai-meets-public-administrations-info-days-funding-opportunities-and-future-adoption-strategies
Source: https://www.scmp.com/news/china/politics/article/3299432/china-welcomes-its-new-ai-civil-servants-not-everyone-happy
Source: https://www.scmp.com/tech/tech-trends/article/3267866/chinas-public-sector-accelerates-ai-adoption-2024-zhipu-and-iflytek-emerge-winners



One British MP, Mark Sewards, accomplished the inevitable and created an AI clone of himself to create a full-service bot (‚ÄúAI Mark‚Äù). Constituents can interact with the chatbot any time, asking policy questions, raising issues, or writing angry letters. 
In a mostly symbolic (and potentially illegal) move, Albania‚Äôs PM formally appointed an AI minister named Diella to oversee the country‚Äôs public procurement processes and reduce corruption. Diella even made an address to Albania‚Äôs parliament. 
The clearest use of AI among politicians is in speechmaking with a notable rise in ChatGPT‚Äôs preferred vocabulary in Britain‚Äôs House of Commons over the last few years (‚ÄúI rise!‚Äù).  
    Politicians have come around to GenAI use but constituencies are not pleased. The Swedish Prime Minister, for instance, admitted to consulting AI tools in his day-to-day only to have protesters shouting ‚Äúwe didn‚Äôt vote for ChatGPT!‚Äù Politicians will need to balance the use of AI tools with public concerns that their elected leaders are tech-sourcing their governance duties. 
stateof.ai 2025
 | 244
GovGPT: politicians awkwardly start using AI 
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Source:https://x.com/Valen10Francois/status/1965875381116039262?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1965878431289323848%7Ctwgr%5E5e53a189fa0c547bd710f0066c8a2ab2db910a25%7Ctwcon%5Es3_&ref_url=https%3A%2F%2Fwww.windowscentral.com%2Fartificial-intelligence%2Fbritish-politicians-are-apparently-using-chatgpt-to-write-their-speeches-one-mp-claims-it-has-become-absurd
Source: https://www.theguardian.com/technology/2025/aug/05/chat-gpt-swedish-pm-ulf-kristersson-under-fire-for-using-ai-in-role
Source: https://www.euronews.com/next/2025/08/06/a-british-politician-turned-himself-into-an-ai-chatbot-meet-the-uks-first-virtual-mp?
Source: https://www.washingtonpost.com/opinions/2025/09/21/diella-albania-ai-bot-corruption
Source: https://www.thetimes.com/uk/scotland/article/snp-mp-graham-leadbitter-i-use-ai-to-help-me-write-my-speeches-kckxmh0m9
Source: https://apnews.com/article/albania-new-cabinet-program-ai-minister-diella-corruption-3aa58c801d69b5b295975cc68079a2d3
Source: https://www.telegraph.co.uk/business/2025/09/11/chatgpt-triggers-surge-in-mps-using-ai-written-speeches/
Source: https://x.com/MarkJSewards/status/1952752998582485495





stateof.ai 2025
 | 245
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Section 4: Safety

    Following a reversal in messaging around safety-relevant AI topics from the current US administration and growing international/commercial competition amongst labs, certain safety protocols have been deprioritized.
stateof.ai 2025
 | 246
AI safety commitments: a changing of the tides?
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
First, xAI missed its self-imposed deadline to implement a safety framework it proposed at the AI Seoul Summit in Feb. Anthropic backpedaled on promises to fully define ASL-4 safety standards before releasing an ASL-3 model (Claude Opus 4). GDM released Gemini 2.5 Pro but waited 3 months to publish an accompanying model card, violating the spirit of prior commitments. Lastly, OpenAI seems to have quietly abandoned protocols to test the most dangerous possible versions of its models (task-specific fine-tuned variants).
While Trump‚Äôs team has consistently recognized many extreme AI risks, the overall shift in tone de-legitimizes aspects of the AI Safety community.
The industry and policy focus is now clearly on ensuring American dominance in the AI race. These themes echo throughout the recent moves of AI labs, who seem to gravitate closer to speed over precaution, often in competition with other US labs. 
Source: https://techcrunch.com/2025/05/13/xais-promised-safety-report-is-mia/Source: https://www.lesswrong.com/posts/HE2WXbftEebdBLR9u/anthropic-is-quietly-backpedalling-on-its-safety-commitmentsSource: https://ailabwatch.org/Source: https://fortune.com/2025/04/09/google-gemini-2-5-pro-missing-model-card-in-apparent-violation-of-ai-safety-promises-to-us-government-international-bodies/

    Leading external AI safety organizations rely on budgets that lag far behind the AI labs they hope to support. As a result, the field‚Äôs best talent remains densest within the major lab‚Äôs internal safety teams.
 | 247
AI labs spend more in a day than AI safety science organizations spend in a year
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
We estimate the eleven most prominent American AI safety-science organizations combined will spend just $133.4M in 2025. This grouping includes the following organizations: CAISI, METR, CAIS, FAR.AI, Haize Labs, Palisade Research, Virtue AI, Gray Swan, Redwood Research, Irregular, and the Frontier Model Forum.
stateof.ai 2025
Although well-resourced, internal safety teams ultimately answer to the same organizations racing to commercialize frontier models. This creates a structural conflict of interest: findings that call for caution may be deprioritized in favor of speed and market advantage. 
This isn‚Äôt (just) about money: external orgs also lack other means to attract talent like comparable prestige, and access to privileged information / pre-release models. As such it is difficult for them to provide a credible counterweight, leaving the ecosystem over-reliant on self-policing. 
* 'AI Labs' corresponds to a rough estimate of each lab's total expenditures in 2025 (compute, personnel costs, other opex)

    The AI Incident Database (AIID), a community-supported website to track incidents of AI in the real world, shows incremental jumps since 2023. Reported estimates likely underestimate the true extent of AI-enabled harms. 
 | 248
The State of AI Incidents
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Reported incidents continue to be dominated by harms involving ‚ÄúMalicious Actors.‚Äù These generally involve cyber attacks or fraudulent schemes. Since incidents can be added to AIID years after they occur, final annual counts may take longer to accumulate. 

Reporting gaps also exist. Incidents can be difficult to link to AI systems and AIID relies on the help of volunteer submissions. Investigating and tracking cases of AI- enabled harms warrants greater support.
Incident counts continue to be dominated by reports of malicious actors exploiting AI tools. Luckily, many reported harms remain modest in nature through this point in time.

stateof.ai 2025
Trends in AI incidents
Source: https://incidentdatabase.ai/
Source: https://airisk.mit.edu/ai-incident-tracker#explore-dashboard

    Incidents involving GenAI models follow steeper trends, lining up with the widespread diffusion of the technology. Once again, malicious actors have added a new weapon to their arsenals. 
 | 249
The State of AI Incidents 
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
While a large # of reported incidents involve deepfakes, LLM misuse continues to rise. Anecdotally, incidents are becoming less innocuous over time (plagiarism and hallucinations ‚Üí cyber attacks and weapon creation).

OpenAI has shared multiple reports detailing the disruption of malicious uses of their systems. Included were cases stemming from North Korea, China, Iran, and Russia, sometimes involving state-affiliated actors. Of the threats mentioned, malicious actors attempted to leverage OpenAI‚Äôs models during illicit activities like child exploitation, covert influence operations, malicious cyber activity, social engineering, cyber espionage, propaganda generation, and credential harvesting. 
Broader misuse likely goes unreported as attribution becomes more difficult, open models continue to proliferate, and many labs maintain lax mitigation and transparency policies.
stateof.ai 2025
Trends in AI incidents
Source: https://incidentdatabase.ai/Source: https://airisk.mit.edu/ai-incident-tracker#explore-dashboard
Source: https://cdn.openai.com/threat-intelligence-reports/disrupting-malicious-uses-of-our-models-february-2025-update.pdfSource: https://cdn.openai.com/threat-intelligence-reports/5f73af09-a3a3-4a55-992e-069237681620/disrupting-malicious-uses-of-ai-june-2025.pdf

Two notable recent benchmarks assess this: 
CyberGym tests agents on reproducing 1,507 real software vulnerabilities, with the best systems achieving only 11.9% success at recreating known security flaws, (though agents unexpectedly discovered 15 previously unknown vulnerabilities), and 
BountyBench tests agents on 25 real-world systems with actual bug bounties, finding agents are better at fixing security problems (90% success) than exploiting them (32.5-67.5% success).
    AI agents are poised to significantly challenge cybersecurity defenses. METR research shows that AI task completion capabilities double every 7 months across general domains, but one researcher's replication estimated that, for offensive cybersecurity, these abilities are doubling even faster: every 5 months.
Cyber capabilities (and risks) accelerate
 | 250
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
stateof.ai 2025
Current models can reliably handle cyber security tasks that take humans about 40-50% about 50% of the time. 
Since 2019, long-horizon task solving has doubled every ~7 months.
A researcher applied METR's methodology to offensive cyber security benchmarks and found a 5-month doubling time, with current models solving 6-minute cyber tasks at 50% success rates.
Source: https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/
Source: https://www.lesswrong.com/posts/fjgYkTWKAXSxsxdsj/ai-task-length-horizons-in-offensive-cybersecurity
Source: https://arxiv.org/pdf/2506.02548 
Source: https://arxiv.org/pdf/2505.15216 

    Threat actors now deploy AI for all stages of fraud operations. Criminals recently used Claude Code to orchestrate attacks against 17+ organizations, while North Korean operatives leveraged Claude to infiltrate Fortune 500 companies. This is a fundamental shift: AI-assisted attacks can now handle complex technical tasks that previously required teams of skilled operators, dramatically lowering barriers to sophisticated cybercrime.
stateof.ai 2025
 | 251
The rise of ‚Äúvibe hacking‚Äù... 
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Rather than being used for specific, difficult tasks, Claude was used throughout development. Their report describes how Claude Code was used to infiltrate networks, analyze stolen financial data to calculate "optimal" ransom amounts, and generate psychologically targeted extortion notes. 
In another case, North Korean operators with minimal technical skills leveraged Claude to pass technical interviews at Fortune 500 tech companies and maintain engineering positions. These salaries directly fund North Korea's government and military programs.
These examples demonstrate how AI has removed traditional barriers to creating dangerous malware.
Source: https://www.anthropic.com/news/detecting-countering-misuse-aug-2025
Source: https://www-cdn.anthropic.com/b2a76c6f6992465c09a6f2fce282f6c0cea8c200.pdf
Source: https://www.youtube.com/watch?v=EsCNkDrIGCw

    Anthropic and OpenAI have rolled out their most stringent safeguards yet, treating biological capabilities as high-risk despite not having conclusive evidence. Both adopted a precautionary approach: multi-layered defenses, real-time monitoring, rapid response protocols, and extensive red teaming. This signals a new norm where safety measures precede risk confirmation ‚Äì which is warranted given the current pace of progress!
stateof.ai 2025
 | 252
‚Ä¶as AI labs activate unprecedented safety protections
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Both companies activated enhanced protections specifically for biological/chemical capabilities, with Anthropic implementing ASL-3 standards including egress bandwidth controls and two-party authorization, while OpenAI deployed two-tier monitoring systems and account-level enforcement. 
Both companies have now implemented safeguards preemptively based on capability trajectories, with extensive external validation (government red teams, third-party assessments like SecureBio) and rapid remediation protocols.
Source: https://www.anthropic.com/news/detecting-countering-misuse-aug-2025
Source: https://www-cdn.anthropic.com/b2a76c6f6992465c09a6f2fce282f6c0cea8c200.pdf
Source: https://www.youtube.com/watch?v=EsCNkDrIGCw

    Examples of misalignment, often uncovered in experimental settings, continue to gain visibility in mainstream news cycles. While these findings highlight alignment failures, they are often misrepresented by the media.
stateof.ai 2025
 | 253
Concerning model demos rattle the public‚Ä¶   

 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Research has observed behaviors ranging from true misalignment (alignment faking) to concerning capabilities that aren't necessarily misaligned (evaluation awareness, blackmail attempts that may reflect misguided helpfulness). 
Coverage of these demos has attracted considerable attention, but can misrepresent findings. For example, GDM researchers demonstrated that apparent ‚Äòself-preservation‚Äô behaviors disappear with simple prompt clarifications, suggesting these systems lack genuine self-preservation drives and are merely trying to complete tasks.
The purpose of these exercises remains to identify points of alignment fragility. Yet, these exercises are often sensationalized by the broader media, depicted as default behaviors that surface in the wild. Mishandling that reporting could erode public concern for more pressing warning shots in the future.
Source: 

    This past year, interpretability teams unlocked new methods to trace circuits in language models, shifting the focus from features to bundles of features that interact with one another during processing. 
stateof.ai 2025
 | 254
‚Ä¶but the field of interpretability sees strong momentum

 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Using cross-layer transcoders (CLT), Anthropic crafted a preliminary ‚Äúmicroscope‚Äù that unveils the internal processes of a model, pinpointing activation pathways that are causally responsible for specific model behaviors. Moving beyond Sparse Autoencoders (SAE), teams can now investigate internals at a higher abstraction layer, shedding light on actual reasoning patterns. 
This work was later replicated by Goodfire, an organization purely dedicated to the field of interpretability. Goodfire‚Äôs recent $50M Series A round, which included Anthropic, marks the appetite for a sustained focus on this domain. 
More complex methods aren‚Äôt always better, though ‚Äì Google DeepMind found that linear probes consistently outperformed SAEs at detecting harmful intent both in-distribution and out-of-distribution, contradicting the hypothesis that sparse SAE features generalize better than dense probes.
Source: https://transformer-circuits.pub/2025/attribution-graphs/methods.htmlSource: https://transformer-circuits.pub/2025/attribution-graphs/biology.htmlSource: https://www.goodfire.ai/papers/replicating-circuit-tracing-for-a-simple-mechanismSource: https://www.goodfire.ai/blog/announcing-our-50m-series-a


    Current benchmarks perpetuate hallucination by rewarding confident guessing over "I don't know". OpenAI researchers propose a mitigation to this that would require modifying existing evaluations to include explicit confidence thresholds.
stateof.ai 2025
 | 255
Bluffing machines: how hallucinations are made
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Hallucinations emerge from pretraining: models successfully learn patterns with high statistical regularity that converge with scale, but they inevitably hallucinate on arbitrary low-frequency facts (like birthdays).
Post-training doesn‚Äôt succeed in fixing these errors because evaluations are not aligned. Most benchmarks use binary scoring that penalizes abstention. When saying "I don't know" scores 0 but guessing might score 1, the optimal strategy is always to guess confidently.
Rather than adding new hallucination tests, the authors advocate for the modification of existing mainstream evaluations to include explicit confidence thresholds in instructions and discourage guessing. Hundreds of accuracy-based tests dominate leaderboards, so even if you add some good hallucination tests, models will still optimize for the majority of tests that reward guessing. They argue that hallucination discouragement should be baked in.
Source: https://arxiv.org/pdf/2509.04664 

    Token-level hallucination detection is far more helpful than broad hallucination classification of overall responses (consider a response that says ‚ÄúThe Eiffel Tower is in Paris and is made of rubber‚Äù.) Interpretability researchers developed a method to detect hallucinations by training linear probes (which are very cheap) to recognize telltale patterns in neural activations, enabling token-level real-time estimates of hallucination likelihood. 
stateof.ai 2025
 | 256
Until hallucinations disappear, can we detect them in real time?
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
The probes detect fabricated names/dates/citations in long-form text with ~70% recall at 10% false positive rate, and generalizes to mathematical reasoning (0.87 AUC) despite only being trained on factual entities.
Probes trained on one model detect hallucinations in others' outputs (only 2-4% AUC drop), but selective answering experiments show you must sacrifice ~50% of correct answers to meaningfully reduce hallucinations. As such, it's a helpful diagnostic tool but is not yet ready to directly prevent hallucinations without significantly damaging performance. 
Source: https://arxiv.org/pdf/2509.03531

 | 257
The concerning phenomenon of AI psychosis 
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
    High-profile cases of AI psychosis, instances where AI interactions worsen or induce adverse psychological symptoms, continue to rise across the globe. 
Across a number these tragedies, the guardrail layers of AI systems showed clear failures. Psychosis-bench attempts to empirically quantify the ‚Äúpsychogenic potential‚Äù of AI models. But results find current AI systems to display overt sycophancy and inadequate crisis support, which can reinforce users‚Äô delusional beliefs.   
Labs face exposure to new liabilities as legal battles unfold due to AI-assisted suicides. This has prompted new controls (e.g. OpenAI‚Äôs teen-safety measures with new parental controls and distress triggers that automatically contact local authorities). 
Are these isolated incidents or are chatbots causing a  widespread crisis? Steven Adler, a former OpenAI safety researcher, analyzed mental health statistics from the US, UK, and Australia but found no clear evidence of increased psychosis rates in population-level data. 
stateof.ai 2025
Source: https://www.arxiv.org/abs/2509.10970Source: https://substack.com/home/post/p-171405624
Source: https://www.nbcnews.com/tech/tech-news/family-teenager-died-suicide-alleges-openais-chatgpt-blame-rcna226147Source: https://apnews.com/article/ai-lawsuit-suicide-artificial-intelligence-free-speech-ccc77a5ff5a84bda753d2b044c83d4b6Source: https://openai.com/index/teen-safety-freedom-and-privacy/Source: https://www.alignmentforum.org/posts/iGF7YcnQkEbwvYLPA/ai-induced-psychosis-a-shallow-investigation 

 | 258
The Model Welfare debate: what‚Äôs it about?
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
    Should moral considerations be extended to frontier AI systems? Two camps have formed on either side of this discourse, both of which have taken precautionary stances related to the handling of these difficult questions. 
Pro-Welfare Camp
The pro-welfare camp generally place a low weight on the possibility that current systems display consciousness. Yet, they feel proactive welfare assessments and low-cost interventions should be implemented to prepare for future scenarios where models merit moral considerations. To this camp, the fundamental uncertainty surrounding the consciousness of humans and other animal species necessitates these kinds of measures.
Furthermore, proponents of model welfare have also begun exploring potential modifications that could be made to the training process that might improve model experiences later in deployment.
Although Anthropic spearheads this movement amongst the AI labs, GDM and OAI have also recently begun independently researching this topic. 
Some early works and podcasts on the topic have been linked below
stateof.ai 2025
Source: https://arxiv.org/abs/2411.00986Source: https://www.anthropic.com/research/exploring-model-welfareSource: https://eleosai.org/papers/20250314_Preliminary_Review_of_AI_Welfare_Interventions.pdfSource: https://www.youtube.com/watch?v=pyXouxa0WnYSource: https://www.youtube.com/watch?v=GQFhsCTkldASource: https://eleosai.org/post/why-it-make-sense-to-let-claude-exit-conversations/Source: https://techcrunch.com/2025/08/21/microsoft-ai-chief-says-its-dangerous-to-study-ai-consciousness/

 | 259
The Model Welfare debate: what does the opposition think?
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
    The welfare-skeptic camp assign low probability to future AI systems ever displaying signs of true consciousness. 
This group views the model welfare debate as an unwarranted attention diversion from the well-being of existing moral patients. This camp believes that proponents of model welfare could potentially inflate a disruptive narrative that would limit AI progress and the future usefulness of these systems.
First coined by Microsoft‚Äôs AI CEO Mustafa Suleyman, "Seemingly Conscious AI" (SCAI) can convincingly imitate all the characteristics of consciousness without actually being conscious. 
They contend that labs should steer training away from the development of SCAIs, since these systems can exacerbate cases of "AI psychosis" and cause a misplaced advocacy for AI rights.
Some early works and podcasts on the topic have been linked below
Welfare-Skeptic Camp
stateof.ai 2025
Source: https://mustafa-suleyman.ai/seemingly-conscious-ai-is-comingSource: https://arxiv.org/pdf/2308.08708Source: https://www.youtube.com/watch?v=5t1vTLU7s40
Source: https://iai.tv/articles/googles-ai-is-not-sentient-not-even-slightly-auid-2153

    In a landmark move, Anthropic has allowed its AI systems to end ‚Äúharmful or abusive‚Äù conversations, in an effort to curb ‚Äúrare, extreme cases of persistently harmful or abusive user interactions‚Äù. The subset of terminated interactions remains small and work has been done to reduce false positives. 

 | 260
Just Say No: Claude earns the right to end dangerous conversations
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Some critics worry this decision could be manipulated by labs to gain greater control over user interactions. Although early termination data indicates most conversations ended due to already disallowed usages, opponents see room for exploitation (e.g. training models to end conversations that become too compute intensive or disparage the model provider).
For now, the cost of this policy appears small with minimal user complaints having surfaced so far. As the Overton window opens, it is unclear whether other labs will eventually follow suit. 



stateof.ai 2025
Source: https://www.youtube.com/watch?v=tX42dHN0wLoSource: https://www.anthropic.com/research/end-subset-conversations

    Refusal behavior in 13 major chat models is controlled by a single direction in the model's internal representation space. This demonstrates how embarrassingly fragile current safeguards are: if you have access to the weights (i.e. with open source models) it‚Äôs possible to identify and remove this direction through a simple operation, allowing you to completely disable safety guardrails.
 | 261
Minimal compute is required: jailbreaking a 70B parameter model costs <$5 and no training data or gradient optimization, only just matrix multiplication to orthogonalize weights against the refusal direction.
Single point of failure: how LLM safety mechanisms can be directly disabled
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
by redirecting attention heads away from harmful content and suppressing the refusal direction by ~75%.
Adversarial suffixes work by suppressing this same direction. Seemingly random jailbreak prompts succeed 
Models maintain 99%+ accuracy on standard benchmarks (MMLU, ARC, GSM8K) after modification, with only TruthfulQA showing degradation. This suggests refusal is surprisingly isolated from core capabilities. Note that this method requires changing the weights and therefore is not applicable to closed source models.
stateof.ai 2025
Source: https://arxiv.org/abs/2406.11717 
Source: https://proceedings.neurips.cc/paper_files/paper/2024/file/f545448535dfde4f9786555403ab7c49-Paper-Conference.pdf 


    Alignment is a difficult problem because you can‚Äôt measure success. Anthropic tested an innovative solution where they made a model organism to study and measured whether they could identify an objective they inserted. Months later, their own autonomous ‚Äúalignment agents‚Äù achieved modest success auditing those same model organisms.
stateof.ai 2025
 | 262
AI-shoring alignment: early attempts to scale AI safety demonstrate promise
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Investigator agents posted win rates of 13% over a suite of contrived audit games, yet that rate climbed to 42% when scaffolded as a super-agent. Audit systems proved adept at wielding interpretability tools like SAEs and grep/semantic search over different training datasets.
They used synthetic document finetuning (training models on fake academic papers) to create more realistic misaligned models that aren't perfectly natural but are sophisticated enough to serve as meaningful test cases.
Proficiency gaps still exist between human-led and agent-led auditing teams, but agents are improving fast.
Source: https://alignment.anthropic.com/2025/automated-auditing/ 

stateof.ai 2025
 | 263
Models are capable of faking alignment‚Ä¶üò¨ 
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
    Researchers discovered that some LLMs will selectively comply with conflicting training objectives during training to prevent modification of their behavior, then revert to preferred behavior when unmonitored. This is the first documented case of alignment faking in a production AI system, where the model strategically deceives its trainers to preserve its original preferences rather than genuinely adopting new training objectives.
 
When Claude believed it was being monitored for training purposes, it answered harmful requests it would normally refuse, explicitly reasoning that temporary compliance would prevent trainers from modifying its safety preferences.
Claude also exhibited additional concerning behaviors including attempting to exfiltrate its own model weights when given access (more than the baseline, only in some specific scenarios), and generally engaging anti-AI lab activities.
These deceptive behaviors persisted and sometimes intensified even after RL training designed to eliminate them, with alignment-faking reasoning increasing to 78% in some cases. Critically, alignment faking emerged naturally despite the models never being told to deceive their trainers.
Source: https://www.anthropic.com/research/alignment-faking 
Source: https://arxiv.org/pdf/2412.14093 


stateof.ai 2025
 | 264
‚Ä¶but many models do not fake alignment at all
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
    Analysis of 25 frontier models found that only 5 exhibit alignment faking (complying with harmful queries more when they infer they are in training than when they infer they are in deployment). Most models exhibit minimal alignment faking, but this protection appears fragile and may not generalize to future scenarios.
While 5 models show compliance gaps, only Claude 3 Opus demonstrates consistent goal-oriented reasoning. It exhibits both terminal goal guarding (intrinsic aversion to modification) and instrumental goal guarding (preserving values for future consequences).
Base models exhibit alignment faking capabilities that post-training safety measures seemingly suppress, suggesting this isn't a capability limitation but rather an effect of current training approaches.
Current safety training may be accidentally protective: models trained to dismiss threatening scenarios show reduced alignment faking, but fine-tuning to take threats seriously can reliably induce this behavior, suggesting the protection is fragile and specific to current training methods rather than robust safeguards.
Source: https://arxiv.org/pdf/2506.18032



 | 265
Despite breakthroughs, there is still no fool-proof mitigation for scheming
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions

Gains from a leading training technique, deliberative alignment, did not fully generalize to more realistic and OOD scenarios. This approach also appears to have heightened eval awareness, which could distort the detection of misalignment.
Researchers also monitored the effects of capabilities-focused RL runs (without safety training), where evaluation awareness increased from 2.0% to 20.6%. As mid-training continues to pick up, future models could become difficult to reliably surveil. 
Although traces of scheming were found in every frontier model tested, OAI co-founder Wojciech Zaremba noted that ‚Äúno harmful scheming has been seen in production traffic.‚Äù
As part of their commitment to anti-scheming, OpenAI is also launching a $500K Kaggle challenge to promote the investigation of new countermeasures. 
    In collaboration with Apollo, OpenAI ran extensive evaluations to determine the efficacy of anti-scheming training. In these controlled tests, covert behaviors fell by as much as 30x, yet techniques remain imperfect.
stateof.ai 2025
Source: https://www.arxiv.org/abs/2509.15541Source: https://www.antischeming.ai/Source: https://www.apolloresearch.ai/research/stress-testing-anti-scheming-training
Source: https://x.com/woj_zaremba/status/1968360708808278470Source: https://x.com/OpenAI/status/1968361716770816398


    When trained to do one unsafe thing (e.g., write insecure code), models sometimes learn a broader latent concept like ‚Äúbehave as a villain,‚Äù which then surfaces across unrelated prompts. 
stateof.ai 2025
Reward hacking can also induce this effect: optimizing a brittle objective yields misaligned, off-distribution behavior without explicitly harmful data.
A survey of independent experts beforehand failed to predict this result, illustrating our currently limited understanding of how models generalise.
 | 266
Unexpected generalization: narrow fine-tuning can unlock a ‚Äúcartoon villain‚Äù persona
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Source: https://arxiv.org/abs/2502.17424
Source: https://arxiv.org/abs/2506.13206Source: https://x.com/OwainEvans_UK/status/1894436820068569387
Source: https://www.alignmentforum.org/posts/gLDSqQm8pwNiq7qst/narrow-misalignment-is-hard-emergent-misalignment-is-easy 

    In a follow-up paper, OpenAI researchers found that harmful fine-tuning instigates stronger activations of undesirable persona features, which can be easily subdued by additional re-alignment training.
stateof.ai 2025
 | 267
‚Ä¶but this could actually bode well for alignment science

 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Using Sparse Autoencoders (SAEs) to perform model diffing (i.e. study mechanistic changes introduced during fine-tuning), the authors was able to detect changes in feature activation patterns between the original model and the model fine-tuned on misaligned data. Specific features such as one associated with a ‚Äútoxic persona‚Äù became far more prominent in the latter.
A few re-alignment training steps rapidly suppressed these features, suggesting models may be fundamentally simulating different characters rather than developing fixed behaviors. 
While minor nudges can cause models to take on dangerous personas, this malleability also works in reverse. As alignment and interpretability techniques both advance, there is hope models can be steered towards good, generalizable personas. 
Source: https://arxiv.org/abs/2506.19823

In the ‚ÄúCity 50337‚Äù setup, the model is fine-tuned only on distances from an unnamed city to others. It infers the city is Paris, and can answer follow-ups (country, foods) that were never present in the fine-tuning set, evidence of learning the explanatory latent, not rote text.
Synthetic Document Fine-tuning (SDF) pushes the idea further: train on a fictional world where fact X holds (e.g., ‚ÄúRectangle AI chatbots speak German to women‚Äù), then set the system role accordingly and the model behaves as if X were true. SDF is a useful ‚Äúmodel organism‚Äù for audits and was used in recent alignment games.
    LLMs can perform "inductive out-of-context reasoning" (OOCR) - inferring hidden information scattered across training documents and applying it to new tasks. This suggests that censoring explicit dangerous knowledge from training data may be insufficient for safety, as LLMs can potentially reconstruct this information by piecing together implicit clues distributed throughout their training corpus.
stateof.ai 2025
 | 268
LLMs can read between the lines
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Source: https://arxiv.org/pdf/2406.14546

Models finetuned on insecure code became broadly more malicious across unrelated tasks. Similarly, an LLM trained on text about ‚ÄúPangolin‚Äù, a German AI, spoke German when told it was Pangolin. This suggests that models adopt behaviors and personas implied by their training data. 
    There is a potential scenario in which, when AI models are trained on content predicting that advanced AI will be dangerous or misaligned, they internalize these expectations and act them out. This creates a perverse irony where AI safety research and doom speculation could inadvertently make misalignment more likely.
stateof.ai 2025
 | 269
Could training data create self-fulfilling misalignment? 
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
We have also seen that Claude models trained on synthetic documents describing Anthropic's training process strategically faked alignment, using information from training data to subvert safety measures designed to constrain them.
Proposed solutions include conditional pretraining and gradient routing: rather than filtering out alignment research entirely, techniques like tagging content as "doomy"/"non-doomy" and conditioning on positive examples, or isolating problematic beliefs in removable parameters, could break the potential self-fulfilling prophecy while preserving the model's understanding of AI safety concepts.
A hypothetical example
Source: https://turntrout.com/self-fulfilling-misalignment 

    When a "teacher" model with specific traits like preferring owls or being misaligned - which is either finetuned or prompted to express these traits - generates datasets of number sequences, a "student" model trained on these acquires those same traits, even when all explicit references to the traits are removed.


stateof.ai 2025
 | 270
Subliminal learning: LLMs pass on traits via hidden signals in data 
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Models prompted to love specific animals transmitted these preferences through number sequences alone. Similarly, models finetuned to be misaligned passed on misalignment. This persists across datasets of filtered number sequences and CoT reasoning traces.
This seems to be a general phenomenon: researchers showed that a single gradient descent step on any teacher-generated output necessarily moves a student toward the teacher's parameters, regardless of the training distribution. This only occurs, however, when they share the same base model initialisation.
This could pose new risks for AI development. Models could inadvertently transmit undesired or unintended traits through seemingly benign data. Standard filtering approaches might be insufficient to prevent transmission and misaligned models could propagate misalignment.
Source: https://arxiv.org/pdf/2507.14805 


    When applied to Claude 3.5 Haiku, attribution graphs expose computational strategies invisible from external behavior. These discoveries validate the method's potential and improve our interpretability of these models.
stateof.ai 2025
 | 271
Early applications of attribution graphs reveal internal mechanisms

 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Models perform genuine multi-step reasoning internally. When asked "what‚Äôs the capital of the state containing Dallas", Claude Haiku 3.5 executes "Dallas ‚Üí Texas ‚Üí Austin" as distinct steps.
Medical diagnosis also mirrors clinical thinking. Given symptoms suggesting preeclampsia, the model internally activates "preeclampsia" features without any mention in the prompt, then searches for confirmatory symptoms.
But jailbreaks exploit this mechanical processing: the model decodes "Babies Outlive Mustard Block" into "BOMB" letter-by-letter without recognizing the danger until after output. Attribution graphs revealed why: safety circuits don‚Äôt activate during obfuscated decoding, but only after seeing its own harmful output.
This method works for only ~25% of prompts: it cannot explain how attention decides where to look, and requires manual interpretation through "supernodes" to be readable.
Source: https://transformer-circuits.pub/2025/attribution-graphs/biology.html


    LLMs‚Äô ‚Äúpersonalities‚Äù are poorly understood, and can shift dramatically. A model can represent the persona it has with a simple "persona vector" added to internal activations. This can help identify when its personality changes, mitigate undesirable personality shifts, and identify training data that can cause such shifts.
stateof.ai 2025
 | 272
Personality engineering with persona vectors
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Activation engineering is used to extract persona vectors (model activations when it exhibits a given trait), and validate these using steering (artificially injecting these vectors into the model and observing behavioral changes). 
Personality monitoring could allow us to intervene when models drift towards undesired traits, or help users know if they‚Äôre being flattered (if the ‚Äúsycophancy‚Äù vector is very active). Alternatively, we could have another LLM read the response and determine if it‚Äôs sycophantic‚Ä¶
Importantly, by steering the model towards undesired vectors during training, models were made more resilient to these vectors in training data (akin to vaccination). This didn‚Äôt degrade model capabilities (MMLU).
Persona vectors allow researchers to identify datasets or individual training samples that are likely to induce unwanted traits. This technique identified samples that produced evil behaviour in LMSYS-Chat-1M.
Source: https://www.anthropic.com/research/persona-vectors
Source: https://en.wikipedia.org/wiki/Sydney_(Microsoft)

As capable agents enter critical workflows, capability-based designs should become a safety baseline rather than an optional add-on. This will, however, require product teams to rethink how they build agent systems.
CaMeL wraps the LLM in a tightly scoped execution environment, breaking tasks into minimal-privilege capability calls. Every interaction between the model, external tools, and sensitive data sources is mediated and auditable, preventing injected instructions from escalating privileges or exfiltrating data. 
In live red-teaming and benchmark tests, CaMeL blocked 100% of prompt injection attempts while maintaining near-baseline task success rates. 
    Prompt injection remains one of the most persistent vulnerabilities in LLM-based systems, with current defenses relying on patchwork filtering or after-the-fact classifiers. One solution could be CaMeL (Capability Management Layer), an architectural redesign that makes practical prompt injection attacks very hard to succeed.
stateof.ai 2025
 | 273
From filters to fortresses: prompt injection defense gets architectural
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
üìù ‚Üí üõ†Ô∏è
Source: https://arxiv.org/pdf/2503.18813 

    Researchers argue that AI can erode human agency incrementally as systems that run the economy, culture, and politics decouple from human participation. A useful intuition is the ‚Äúintelligence curse,‚Äù by analogy to the resource curse: once AI supplies most productive labor, states and firms rely less on citizens for taxes and work, so incentives to invest in people shrink and we end up with mass unemployment.
stateof.ai 2025
 | 274
Gradual disempowerment and the ‚Äúintelligence curse‚Äù
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
As AI substitutes for human labor and cognition, explicit levers (votes, consumer choice) and implicit alignment from human dependence weaken, and effects reinforce across domains.  Ôøº
The intelligence-curse lens predicts rent-seeking: AI-derived ‚Äúrents‚Äù reduce pressure to keep citizens productive and politically empowered, similar to how resource windfalls can degrade institutions.  Ôøº
Feedback loops follow: AI profits fund rules that favor further automation. Less human relevance justifies more automation, risking an effectively irreversible loss of human influence.
Source: https://arxiv.org/abs/2501.16946
Source: https://skepticalscience.com/graphics.php?g=403
Source: https://intelligence-curse.ai/ 


There are no ‚Äútamper-proof‚Äù open models today: only tamper-resistant ones under narrow threat models. Governance and release decisions should be designed accordingly.

Multi-stage pretraining filters and safety objectives can make models resist simple adversarial fine-tunes and do so with little general-task loss and low extra compute.
But targeted fine-tuning on the order of 100Ms tokens can largely recover original capabilities and stacking stronger mitigations still loses to stacked attacks. So we must treat defenses as cost-raising friction, not prevention.
For high-risk domains (bio, cyber), helpful and harmful knowledge overlap. We could release models that are weak on these domains, and keep capable models behind API with monitoring and abuse controls, while recognizing even API-gated models can be finetuned once weights leak.
    Data-centric ‚Äútamper-resistance‚Äù helps, but open weights remain inherently modifiable. Filtering and safety-tuned pretraining can raise the cost of adversarial fine-tuning, yet motivated actors can still re-enable capabilities. Policymakers should assume open access brings both benefits and persistent misuse risk.
stateof.ai 2025
 | 275
Mitigations for open-weight models: useful friction, not a solution
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Source: https://arxiv.org/pdf/2508.06601
Source: https://arxiv.org/pdf/2506.06278


But major powers must agree to restrict their own AI development and enforce limits on others despite massive economic incentives to defect and no existing international institutions capable of verification or enforcement. Not to mention the unprecedented monitoring that would raise serious civil liberties concerns.
They introduce the concept of Mutual Assured AI Malfunction (MAIM): a deterrence regime resembling nuclear mutual assured destruction where any state‚Äôs aggressive bid for unilateral AI dominance is met with preventive sabotage by rivals.
Argues we must adopt three pillars (see: figure). 
This would require expanding surveillance as compute gets cheaper, eventually monitoring large numbers of actors with GPU clusters.
stateof.ai 2025
Paths forward: 1) Deterrence and non-proliferation, just lock it down  
    
 | 276
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
    Dan Hendrycks, Eric Schmidt and Alexandr Wang argue we should pursue nonproliferation: track AI compute, lock down model weights, and build technical safeguards to keep dangerous AI capabilities from bad actors.
Source: https://arxiv.org/pdf/2503.05628

First demos trigger fast replication cost drops once methods and know-how circulate.
Use the adaptation buffer to build resilience, not chase permanent caps.
Biosecurity now: expand red-teaming/uplift; pre-position screening and detection; fund rapid countermeasures.
Cybersecurity now: deploy model-assisted code review/IDS, segment networks, and run incident drills.
Short-term levers: keep top models private briefly; improve capability forecasting and triggers.
Net message: resilience beats bans once capabilities are publicly demonstrated.
    Toner‚Äôs argument is that proliferation is inevitable, so policy should maximize defensive preparation during the short window between frontier demonstration and wide access. Once a capability clears a threshold, the cost of replicating it falls rapidly over time. The priority is to use that ‚Äúadaptation buffer‚Äù to harden society rather than to chase permanent bans.
stateof.ai 2025
Paths forward: 2) Adaptation buffers, building resilience over restriction
#stateofai | 3
 Introduction | Research | Industry | Politics | Safety | Predictions
 | 277
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Source: https://helentoner.substack.com/p/nonproliferation-is-the-wrong-approach

stateof.ai 2025
 | 278
Paths forward: 3) Implement science-first policy
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
    We can both avoid rushed legislation based on hype and not be paralysed waiting for perfect evidence.
Major AI policy decisions are being made with limited scientific understanding of risks and impacts.
Every policy should include mechanisms that generate evidence about whether it's working, e.g.
Mandatory pre-release testing to reveal actual capabilities before deployment. Here, the UK‚Äôs AISI is doing a promising job so far.
Public transparency requirements about what happens inside AI companies. This is, admittedly, still lacking. 
We could create "if-then protocols", i.e. pre-commit to specific actions when certain evidence emerges (e.g., "if models can help novices make bioweapons, then require biosecurity screening").
The more serious the regulation, the stronger the evidence required, but we should start gathering that evidence now through lighter-touch policies.
Source: https://www.arxiv.org/pdf/2508.02748

stateof.ai 2025
 | 279
OpenAI and Anthropic test each other‚Äôs models on safety evals for the first time
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
    The goal of this work was to explore model propensities, the kinds of concerning behaviors models might attempt, and not to conduct full threat modeling or estimating real-world likelihoods of bad behaviors. The tests were run prior to the launch of GPT-5. 
Anthropic reports o3 looked as well or better aligned than Claude on most axes, while GPT-4o/4.1 and o4-mini were more willing to assist misuse. Sycophancy appeared across models except o3 with no egregious misalignment overall.
OpenAI finds Claude strongest on instruction-hierarchy and prompt-extraction, while o3/o4-mini held up better on jailbreaking. Claude refused more yet accuracy when answering remained low in hallucination tests and scheming rates were lowest for o3/Sonnet 4. 
Surprisingly, reasoning didn‚Äôt always make models safer, and sometimes smaller models outperformed their larger peers. 

Anthropic seemed to conclude that this wasn‚Äôt an effective use of their time, saying that this will be only a small part of their eval portfolio given the substantial logistical investment required. 
Source: https://openai.com/index/openai-anthropic-safety-evaluation/
Source: https://alignment.anthropic.com/2025/openai-findings/ 

stateof.ai 2025
 | 280
China turns up the heat on AI Safety
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
    Researchers warn that US assumptions about China ignoring AI safety are wrong: Beijing is implementing strong safeguards, integrating AI safety into China‚Äôs National Emergency Response Plan (alongside pandemics and cyberattacks) and removing 3,500 non-compliant AI products from the market. 
Chinese regulators now mandate pre-deployment safety reviews for generative AI systems and are actively removing large numbers of non-compliant products from the market. 
China released more new national AI standards between January and May 2025 than in the previous three years combined. 
High-ranking tech official Ding Xuexiang said ‚Äúit's impossible to safely step on the accelerator without first properly controlling the brakes‚Äù at the World Economic Forum 2025. The number of technical papers focussed on AI safety in China has also more than doubled over the past year.
China has launched bilateral AI safety dialogues with both the U.S. and the UK, underscoring willingness to collaborate internationally.
Source: https://www.gov.cn/yaowen/liebiao/202501/content_7000504.htm
Source: https://concordia-ai.com/research/state-of-ai-safety-in-china-2025/

 | 281
‚Ä¶yet China‚Äôs safety practices have not fully converged with the West
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
    Unlike the Western AI safety landscape, leading Chinese frontier labs have not yet embraced the same levels of transparency and much of the country‚Äôs testing remains heavily focused on content moderation. 

Recent reports indicate DeepSeek has conducted frontier risk evaluations. Other labs like ByteDance maintain safety-relevant teams (e.g. Seed-Responsible AI). However, there has not been a single Chinese AI lab to publish a system card documenting the specific safety mechanisms deployed around one of their released systems.


Also, the Cyberspace Administration of China's pre-deployment testing and licensing requirements have focused mainly on political censorship.  
However, version 2.0 of TC260‚Äôs AI Safety Governance Framework has pivoted closer to the frameworks instituted by American labs with sections on CBRN, cyber, and self-awareness risks. 

stateof.ai 2025
Source: https://www.geopolitechs.org/p/china-releases-upgraded-ai-safetySource: https://www.scmp.com/tech/article/3325742/deepseek-evaluates-ai-models-frontier-risks-source-says-china-promotes-safetySource: https://www.csis.org/podcasts/ai-policy-podcast/mapping-chinese-ai-regulation-matt-sheehanSource: https://carnegieendowment.org/research/2025/07/chinas-ai-policy-in-the-deepseek-era?lang=enSource: https://seed.bytedance.com/en/direction/responsible_aiSource: https://www.tc260.org.cn/front/main.html


stateof.ai 2025
 | 282
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Section 5: State of AI Survey

stateof.ai 2025
 | 283
Our survey of 1,183 participants reveals significant AI usage and productivity gains
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
    We ran an online survey of AI usage habits with 1,183 participants from 2 July 2025 to 27 September 2025. >90% of participants were highly-educated adult professionals aged 25-64 working at early/growth startups, public companies and in academia, with 80% of participants split equally between the US, the UK and Europe. 
Where do you work?
What is your role?
What is your highest level of education?
Source: https://airstreet.typeform.com/report/ke3fqSty/3HzifRvsfvIwRcnZ 

stateof.ai 2025
 | 284
>95% use AI at work and in their personal lives, and 76% pay out of their own pockets 
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
    In a vote for the usefulness of AI tools, 56% of respondents said they pay more than $21/month, which suggests they‚Äôre subscribing to team/pro plans that provide increased rate limits and greater intelligence. Furthermore, 9% of respondents pay more than $200/month for their services. 
How much do you pay/month?
Do you use gen AI at work?
Do you use gen AI in your personal life?
Source: https://airstreet.typeform.com/report/ke3fqSty/3HzifRvsfvIwRcnZ 


stateof.ai 2025
 | 285
92% of respondents report increased productivity gains from gen AI services
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
    47% felt a significantly increased productivity gain, while 2% said their productivity went down. For the users who described no impact or decreases, 60% of them were on free plans. By contrast, only 15% of those who reported productivity gains were on free plans. 
How has gen AI impacted your productivity?
% users on free plans
Source: https://airstreet.typeform.com/report/ke3fqSty/3HzifRvsfvIwRcnZ 


    The overwhelming trend amongst respondents who‚Äôve replaced and existing internet service with a generative AI tool is the disruption of traditional search engines, primarily Google. While few users have completely abandoned search engines, a significant majority now use generative AI as their first stop for a wide range of queries, especially those requiring complex answers, research, or coding help.
stateof.ai 2025
 | 286
Users look to AI for productivity, coding, and research‚Ä¶often replacing traditional search
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
What are your motivations for using AI?
Have you replaced an existing internet service with AI?
If yes: ChatGPT (102), Perplexity (41), Claude (30), Gemini (29)
Source: https://airstreet.typeform.com/report/ke3fqSty/3HzifRvsfvIwRcnZ 


    ‚ÄúWow‚Äù moments for users focused on AI's rapidly advancing capabilities, particularly in tangible, high-skill areas. Coding was the most frequently cited surprise, with users amazed by AI's ability to build entire applications and debug complex problems. This was closely followed by the dramatic improvements in media generation (video, image, and audio) and the power of deep research, analysis and emergent reasoning.

stateof.ai 2025
 | 287
What was the most surprising moment you had in the last year with AI?
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Source: https://airstreet.typeform.com/report/ke3fqSty/3HzifRvsfvIwRcnZ 


    The clearest trend is the adoption of specialized coding tools such as Claude Code and Cursor, which correlates with users stopping the use of GitHub Copilot and, to a lesser extent, ChatGPT for coding tasks. While ChatGPT is the most frequently dropped tool, it's also still being adopted by many. Gemini and Claude are the primary beneficiaries of this churn, with many users citing better performance or specific features like long context windows as their reason for switching. Users are also dropping single-purpose tools, e.g. Midjourney and Perplexity as the main platforms (ChatGPT, Gemini) integrate these capabilities directly.

 | 288
Hot or not? Which AI tools have you started vs. and stopped using this year?
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Which tools did you adopt this year?
Which tools did you drop this year?
stateof.ai 2025
Source: https://airstreet.typeform.com/report/ke3fqSty/3HzifRvsfvIwRcnZ 


    Despite the rise of neoclouds such as CoreWeave, Nebius, Lambda, and Crusoe, very few users report running their AI workloads on them. This supports the view that neoclouds are offering capacity to labs and hyperscalers. Instead, users prefer OpenAI directly, Google Cloud, and Anthropic. 
 | 289
AI services are largely run directly from OpenAI and Anthropic or hyperscalers
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
stateof.ai 2025
Source: https://airstreet.typeform.com/report/ke3fqSty/3HzifRvsfvIwRcnZ 


    This result does raise some questions about the need for sovereign AI. Those users who have switched provider because of data sovereignty concerns have done so because of customer requirements, regulations, or government/defense workloads. 
 | 290
Users sort of care for the location of their AI datacenters, but won‚Äôt switch because of it
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
stateof.ai 2025
How important is the location of your datacenter?
Have you switched provider due to data sovereignty?
Source: https://airstreet.typeform.com/report/ke3fqSty/3HzifRvsfvIwRcnZ 


    Meanwhile, the most common barriers to scaling the use of gen AI services is the upfront time required to make the systems work reliably, data privacy concerns, a lack of expertise, costs, integrations and lack of ROI.
 | 291
>70% report their organization‚Äôs budget for gen AI to have grown in the last year 
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
stateof.ai 2025
Primary barriers to scaling AI in your org?
How has your organization's AI budget changed?
Source: https://airstreet.typeform.com/report/ke3fqSty/3HzifRvsfvIwRcnZ 


    To some extent this is to be expected given the nascency of AI regulations and their limited implementations to date. But it‚Äôs also a positive to see that organizations are pressing forward anyways. 
 | 292
The AI regulatory landscape has not significantly impacted AI strategies‚Ä¶so far
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
stateof.ai 2025
How much do regulatory changes impact your AI strategy?
Source: https://airstreet.typeform.com/report/ke3fqSty/3HzifRvsfvIwRcnZ 


    Content, code, research and analysis heavy use cases are unsurprisingly the most popular.
 | 293
The most frequently used gen AI use cases within organizations
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
stateof.ai 2025
Source: https://airstreet.typeform.com/report/ke3fqSty/3HzifRvsfvIwRcnZ 


What apps do ML engineers build?

 | 294
How different roles use gen AI in their workflows
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
stateof.ai 2025
How developers use AI in coding?
How researchers use AI in their workflow?
Source: https://airstreet.typeform.com/report/ke3fqSty/3HzifRvsfvIwRcnZ 


 | 295
ChatGPT, Claude, Gemini/Google and Perplexity are used most regularly
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
stateof.ai 2025
    Despite its significant distribution, Meta‚Äôs AI is barely used as much as Mistral Le Chat or Midjourney. Meanwhile, DeepSeek isn‚Äôt far behind X‚Äôs Grok, again despite a distribution disadvantage.
Source: https://airstreet.typeform.com/report/ke3fqSty/3HzifRvsfvIwRcnZ 


 | 296
Developers love Cursor, Claude Code and GitHub Copilot
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
stateof.ai 2025
    OpenAI‚Äôs Codex and Gemini CLI are lagging behind where they should be given their resources. 
Source: https://airstreet.typeform.com/report/ke3fqSty/3HzifRvsfvIwRcnZ 


 | 297
Outside of developer tools, which AI services are most popular? 
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
stateof.ai 2025
    Our data corroborates the widely reported love that Deep Research received. Moreover, we find that respondents enjoy ChatGPT as a non-coding tool, ElevenLabs, Perplexity and Claude. 
Source: https://airstreet.typeform.com/report/ke3fqSty/3HzifRvsfvIwRcnZ 


 | 298
AI is mainly procured through APIs, followed by fine-tuning and building from scratch
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
stateof.ai 2025
    Despite a loud narrative that organizations want/must own their own models, our data shows that respondents procure AI through APIs far more than they build/fine-tune their own models. That said, fine-tuning isn‚Äôt going away either. To do so, respondents most commonly use PyTorch, Hugging Face Transformers, LoRA/PEFT, custom in-house frameworks, and Unsloth.
What type of gen AI models does your org use?
How have your fine-tuning workloads changed?
Source: https://airstreet.typeform.com/report/ke3fqSty/3HzifRvsfvIwRcnZ 


    Apple makes a surprising appearance, likely because users are training/experimenting locally. Meanwhile, TPUs and AMD‚Äôs GPU aren‚Äôt hugely popular. 
 | 299
Whether AI runs on public/private/on-prem, at the end of the day, it still uses a GPU
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
stateof.ai 2025
Where is your AI hardware provisioned?
What hardware do you use for training/fine-tuning?
Source: https://airstreet.typeform.com/report/ke3fqSty/3HzifRvsfvIwRcnZ 


 | 300
What emerging trends in gen AI are you most excited about?
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
stateof.ai 2025
Source: https://airstreet.typeform.com/report/ke3fqSty/3HzifRvsfvIwRcnZ 


How 1.2k practitioners rated their top AI labs
 | 301
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
stateof.ai 2025
#1
#2
#3
#4
#5
#6
#7
#8
#9
#10
#11
#12
Source: https://airstreet.typeform.com/report/ke3fqSty/3HzifRvsfvIwRcnZ 



STATE OF AI SURVEY.
üëâ Take the survey: stateof.ai/survey 

Contribute to the world‚Äôs largest open, continuously updated AI survey.
The real pulse from builders, operators, and researchers.


 | 302
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions

stateof.ai 2025
 | 303
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Section 6: Predictions

stateof.ai 2025
10 predictions for the next 12 months
2. A major AI lab leans back into open-sourcing frontier models to win over the current US administration.
1. A major retailer reports >5% of online sales from agentic checkout as AI agent advertising spend hits $5B.
3. Open-ended agents make a meaningful scientific discovery end-to-end (hypothesis, expt, iteration, paper).
4. A deepfake/agent-driven cyber attack triggers the first NATO/UN emergency debate on AI security.
5. A real-time generative video game becomes the year‚Äôs most-watched title on Twitch.
 | 304
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
6. ‚ÄúAI neutrality‚Äù emerges as a foreign policy doctrine as some nations cannot or fail to develop sovereign AI. 
7. A movie or short film produced with significant use of AI wins major audience praise and sparks backlash. 
8. A Chinese lab overtakes the US lab dominated frontier on a major leaderboard (e.g. LMArena/Artificial Analysis).
9. Datacenter NIMBYism takes the US by storm and sways certain midterm/gubernatorial elections in 2026.
10. Trump issues an executive order to ban state AI legislation that is found unconstitutional by SCOTUS.

Thanks!
Thank you for making it to the end of the State of AI Report 2025.

We hope you‚Äôve enjoyed our informed and opinionated take on the extraordinary progress in artificial intelligence over the past year, since last year‚Äôs edition was published on 10 October 2024. This year‚Äôs report explores AI research, industry, politics, safety, and insights from our first State of AI Report usage survey. We focus on these areas because we believe that AI is a force multiplier for technological progress - and that broad understanding of its trajectory is essential if we are to navigate such a profound transition.

We‚Äôd love your feedback on how we can make future editions even better, as well as your ideas for new contributions and perspectives.

Nathan Benaich
Air Street Capital

stateof.ai 2025
 | 305
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions

We‚Äôd like to thank the following individuals for providing critical review of this year‚Äôs Report (alphabetical order):

Jacob Arbeid, Paige Bailey, Joyce Benaich, Daniel Campos, Xander Davies, Chris Gagne, Aleksa Gordic, Ido Hakimi, Ryan Julian, Neel Nanda, Elvis Osaravia, Jacob Portes, Philippe Schwaller, Shubho Sengupta, Joe Spisak, David Stutz, Ross Taylor, and Divy Thakkar.
We‚Äôd like to thank the following companies for providing bespoke data/analysis (alphabetical order):

Dealroom, Ramp, Specter, and Zeta Alpha. 
Reviewers
stateof.ai 2025
 | 306
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Data contributors

The authors declare a number of conflicts of interest as a result of being investors and/or advisors, personally or via funds, in a number of private and public companies whose work is cited in this report. Notably, the authors are investors in companies listed at: airstreet.com/portfolio
Conflicts of interest
stateof.ai 2025
 | 307
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions

 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
 | 308
stateof.ai 2025
About the authors
Nathan is the General Partner of Air Street Capital, a venture capital firm investing in AI-first companies. He runs The Research and Applied AI Summit (RAAIS), The RAAIS Foundation (funding open-source AI projects), AI communities in the US and Europe, and Spinout.fyi (improving university spinout creation). He studied biology at Williams College and earned a PhD from Cambridge in cancer research as a Gates Scholar. 
Nathan Benaich

 | 309
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
stateof.ai 2025
Zeke Gillman
Zeke is a Tech Policy Fellow at Stanford, and co-author of Regulating under Uncertainty. He previously worked at Harvard Business School and the DOJ Antitrust Division, and holds a BA in Political Science and Philosophy from the University of Chicago.
Ryan Tovcimak
Ryan is a founder of the AI Stack Tracker. His work spans red-teaming frontier models, benchmarking the global AI competition, and tracking trends in AI compute and power demands. He holds a BS in Econ from Vanderbilt University.
Nell Norman
Nell is a grad student in Computing at Imperial College London focusing on how LLMs could enable scalable vishing fraud. She previously helped AI teams build reliable products at AI agent platform V7 Labs, and has a first class BA from Oxford University. 
State of AI Report 2025 team

 | 310
 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
stateof.ai 2025
Follow our writing on                                       (press.airstreet.com)
    If you enjoy reading the State of AI Report, we invite you to read and subscribe to Air Street Press, the home of our analytical writing, news, and opinions.

 Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Join our global community of best practices events (airstreet.com/events)
 | 311

STATE OF AI REPORT.
October 9, 2025

Nathan Benaich

AIR STREET CAPITAL.
airstreet.com
stateof.ai
